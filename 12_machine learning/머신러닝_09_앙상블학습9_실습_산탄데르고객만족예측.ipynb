{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류 실습 : XGBoost와 LightGBM을 이용하여 고객만족 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 데이터\n",
    "- Kaggle의 산탄데르 고객만족(Santander Customer Satisfaction) 데이터 세트\n",
    "- 산탄데르 은행이 캐글에 의뢰한 데이터\n",
    "- https://www.kaggle.com/c/santander-customer-satisfaction/data\n",
    "- features : 370개, 모두 익명 처리\n",
    "- target : 1이면 불만, 0이면 만족\n",
    "- 모델의 성능평가:  ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 준비 및 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets shape: (76020, 371)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39205.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49278.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67333.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n",
       "0   1     2     23                 0.0                      0.0   \n",
       "1   3     2     34                 0.0                      0.0   \n",
       "2   4     2     23                 0.0                      0.0   \n",
       "\n",
       "   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "0                      0.0                      0.0                      0.0   \n",
       "1                      0.0                      0.0                      0.0   \n",
       "2                      0.0                      0.0                      0.0   \n",
       "\n",
       "   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n",
       "0                      0.0                      0.0  ...   \n",
       "1                      0.0                      0.0  ...   \n",
       "2                      0.0                      0.0  ...   \n",
       "\n",
       "   saldo_medio_var33_hace2  saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n",
       "0                      0.0                      0.0                     0.0   \n",
       "1                      0.0                      0.0                     0.0   \n",
       "2                      0.0                      0.0                     0.0   \n",
       "\n",
       "   saldo_medio_var33_ult3  saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "0                     0.0                      0.0                      0.0   \n",
       "1                     0.0                      0.0                      0.0   \n",
       "2                     0.0                      0.0                      0.0   \n",
       "\n",
       "   saldo_medio_var44_ult1  saldo_medio_var44_ult3     var38  TARGET  \n",
       "0                     0.0                     0.0  39205.17       0  \n",
       "1                     0.0                     0.0  49278.03       0  \n",
       "2                     0.0                     0.0  67333.77       0  \n",
       "\n",
       "[3 rows x 371 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/santander/train.csv', encoding='latin-1')\n",
    "print(f'datasets shape: {df.shape}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET\n",
       "0    73012\n",
       "1     3008\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.TARGET.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불만족 비율: 3.96\n"
     ]
    }
   ],
   "source": [
    "print(f'불만족 비율: {df[df.TARGET == 1].TARGET.count() / df.TARGET.count() * 100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>7.602000e+04</td>\n",
       "      <td>76020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75964.050723</td>\n",
       "      <td>-1523.199277</td>\n",
       "      <td>33.212865</td>\n",
       "      <td>86.208265</td>\n",
       "      <td>72.363067</td>\n",
       "      <td>119.529632</td>\n",
       "      <td>3.559130</td>\n",
       "      <td>6.472698</td>\n",
       "      <td>0.412946</td>\n",
       "      <td>0.567352</td>\n",
       "      <td>...</td>\n",
       "      <td>7.935824</td>\n",
       "      <td>1.365146</td>\n",
       "      <td>12.215580</td>\n",
       "      <td>8.784074</td>\n",
       "      <td>31.505324</td>\n",
       "      <td>1.858575</td>\n",
       "      <td>76.026165</td>\n",
       "      <td>56.614351</td>\n",
       "      <td>1.172358e+05</td>\n",
       "      <td>0.039569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43781.947379</td>\n",
       "      <td>39033.462364</td>\n",
       "      <td>12.956486</td>\n",
       "      <td>1614.757313</td>\n",
       "      <td>339.315831</td>\n",
       "      <td>546.266294</td>\n",
       "      <td>93.155749</td>\n",
       "      <td>153.737066</td>\n",
       "      <td>30.604864</td>\n",
       "      <td>36.513513</td>\n",
       "      <td>...</td>\n",
       "      <td>455.887218</td>\n",
       "      <td>113.959637</td>\n",
       "      <td>783.207399</td>\n",
       "      <td>538.439211</td>\n",
       "      <td>2013.125393</td>\n",
       "      <td>147.786584</td>\n",
       "      <td>4040.337842</td>\n",
       "      <td>2852.579397</td>\n",
       "      <td>1.826646e+05</td>\n",
       "      <td>0.194945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.163750e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38104.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.787061e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>76043.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.064092e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>113748.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.187563e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>151838.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>210000.000000</td>\n",
       "      <td>12888.030000</td>\n",
       "      <td>21024.810000</td>\n",
       "      <td>8237.820000</td>\n",
       "      <td>11073.570000</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50003.880000</td>\n",
       "      <td>20385.720000</td>\n",
       "      <td>138831.630000</td>\n",
       "      <td>91778.730000</td>\n",
       "      <td>438329.220000</td>\n",
       "      <td>24650.010000</td>\n",
       "      <td>681462.900000</td>\n",
       "      <td>397884.300000</td>\n",
       "      <td>2.203474e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID           var3         var15  imp_ent_var16_ult1  \\\n",
       "count   76020.000000   76020.000000  76020.000000        76020.000000   \n",
       "mean    75964.050723   -1523.199277     33.212865           86.208265   \n",
       "std     43781.947379   39033.462364     12.956486         1614.757313   \n",
       "min         1.000000 -999999.000000      5.000000            0.000000   \n",
       "25%     38104.750000       2.000000     23.000000            0.000000   \n",
       "50%     76043.000000       2.000000     28.000000            0.000000   \n",
       "75%    113748.750000       2.000000     40.000000            0.000000   \n",
       "max    151838.000000     238.000000    105.000000       210000.000000   \n",
       "\n",
       "       imp_op_var39_comer_ult1  imp_op_var39_comer_ult3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                 72.363067               119.529632   \n",
       "std                 339.315831               546.266294   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max               12888.030000             21024.810000   \n",
       "\n",
       "       imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                  3.559130                 6.472698   \n",
       "std                  93.155749               153.737066   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max                8237.820000             11073.570000   \n",
       "\n",
       "       imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n",
       "count             76020.000000             76020.000000  ...   \n",
       "mean                  0.412946                 0.567352  ...   \n",
       "std                  30.604864                36.513513  ...   \n",
       "min                   0.000000                 0.000000  ...   \n",
       "25%                   0.000000                 0.000000  ...   \n",
       "50%                   0.000000                 0.000000  ...   \n",
       "75%                   0.000000                 0.000000  ...   \n",
       "max                6600.000000              6600.000000  ...   \n",
       "\n",
       "       saldo_medio_var33_hace2  saldo_medio_var33_hace3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                  7.935824                 1.365146   \n",
       "std                 455.887218               113.959637   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max               50003.880000             20385.720000   \n",
       "\n",
       "       saldo_medio_var33_ult1  saldo_medio_var33_ult3  \\\n",
       "count            76020.000000            76020.000000   \n",
       "mean                12.215580                8.784074   \n",
       "std                783.207399              538.439211   \n",
       "min                  0.000000                0.000000   \n",
       "25%                  0.000000                0.000000   \n",
       "50%                  0.000000                0.000000   \n",
       "75%                  0.000000                0.000000   \n",
       "max             138831.630000            91778.730000   \n",
       "\n",
       "       saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                 31.505324                 1.858575   \n",
       "std                2013.125393               147.786584   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max              438329.220000             24650.010000   \n",
       "\n",
       "       saldo_medio_var44_ult1  saldo_medio_var44_ult3         var38  \\\n",
       "count            76020.000000            76020.000000  7.602000e+04   \n",
       "mean                76.026165               56.614351  1.172358e+05   \n",
       "std               4040.337842             2852.579397  1.826646e+05   \n",
       "min                  0.000000                0.000000  5.163750e+03   \n",
       "25%                  0.000000                0.000000  6.787061e+04   \n",
       "50%                  0.000000                0.000000  1.064092e+05   \n",
       "75%                  0.000000                0.000000  1.187563e+05   \n",
       "max             681462.900000           397884.300000  2.203474e+07   \n",
       "\n",
       "             TARGET  \n",
       "count  76020.000000  \n",
       "mean       0.039569  \n",
       "std        0.194945  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 371 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리\n",
    "\n",
    "- 결측치 처리 / 피처 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var3\n",
       " 2         74165\n",
       " 8           138\n",
       "-999999      116\n",
       " 9           110\n",
       " 3           108\n",
       "           ...  \n",
       " 231           1\n",
       " 188           1\n",
       " 168           1\n",
       " 135           1\n",
       " 87            1\n",
       "Name: count, Length: 208, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.var3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var3\n",
       "2      74281\n",
       "8        138\n",
       "9        110\n",
       "3        108\n",
       "1        105\n",
       "       ...  \n",
       "231        1\n",
       "188        1\n",
       "168        1\n",
       "135        1\n",
       "87         1\n",
       "Name: count, Length: 207, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.var3.replace(-999999, 2, inplace=True)\n",
    "df.var3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
       "       'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1',\n",
       "       'imp_op_var40_comer_ult3', 'imp_op_var40_efect_ult1',\n",
       "       'imp_op_var40_efect_ult3',\n",
       "       ...\n",
       "       'saldo_medio_var33_hace2', 'saldo_medio_var33_hace3',\n",
       "       'saldo_medio_var33_ult1', 'saldo_medio_var33_ult3',\n",
       "       'saldo_medio_var44_hace2', 'saldo_medio_var44_hace3',\n",
       "       'saldo_medio_var44_ult1', 'saldo_medio_var44_ult3', 'var38', 'TARGET'],\n",
       "      dtype='object', length=371)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습/테스트 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2,\n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터: TARGET\n",
      "0    96.096422\n",
      "1     3.903578\n",
      "Name: count, dtype: float64\n",
      "\n",
      "테스트데이터: TARGET\n",
      "0    95.830045\n",
      "1     4.169955\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f'학습데이터: {y_train.value_counts()/y_train.count()*100}\\n')\n",
    "print(f'테스트데이터: {y_test.value_counts()/y_test.count()*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습/검증 데이터 세트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(x_train,y_train, test_size=0.3,\n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost 모델 학습과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.83869\tvalidation_1-auc:0.83603\n",
      "[1]\tvalidation_0-auc:0.84000\tvalidation_1-auc:0.83881\n",
      "[2]\tvalidation_0-auc:0.84026\tvalidation_1-auc:0.83848\n",
      "[3]\tvalidation_0-auc:0.84016\tvalidation_1-auc:0.83998\n",
      "[4]\tvalidation_0-auc:0.84162\tvalidation_1-auc:0.84068\n",
      "[5]\tvalidation_0-auc:0.84435\tvalidation_1-auc:0.84119\n",
      "[6]\tvalidation_0-auc:0.84545\tvalidation_1-auc:0.84260\n",
      "[7]\tvalidation_0-auc:0.84609\tvalidation_1-auc:0.84416\n",
      "[8]\tvalidation_0-auc:0.84772\tvalidation_1-auc:0.84558\n",
      "[9]\tvalidation_0-auc:0.84825\tvalidation_1-auc:0.84570\n",
      "[10]\tvalidation_0-auc:0.84888\tvalidation_1-auc:0.84622\n",
      "[11]\tvalidation_0-auc:0.84935\tvalidation_1-auc:0.84663\n",
      "[12]\tvalidation_0-auc:0.84961\tvalidation_1-auc:0.84666\n",
      "[13]\tvalidation_0-auc:0.84976\tvalidation_1-auc:0.84696\n",
      "[14]\tvalidation_0-auc:0.85081\tvalidation_1-auc:0.84708\n",
      "[15]\tvalidation_0-auc:0.85326\tvalidation_1-auc:0.84850\n",
      "[16]\tvalidation_0-auc:0.85464\tvalidation_1-auc:0.84944\n",
      "[17]\tvalidation_0-auc:0.85532\tvalidation_1-auc:0.84983\n",
      "[18]\tvalidation_0-auc:0.85608\tvalidation_1-auc:0.84994\n",
      "[19]\tvalidation_0-auc:0.85756\tvalidation_1-auc:0.85075\n",
      "[20]\tvalidation_0-auc:0.85820\tvalidation_1-auc:0.85182\n",
      "[21]\tvalidation_0-auc:0.85860\tvalidation_1-auc:0.85259\n",
      "[22]\tvalidation_0-auc:0.85907\tvalidation_1-auc:0.85323\n",
      "[23]\tvalidation_0-auc:0.85928\tvalidation_1-auc:0.85396\n",
      "[24]\tvalidation_0-auc:0.85986\tvalidation_1-auc:0.85435\n",
      "[25]\tvalidation_0-auc:0.86034\tvalidation_1-auc:0.85469\n",
      "[26]\tvalidation_0-auc:0.86098\tvalidation_1-auc:0.85528\n",
      "[27]\tvalidation_0-auc:0.86145\tvalidation_1-auc:0.85566\n",
      "[28]\tvalidation_0-auc:0.86166\tvalidation_1-auc:0.85595\n",
      "[29]\tvalidation_0-auc:0.86194\tvalidation_1-auc:0.85603\n",
      "[30]\tvalidation_0-auc:0.86226\tvalidation_1-auc:0.85662\n",
      "[31]\tvalidation_0-auc:0.86263\tvalidation_1-auc:0.85723\n",
      "[32]\tvalidation_0-auc:0.86301\tvalidation_1-auc:0.85765\n",
      "[33]\tvalidation_0-auc:0.86360\tvalidation_1-auc:0.85834\n",
      "[34]\tvalidation_0-auc:0.86404\tvalidation_1-auc:0.85895\n",
      "[35]\tvalidation_0-auc:0.86440\tvalidation_1-auc:0.85908\n",
      "[36]\tvalidation_0-auc:0.86477\tvalidation_1-auc:0.85992\n",
      "[37]\tvalidation_0-auc:0.86519\tvalidation_1-auc:0.86062\n",
      "[38]\tvalidation_0-auc:0.86559\tvalidation_1-auc:0.86158\n",
      "[39]\tvalidation_0-auc:0.86605\tvalidation_1-auc:0.86192\n",
      "[40]\tvalidation_0-auc:0.86645\tvalidation_1-auc:0.86222\n",
      "[41]\tvalidation_0-auc:0.86679\tvalidation_1-auc:0.86257\n",
      "[42]\tvalidation_0-auc:0.86727\tvalidation_1-auc:0.86324\n",
      "[43]\tvalidation_0-auc:0.86796\tvalidation_1-auc:0.86386\n",
      "[44]\tvalidation_0-auc:0.86851\tvalidation_1-auc:0.86463\n",
      "[45]\tvalidation_0-auc:0.86896\tvalidation_1-auc:0.86508\n",
      "[46]\tvalidation_0-auc:0.86968\tvalidation_1-auc:0.86547\n",
      "[47]\tvalidation_0-auc:0.87002\tvalidation_1-auc:0.86579\n",
      "[48]\tvalidation_0-auc:0.87080\tvalidation_1-auc:0.86619\n",
      "[49]\tvalidation_0-auc:0.87125\tvalidation_1-auc:0.86673\n",
      "[50]\tvalidation_0-auc:0.87183\tvalidation_1-auc:0.86717\n",
      "[51]\tvalidation_0-auc:0.87233\tvalidation_1-auc:0.86761\n",
      "[52]\tvalidation_0-auc:0.87272\tvalidation_1-auc:0.86792\n",
      "[53]\tvalidation_0-auc:0.87315\tvalidation_1-auc:0.86834\n",
      "[54]\tvalidation_0-auc:0.87358\tvalidation_1-auc:0.86878\n",
      "[55]\tvalidation_0-auc:0.87401\tvalidation_1-auc:0.86906\n",
      "[56]\tvalidation_0-auc:0.87456\tvalidation_1-auc:0.86990\n",
      "[57]\tvalidation_0-auc:0.87502\tvalidation_1-auc:0.87008\n",
      "[58]\tvalidation_0-auc:0.87544\tvalidation_1-auc:0.87044\n",
      "[59]\tvalidation_0-auc:0.87577\tvalidation_1-auc:0.87080\n",
      "[60]\tvalidation_0-auc:0.87612\tvalidation_1-auc:0.87157\n",
      "[61]\tvalidation_0-auc:0.87659\tvalidation_1-auc:0.87200\n",
      "[62]\tvalidation_0-auc:0.87704\tvalidation_1-auc:0.87225\n",
      "[63]\tvalidation_0-auc:0.87747\tvalidation_1-auc:0.87271\n",
      "[64]\tvalidation_0-auc:0.87778\tvalidation_1-auc:0.87286\n",
      "[65]\tvalidation_0-auc:0.87819\tvalidation_1-auc:0.87312\n",
      "[66]\tvalidation_0-auc:0.87854\tvalidation_1-auc:0.87369\n",
      "[67]\tvalidation_0-auc:0.87889\tvalidation_1-auc:0.87403\n",
      "[68]\tvalidation_0-auc:0.87925\tvalidation_1-auc:0.87430\n",
      "[69]\tvalidation_0-auc:0.87967\tvalidation_1-auc:0.87465\n",
      "[70]\tvalidation_0-auc:0.87997\tvalidation_1-auc:0.87498\n",
      "[71]\tvalidation_0-auc:0.88025\tvalidation_1-auc:0.87531\n",
      "[72]\tvalidation_0-auc:0.88054\tvalidation_1-auc:0.87580\n",
      "[73]\tvalidation_0-auc:0.88079\tvalidation_1-auc:0.87603\n",
      "[74]\tvalidation_0-auc:0.88102\tvalidation_1-auc:0.87624\n",
      "[75]\tvalidation_0-auc:0.88137\tvalidation_1-auc:0.87666\n",
      "[76]\tvalidation_0-auc:0.88169\tvalidation_1-auc:0.87697\n",
      "[77]\tvalidation_0-auc:0.88187\tvalidation_1-auc:0.87729\n",
      "[78]\tvalidation_0-auc:0.88220\tvalidation_1-auc:0.87737\n",
      "[79]\tvalidation_0-auc:0.88250\tvalidation_1-auc:0.87767\n",
      "[80]\tvalidation_0-auc:0.88269\tvalidation_1-auc:0.87785\n",
      "[81]\tvalidation_0-auc:0.88300\tvalidation_1-auc:0.87811\n",
      "[82]\tvalidation_0-auc:0.88318\tvalidation_1-auc:0.87819\n",
      "[83]\tvalidation_0-auc:0.88337\tvalidation_1-auc:0.87841\n",
      "[84]\tvalidation_0-auc:0.88373\tvalidation_1-auc:0.87850\n",
      "[85]\tvalidation_0-auc:0.88391\tvalidation_1-auc:0.87867\n",
      "[86]\tvalidation_0-auc:0.88403\tvalidation_1-auc:0.87889\n",
      "[87]\tvalidation_0-auc:0.88428\tvalidation_1-auc:0.87909\n",
      "[88]\tvalidation_0-auc:0.88454\tvalidation_1-auc:0.87914\n",
      "[89]\tvalidation_0-auc:0.88490\tvalidation_1-auc:0.87942\n",
      "[90]\tvalidation_0-auc:0.88501\tvalidation_1-auc:0.87954\n",
      "[91]\tvalidation_0-auc:0.88538\tvalidation_1-auc:0.87980\n",
      "[92]\tvalidation_0-auc:0.88546\tvalidation_1-auc:0.87999\n",
      "[93]\tvalidation_0-auc:0.88590\tvalidation_1-auc:0.88042\n",
      "[94]\tvalidation_0-auc:0.88601\tvalidation_1-auc:0.88067\n",
      "[95]\tvalidation_0-auc:0.88628\tvalidation_1-auc:0.88074\n",
      "[96]\tvalidation_0-auc:0.88658\tvalidation_1-auc:0.88096\n",
      "[97]\tvalidation_0-auc:0.88708\tvalidation_1-auc:0.88134\n",
      "[98]\tvalidation_0-auc:0.88712\tvalidation_1-auc:0.88147\n",
      "[99]\tvalidation_0-auc:0.88755\tvalidation_1-auc:0.88170\n",
      "[100]\tvalidation_0-auc:0.88820\tvalidation_1-auc:0.88235\n",
      "[101]\tvalidation_0-auc:0.88826\tvalidation_1-auc:0.88252\n",
      "[102]\tvalidation_0-auc:0.88866\tvalidation_1-auc:0.88294\n",
      "[103]\tvalidation_0-auc:0.88899\tvalidation_1-auc:0.88314\n",
      "[104]\tvalidation_0-auc:0.88908\tvalidation_1-auc:0.88321\n",
      "[105]\tvalidation_0-auc:0.88948\tvalidation_1-auc:0.88354\n",
      "[106]\tvalidation_0-auc:0.88963\tvalidation_1-auc:0.88352\n",
      "[107]\tvalidation_0-auc:0.88991\tvalidation_1-auc:0.88375\n",
      "[108]\tvalidation_0-auc:0.88997\tvalidation_1-auc:0.88382\n",
      "[109]\tvalidation_0-auc:0.89010\tvalidation_1-auc:0.88387\n",
      "[110]\tvalidation_0-auc:0.89052\tvalidation_1-auc:0.88405\n",
      "[111]\tvalidation_0-auc:0.89057\tvalidation_1-auc:0.88411\n",
      "[112]\tvalidation_0-auc:0.89077\tvalidation_1-auc:0.88414\n",
      "[113]\tvalidation_0-auc:0.89101\tvalidation_1-auc:0.88444\n",
      "[114]\tvalidation_0-auc:0.89113\tvalidation_1-auc:0.88455\n",
      "[115]\tvalidation_0-auc:0.89117\tvalidation_1-auc:0.88460\n",
      "[116]\tvalidation_0-auc:0.89135\tvalidation_1-auc:0.88473\n",
      "[117]\tvalidation_0-auc:0.89153\tvalidation_1-auc:0.88487\n",
      "[118]\tvalidation_0-auc:0.89178\tvalidation_1-auc:0.88509\n",
      "[119]\tvalidation_0-auc:0.89188\tvalidation_1-auc:0.88517\n",
      "[120]\tvalidation_0-auc:0.89207\tvalidation_1-auc:0.88530\n",
      "[121]\tvalidation_0-auc:0.89212\tvalidation_1-auc:0.88536\n",
      "[122]\tvalidation_0-auc:0.89232\tvalidation_1-auc:0.88554\n",
      "[123]\tvalidation_0-auc:0.89239\tvalidation_1-auc:0.88558\n",
      "[124]\tvalidation_0-auc:0.89253\tvalidation_1-auc:0.88556\n",
      "[125]\tvalidation_0-auc:0.89267\tvalidation_1-auc:0.88566\n",
      "[126]\tvalidation_0-auc:0.89270\tvalidation_1-auc:0.88571\n",
      "[127]\tvalidation_0-auc:0.89276\tvalidation_1-auc:0.88576\n",
      "[128]\tvalidation_0-auc:0.89293\tvalidation_1-auc:0.88581\n",
      "[129]\tvalidation_0-auc:0.89296\tvalidation_1-auc:0.88582\n",
      "[130]\tvalidation_0-auc:0.89343\tvalidation_1-auc:0.88611\n",
      "[131]\tvalidation_0-auc:0.89366\tvalidation_1-auc:0.88631\n",
      "[132]\tvalidation_0-auc:0.89380\tvalidation_1-auc:0.88638\n",
      "[133]\tvalidation_0-auc:0.89383\tvalidation_1-auc:0.88642\n",
      "[134]\tvalidation_0-auc:0.89393\tvalidation_1-auc:0.88650\n",
      "[135]\tvalidation_0-auc:0.89410\tvalidation_1-auc:0.88671\n",
      "[136]\tvalidation_0-auc:0.89414\tvalidation_1-auc:0.88680\n",
      "[137]\tvalidation_0-auc:0.89426\tvalidation_1-auc:0.88680\n",
      "[138]\tvalidation_0-auc:0.89446\tvalidation_1-auc:0.88716\n",
      "[139]\tvalidation_0-auc:0.89451\tvalidation_1-auc:0.88718\n",
      "[140]\tvalidation_0-auc:0.89476\tvalidation_1-auc:0.88744\n",
      "[141]\tvalidation_0-auc:0.89479\tvalidation_1-auc:0.88750\n",
      "[142]\tvalidation_0-auc:0.89480\tvalidation_1-auc:0.88750\n",
      "[143]\tvalidation_0-auc:0.89492\tvalidation_1-auc:0.88760\n",
      "[144]\tvalidation_0-auc:0.89495\tvalidation_1-auc:0.88773\n",
      "[145]\tvalidation_0-auc:0.89512\tvalidation_1-auc:0.88786\n",
      "[146]\tvalidation_0-auc:0.89528\tvalidation_1-auc:0.88798\n",
      "[147]\tvalidation_0-auc:0.89540\tvalidation_1-auc:0.88813\n",
      "[148]\tvalidation_0-auc:0.89543\tvalidation_1-auc:0.88816\n",
      "[149]\tvalidation_0-auc:0.89547\tvalidation_1-auc:0.88822\n",
      "[150]\tvalidation_0-auc:0.89550\tvalidation_1-auc:0.88828\n",
      "[151]\tvalidation_0-auc:0.89558\tvalidation_1-auc:0.88835\n",
      "[152]\tvalidation_0-auc:0.89562\tvalidation_1-auc:0.88839\n",
      "[153]\tvalidation_0-auc:0.89570\tvalidation_1-auc:0.88846\n",
      "[154]\tvalidation_0-auc:0.89596\tvalidation_1-auc:0.88890\n",
      "[155]\tvalidation_0-auc:0.89609\tvalidation_1-auc:0.88902\n",
      "[156]\tvalidation_0-auc:0.89614\tvalidation_1-auc:0.88908\n",
      "[157]\tvalidation_0-auc:0.89616\tvalidation_1-auc:0.88910\n",
      "[158]\tvalidation_0-auc:0.89643\tvalidation_1-auc:0.88937\n",
      "[159]\tvalidation_0-auc:0.89657\tvalidation_1-auc:0.88947\n",
      "[160]\tvalidation_0-auc:0.89658\tvalidation_1-auc:0.88948\n",
      "[161]\tvalidation_0-auc:0.89665\tvalidation_1-auc:0.88957\n",
      "[162]\tvalidation_0-auc:0.89669\tvalidation_1-auc:0.88961\n",
      "[163]\tvalidation_0-auc:0.89679\tvalidation_1-auc:0.88967\n",
      "[164]\tvalidation_0-auc:0.89693\tvalidation_1-auc:0.88985\n",
      "[165]\tvalidation_0-auc:0.89697\tvalidation_1-auc:0.88988\n",
      "[166]\tvalidation_0-auc:0.89702\tvalidation_1-auc:0.88994\n",
      "[167]\tvalidation_0-auc:0.89707\tvalidation_1-auc:0.89002\n",
      "[168]\tvalidation_0-auc:0.89720\tvalidation_1-auc:0.89012\n",
      "[169]\tvalidation_0-auc:0.89725\tvalidation_1-auc:0.89018\n",
      "[170]\tvalidation_0-auc:0.89733\tvalidation_1-auc:0.89024\n",
      "[171]\tvalidation_0-auc:0.89747\tvalidation_1-auc:0.89043\n",
      "[172]\tvalidation_0-auc:0.89749\tvalidation_1-auc:0.89047\n",
      "[173]\tvalidation_0-auc:0.89762\tvalidation_1-auc:0.89055\n",
      "[174]\tvalidation_0-auc:0.89763\tvalidation_1-auc:0.89055\n",
      "[175]\tvalidation_0-auc:0.89770\tvalidation_1-auc:0.89060\n",
      "[176]\tvalidation_0-auc:0.89775\tvalidation_1-auc:0.89066\n",
      "[177]\tvalidation_0-auc:0.89778\tvalidation_1-auc:0.89069\n",
      "[178]\tvalidation_0-auc:0.89778\tvalidation_1-auc:0.89070\n",
      "[179]\tvalidation_0-auc:0.89797\tvalidation_1-auc:0.89096\n",
      "[180]\tvalidation_0-auc:0.89803\tvalidation_1-auc:0.89098\n",
      "[181]\tvalidation_0-auc:0.89830\tvalidation_1-auc:0.89128\n",
      "[182]\tvalidation_0-auc:0.89836\tvalidation_1-auc:0.89135\n",
      "[183]\tvalidation_0-auc:0.89848\tvalidation_1-auc:0.89150\n",
      "[184]\tvalidation_0-auc:0.89870\tvalidation_1-auc:0.89168\n",
      "[185]\tvalidation_0-auc:0.89879\tvalidation_1-auc:0.89173\n",
      "[186]\tvalidation_0-auc:0.89883\tvalidation_1-auc:0.89178\n",
      "[187]\tvalidation_0-auc:0.89889\tvalidation_1-auc:0.89179\n",
      "[188]\tvalidation_0-auc:0.89895\tvalidation_1-auc:0.89181\n",
      "[189]\tvalidation_0-auc:0.89899\tvalidation_1-auc:0.89185\n",
      "[190]\tvalidation_0-auc:0.89904\tvalidation_1-auc:0.89189\n",
      "[191]\tvalidation_0-auc:0.89926\tvalidation_1-auc:0.89226\n",
      "[192]\tvalidation_0-auc:0.89933\tvalidation_1-auc:0.89230\n",
      "[193]\tvalidation_0-auc:0.89946\tvalidation_1-auc:0.89244\n",
      "[194]\tvalidation_0-auc:0.89959\tvalidation_1-auc:0.89253\n",
      "[195]\tvalidation_0-auc:0.89979\tvalidation_1-auc:0.89272\n",
      "[196]\tvalidation_0-auc:0.89982\tvalidation_1-auc:0.89275\n",
      "[197]\tvalidation_0-auc:0.89991\tvalidation_1-auc:0.89289\n",
      "[198]\tvalidation_0-auc:0.89997\tvalidation_1-auc:0.89293\n",
      "[199]\tvalidation_0-auc:0.90013\tvalidation_1-auc:0.89299\n",
      "[200]\tvalidation_0-auc:0.90028\tvalidation_1-auc:0.89304\n",
      "[201]\tvalidation_0-auc:0.90035\tvalidation_1-auc:0.89313\n",
      "[202]\tvalidation_0-auc:0.90050\tvalidation_1-auc:0.89325\n",
      "[203]\tvalidation_0-auc:0.90056\tvalidation_1-auc:0.89331\n",
      "[204]\tvalidation_0-auc:0.90059\tvalidation_1-auc:0.89333\n",
      "[205]\tvalidation_0-auc:0.90064\tvalidation_1-auc:0.89337\n",
      "[206]\tvalidation_0-auc:0.90085\tvalidation_1-auc:0.89353\n",
      "[207]\tvalidation_0-auc:0.90088\tvalidation_1-auc:0.89354\n",
      "[208]\tvalidation_0-auc:0.90092\tvalidation_1-auc:0.89356\n",
      "[209]\tvalidation_0-auc:0.90095\tvalidation_1-auc:0.89360\n",
      "[210]\tvalidation_0-auc:0.90106\tvalidation_1-auc:0.89375\n",
      "[211]\tvalidation_0-auc:0.90126\tvalidation_1-auc:0.89400\n",
      "[212]\tvalidation_0-auc:0.90143\tvalidation_1-auc:0.89417\n",
      "[213]\tvalidation_0-auc:0.90156\tvalidation_1-auc:0.89436\n",
      "[214]\tvalidation_0-auc:0.90174\tvalidation_1-auc:0.89453\n",
      "[215]\tvalidation_0-auc:0.90179\tvalidation_1-auc:0.89456\n",
      "[216]\tvalidation_0-auc:0.90187\tvalidation_1-auc:0.89460\n",
      "[217]\tvalidation_0-auc:0.90200\tvalidation_1-auc:0.89479\n",
      "[218]\tvalidation_0-auc:0.90217\tvalidation_1-auc:0.89493\n",
      "[219]\tvalidation_0-auc:0.90221\tvalidation_1-auc:0.89495\n",
      "[220]\tvalidation_0-auc:0.90223\tvalidation_1-auc:0.89496\n",
      "[221]\tvalidation_0-auc:0.90227\tvalidation_1-auc:0.89500\n",
      "[222]\tvalidation_0-auc:0.90245\tvalidation_1-auc:0.89522\n",
      "[223]\tvalidation_0-auc:0.90270\tvalidation_1-auc:0.89560\n",
      "[224]\tvalidation_0-auc:0.90281\tvalidation_1-auc:0.89565\n",
      "[225]\tvalidation_0-auc:0.90285\tvalidation_1-auc:0.89569\n",
      "[226]\tvalidation_0-auc:0.90300\tvalidation_1-auc:0.89586\n",
      "[227]\tvalidation_0-auc:0.90304\tvalidation_1-auc:0.89586\n",
      "[228]\tvalidation_0-auc:0.90307\tvalidation_1-auc:0.89588\n",
      "[229]\tvalidation_0-auc:0.90335\tvalidation_1-auc:0.89624\n",
      "[230]\tvalidation_0-auc:0.90354\tvalidation_1-auc:0.89638\n",
      "[231]\tvalidation_0-auc:0.90356\tvalidation_1-auc:0.89639\n",
      "[232]\tvalidation_0-auc:0.90361\tvalidation_1-auc:0.89640\n",
      "[233]\tvalidation_0-auc:0.90367\tvalidation_1-auc:0.89641\n",
      "[234]\tvalidation_0-auc:0.90370\tvalidation_1-auc:0.89643\n",
      "[235]\tvalidation_0-auc:0.90374\tvalidation_1-auc:0.89647\n",
      "[236]\tvalidation_0-auc:0.90386\tvalidation_1-auc:0.89659\n",
      "[237]\tvalidation_0-auc:0.90410\tvalidation_1-auc:0.89689\n",
      "[238]\tvalidation_0-auc:0.90424\tvalidation_1-auc:0.89699\n",
      "[239]\tvalidation_0-auc:0.90430\tvalidation_1-auc:0.89703\n",
      "[240]\tvalidation_0-auc:0.90434\tvalidation_1-auc:0.89708\n",
      "[241]\tvalidation_0-auc:0.90446\tvalidation_1-auc:0.89724\n",
      "[242]\tvalidation_0-auc:0.90462\tvalidation_1-auc:0.89736\n",
      "[243]\tvalidation_0-auc:0.90488\tvalidation_1-auc:0.89768\n",
      "[244]\tvalidation_0-auc:0.90492\tvalidation_1-auc:0.89771\n",
      "[245]\tvalidation_0-auc:0.90494\tvalidation_1-auc:0.89773\n",
      "[246]\tvalidation_0-auc:0.90499\tvalidation_1-auc:0.89774\n",
      "[247]\tvalidation_0-auc:0.90513\tvalidation_1-auc:0.89788\n",
      "[248]\tvalidation_0-auc:0.90516\tvalidation_1-auc:0.89790\n",
      "[249]\tvalidation_0-auc:0.90530\tvalidation_1-auc:0.89799\n",
      "[250]\tvalidation_0-auc:0.90533\tvalidation_1-auc:0.89799\n",
      "[251]\tvalidation_0-auc:0.90552\tvalidation_1-auc:0.89822\n",
      "[252]\tvalidation_0-auc:0.90563\tvalidation_1-auc:0.89835\n",
      "[253]\tvalidation_0-auc:0.90564\tvalidation_1-auc:0.89836\n",
      "[254]\tvalidation_0-auc:0.90566\tvalidation_1-auc:0.89839\n",
      "[255]\tvalidation_0-auc:0.90569\tvalidation_1-auc:0.89841\n",
      "[256]\tvalidation_0-auc:0.90584\tvalidation_1-auc:0.89850\n",
      "[257]\tvalidation_0-auc:0.90588\tvalidation_1-auc:0.89853\n",
      "[258]\tvalidation_0-auc:0.90602\tvalidation_1-auc:0.89870\n",
      "[259]\tvalidation_0-auc:0.90607\tvalidation_1-auc:0.89873\n",
      "[260]\tvalidation_0-auc:0.90608\tvalidation_1-auc:0.89875\n",
      "[261]\tvalidation_0-auc:0.90609\tvalidation_1-auc:0.89877\n",
      "[262]\tvalidation_0-auc:0.90629\tvalidation_1-auc:0.89885\n",
      "[263]\tvalidation_0-auc:0.90631\tvalidation_1-auc:0.89887\n",
      "[264]\tvalidation_0-auc:0.90637\tvalidation_1-auc:0.89891\n",
      "[265]\tvalidation_0-auc:0.90650\tvalidation_1-auc:0.89902\n",
      "[266]\tvalidation_0-auc:0.90652\tvalidation_1-auc:0.89905\n",
      "[267]\tvalidation_0-auc:0.90659\tvalidation_1-auc:0.89916\n",
      "[268]\tvalidation_0-auc:0.90674\tvalidation_1-auc:0.89931\n",
      "[269]\tvalidation_0-auc:0.90693\tvalidation_1-auc:0.89959\n",
      "[270]\tvalidation_0-auc:0.90695\tvalidation_1-auc:0.89960\n",
      "[271]\tvalidation_0-auc:0.90697\tvalidation_1-auc:0.89963\n",
      "[272]\tvalidation_0-auc:0.90705\tvalidation_1-auc:0.89971\n",
      "[273]\tvalidation_0-auc:0.90708\tvalidation_1-auc:0.89974\n",
      "[274]\tvalidation_0-auc:0.90710\tvalidation_1-auc:0.89976\n",
      "[275]\tvalidation_0-auc:0.90712\tvalidation_1-auc:0.89978\n",
      "[276]\tvalidation_0-auc:0.90749\tvalidation_1-auc:0.90024\n",
      "[277]\tvalidation_0-auc:0.90750\tvalidation_1-auc:0.90026\n",
      "[278]\tvalidation_0-auc:0.90765\tvalidation_1-auc:0.90048\n",
      "[279]\tvalidation_0-auc:0.90767\tvalidation_1-auc:0.90049\n",
      "[280]\tvalidation_0-auc:0.90770\tvalidation_1-auc:0.90052\n",
      "[281]\tvalidation_0-auc:0.90772\tvalidation_1-auc:0.90054\n",
      "[282]\tvalidation_0-auc:0.90777\tvalidation_1-auc:0.90057\n",
      "[283]\tvalidation_0-auc:0.90778\tvalidation_1-auc:0.90058\n",
      "[284]\tvalidation_0-auc:0.90787\tvalidation_1-auc:0.90070\n",
      "[285]\tvalidation_0-auc:0.90804\tvalidation_1-auc:0.90086\n",
      "[286]\tvalidation_0-auc:0.90806\tvalidation_1-auc:0.90088\n",
      "[287]\tvalidation_0-auc:0.90810\tvalidation_1-auc:0.90091\n",
      "[288]\tvalidation_0-auc:0.90813\tvalidation_1-auc:0.90095\n",
      "[289]\tvalidation_0-auc:0.90814\tvalidation_1-auc:0.90096\n",
      "[290]\tvalidation_0-auc:0.90827\tvalidation_1-auc:0.90107\n",
      "[291]\tvalidation_0-auc:0.90842\tvalidation_1-auc:0.90130\n",
      "[292]\tvalidation_0-auc:0.90844\tvalidation_1-auc:0.90135\n",
      "[293]\tvalidation_0-auc:0.90859\tvalidation_1-auc:0.90151\n",
      "[294]\tvalidation_0-auc:0.90863\tvalidation_1-auc:0.90151\n",
      "[295]\tvalidation_0-auc:0.90885\tvalidation_1-auc:0.90170\n",
      "[296]\tvalidation_0-auc:0.90895\tvalidation_1-auc:0.90177\n",
      "[297]\tvalidation_0-auc:0.90896\tvalidation_1-auc:0.90177\n",
      "[298]\tvalidation_0-auc:0.90902\tvalidation_1-auc:0.90184\n",
      "[299]\tvalidation_0-auc:0.90923\tvalidation_1-auc:0.90205\n",
      "[300]\tvalidation_0-auc:0.90937\tvalidation_1-auc:0.90229\n",
      "[301]\tvalidation_0-auc:0.90951\tvalidation_1-auc:0.90248\n",
      "[302]\tvalidation_0-auc:0.90958\tvalidation_1-auc:0.90253\n",
      "[303]\tvalidation_0-auc:0.90959\tvalidation_1-auc:0.90253\n",
      "[304]\tvalidation_0-auc:0.90971\tvalidation_1-auc:0.90262\n",
      "[305]\tvalidation_0-auc:0.90972\tvalidation_1-auc:0.90263\n",
      "[306]\tvalidation_0-auc:0.90973\tvalidation_1-auc:0.90264\n",
      "[307]\tvalidation_0-auc:0.90976\tvalidation_1-auc:0.90264\n",
      "[308]\tvalidation_0-auc:0.90979\tvalidation_1-auc:0.90266\n",
      "[309]\tvalidation_0-auc:0.90982\tvalidation_1-auc:0.90268\n",
      "[310]\tvalidation_0-auc:0.90983\tvalidation_1-auc:0.90269\n",
      "[311]\tvalidation_0-auc:0.90990\tvalidation_1-auc:0.90274\n",
      "[312]\tvalidation_0-auc:0.90990\tvalidation_1-auc:0.90275\n",
      "[313]\tvalidation_0-auc:0.90993\tvalidation_1-auc:0.90277\n",
      "[314]\tvalidation_0-auc:0.91004\tvalidation_1-auc:0.90283\n",
      "[315]\tvalidation_0-auc:0.91017\tvalidation_1-auc:0.90303\n",
      "[316]\tvalidation_0-auc:0.91023\tvalidation_1-auc:0.90304\n",
      "[317]\tvalidation_0-auc:0.91029\tvalidation_1-auc:0.90308\n",
      "[318]\tvalidation_0-auc:0.91031\tvalidation_1-auc:0.90309\n",
      "[319]\tvalidation_0-auc:0.91035\tvalidation_1-auc:0.90311\n",
      "[320]\tvalidation_0-auc:0.91038\tvalidation_1-auc:0.90316\n",
      "[321]\tvalidation_0-auc:0.91039\tvalidation_1-auc:0.90316\n",
      "[322]\tvalidation_0-auc:0.91040\tvalidation_1-auc:0.90317\n",
      "[323]\tvalidation_0-auc:0.91072\tvalidation_1-auc:0.90340\n",
      "[324]\tvalidation_0-auc:0.91089\tvalidation_1-auc:0.90366\n",
      "[325]\tvalidation_0-auc:0.91100\tvalidation_1-auc:0.90375\n",
      "[326]\tvalidation_0-auc:0.91101\tvalidation_1-auc:0.90377\n",
      "[327]\tvalidation_0-auc:0.91120\tvalidation_1-auc:0.90398\n",
      "[328]\tvalidation_0-auc:0.91130\tvalidation_1-auc:0.90410\n",
      "[329]\tvalidation_0-auc:0.91137\tvalidation_1-auc:0.90415\n",
      "[330]\tvalidation_0-auc:0.91143\tvalidation_1-auc:0.90426\n",
      "[331]\tvalidation_0-auc:0.91157\tvalidation_1-auc:0.90440\n",
      "[332]\tvalidation_0-auc:0.91159\tvalidation_1-auc:0.90442\n",
      "[333]\tvalidation_0-auc:0.91162\tvalidation_1-auc:0.90442\n",
      "[334]\tvalidation_0-auc:0.91164\tvalidation_1-auc:0.90444\n",
      "[335]\tvalidation_0-auc:0.91165\tvalidation_1-auc:0.90446\n",
      "[336]\tvalidation_0-auc:0.91167\tvalidation_1-auc:0.90447\n",
      "[337]\tvalidation_0-auc:0.91169\tvalidation_1-auc:0.90450\n",
      "[338]\tvalidation_0-auc:0.91189\tvalidation_1-auc:0.90470\n",
      "[339]\tvalidation_0-auc:0.91216\tvalidation_1-auc:0.90509\n",
      "[340]\tvalidation_0-auc:0.91220\tvalidation_1-auc:0.90511\n",
      "[341]\tvalidation_0-auc:0.91221\tvalidation_1-auc:0.90512\n",
      "[342]\tvalidation_0-auc:0.91236\tvalidation_1-auc:0.90530\n",
      "[343]\tvalidation_0-auc:0.91238\tvalidation_1-auc:0.90533\n",
      "[344]\tvalidation_0-auc:0.91247\tvalidation_1-auc:0.90539\n",
      "[345]\tvalidation_0-auc:0.91251\tvalidation_1-auc:0.90540\n",
      "[346]\tvalidation_0-auc:0.91252\tvalidation_1-auc:0.90544\n",
      "[347]\tvalidation_0-auc:0.91278\tvalidation_1-auc:0.90560\n",
      "[348]\tvalidation_0-auc:0.91281\tvalidation_1-auc:0.90562\n",
      "[349]\tvalidation_0-auc:0.91283\tvalidation_1-auc:0.90565\n",
      "[350]\tvalidation_0-auc:0.91316\tvalidation_1-auc:0.90592\n",
      "[351]\tvalidation_0-auc:0.91340\tvalidation_1-auc:0.90617\n",
      "[352]\tvalidation_0-auc:0.91347\tvalidation_1-auc:0.90627\n",
      "[353]\tvalidation_0-auc:0.91371\tvalidation_1-auc:0.90643\n",
      "[354]\tvalidation_0-auc:0.91398\tvalidation_1-auc:0.90669\n",
      "[355]\tvalidation_0-auc:0.91400\tvalidation_1-auc:0.90673\n",
      "[356]\tvalidation_0-auc:0.91401\tvalidation_1-auc:0.90674\n",
      "[357]\tvalidation_0-auc:0.91407\tvalidation_1-auc:0.90681\n",
      "[358]\tvalidation_0-auc:0.91431\tvalidation_1-auc:0.90707\n",
      "[359]\tvalidation_0-auc:0.91441\tvalidation_1-auc:0.90716\n",
      "[360]\tvalidation_0-auc:0.91444\tvalidation_1-auc:0.90718\n",
      "[361]\tvalidation_0-auc:0.91452\tvalidation_1-auc:0.90727\n",
      "[362]\tvalidation_0-auc:0.91455\tvalidation_1-auc:0.90731\n",
      "[363]\tvalidation_0-auc:0.91458\tvalidation_1-auc:0.90733\n",
      "[364]\tvalidation_0-auc:0.91460\tvalidation_1-auc:0.90733\n",
      "[365]\tvalidation_0-auc:0.91491\tvalidation_1-auc:0.90760\n",
      "[366]\tvalidation_0-auc:0.91504\tvalidation_1-auc:0.90775\n",
      "[367]\tvalidation_0-auc:0.91514\tvalidation_1-auc:0.90790\n",
      "[368]\tvalidation_0-auc:0.91534\tvalidation_1-auc:0.90823\n",
      "[369]\tvalidation_0-auc:0.91550\tvalidation_1-auc:0.90843\n",
      "[370]\tvalidation_0-auc:0.91554\tvalidation_1-auc:0.90846\n",
      "[371]\tvalidation_0-auc:0.91567\tvalidation_1-auc:0.90859\n",
      "[372]\tvalidation_0-auc:0.91591\tvalidation_1-auc:0.90879\n",
      "[373]\tvalidation_0-auc:0.91595\tvalidation_1-auc:0.90883\n",
      "[374]\tvalidation_0-auc:0.91595\tvalidation_1-auc:0.90884\n",
      "[375]\tvalidation_0-auc:0.91619\tvalidation_1-auc:0.90906\n",
      "[376]\tvalidation_0-auc:0.91620\tvalidation_1-auc:0.90906\n",
      "[377]\tvalidation_0-auc:0.91646\tvalidation_1-auc:0.90926\n",
      "[378]\tvalidation_0-auc:0.91669\tvalidation_1-auc:0.90949\n",
      "[379]\tvalidation_0-auc:0.91681\tvalidation_1-auc:0.90965\n",
      "[380]\tvalidation_0-auc:0.91701\tvalidation_1-auc:0.90996\n",
      "[381]\tvalidation_0-auc:0.91704\tvalidation_1-auc:0.91000\n",
      "[382]\tvalidation_0-auc:0.91704\tvalidation_1-auc:0.91001\n",
      "[383]\tvalidation_0-auc:0.91712\tvalidation_1-auc:0.91013\n",
      "[384]\tvalidation_0-auc:0.91713\tvalidation_1-auc:0.91014\n",
      "[385]\tvalidation_0-auc:0.91716\tvalidation_1-auc:0.91015\n",
      "[386]\tvalidation_0-auc:0.91738\tvalidation_1-auc:0.91044\n",
      "[387]\tvalidation_0-auc:0.91747\tvalidation_1-auc:0.91051\n",
      "[388]\tvalidation_0-auc:0.91762\tvalidation_1-auc:0.91072\n",
      "[389]\tvalidation_0-auc:0.91765\tvalidation_1-auc:0.91075\n",
      "[390]\tvalidation_0-auc:0.91768\tvalidation_1-auc:0.91076\n",
      "[391]\tvalidation_0-auc:0.91769\tvalidation_1-auc:0.91078\n",
      "[392]\tvalidation_0-auc:0.91771\tvalidation_1-auc:0.91080\n",
      "[393]\tvalidation_0-auc:0.91772\tvalidation_1-auc:0.91081\n",
      "[394]\tvalidation_0-auc:0.91778\tvalidation_1-auc:0.91088\n",
      "[395]\tvalidation_0-auc:0.91780\tvalidation_1-auc:0.91090\n",
      "[396]\tvalidation_0-auc:0.91795\tvalidation_1-auc:0.91104\n",
      "[397]\tvalidation_0-auc:0.91796\tvalidation_1-auc:0.91104\n",
      "[398]\tvalidation_0-auc:0.91800\tvalidation_1-auc:0.91110\n",
      "[399]\tvalidation_0-auc:0.91800\tvalidation_1-auc:0.91110\n",
      "[400]\tvalidation_0-auc:0.91801\tvalidation_1-auc:0.91111\n",
      "[401]\tvalidation_0-auc:0.91826\tvalidation_1-auc:0.91119\n",
      "[402]\tvalidation_0-auc:0.91834\tvalidation_1-auc:0.91131\n",
      "[403]\tvalidation_0-auc:0.91846\tvalidation_1-auc:0.91148\n",
      "[404]\tvalidation_0-auc:0.91852\tvalidation_1-auc:0.91159\n",
      "[405]\tvalidation_0-auc:0.91863\tvalidation_1-auc:0.91174\n",
      "[406]\tvalidation_0-auc:0.91872\tvalidation_1-auc:0.91186\n",
      "[407]\tvalidation_0-auc:0.91886\tvalidation_1-auc:0.91202\n",
      "[408]\tvalidation_0-auc:0.91896\tvalidation_1-auc:0.91212\n",
      "[409]\tvalidation_0-auc:0.91918\tvalidation_1-auc:0.91239\n",
      "[410]\tvalidation_0-auc:0.91938\tvalidation_1-auc:0.91252\n",
      "[411]\tvalidation_0-auc:0.91955\tvalidation_1-auc:0.91268\n",
      "[412]\tvalidation_0-auc:0.91956\tvalidation_1-auc:0.91268\n",
      "[413]\tvalidation_0-auc:0.91967\tvalidation_1-auc:0.91277\n",
      "[414]\tvalidation_0-auc:0.91975\tvalidation_1-auc:0.91284\n",
      "[415]\tvalidation_0-auc:0.91999\tvalidation_1-auc:0.91313\n",
      "[416]\tvalidation_0-auc:0.92000\tvalidation_1-auc:0.91314\n",
      "[417]\tvalidation_0-auc:0.92021\tvalidation_1-auc:0.91335\n",
      "[418]\tvalidation_0-auc:0.92022\tvalidation_1-auc:0.91335\n",
      "[419]\tvalidation_0-auc:0.92025\tvalidation_1-auc:0.91337\n",
      "[420]\tvalidation_0-auc:0.92038\tvalidation_1-auc:0.91351\n",
      "[421]\tvalidation_0-auc:0.92039\tvalidation_1-auc:0.91353\n",
      "[422]\tvalidation_0-auc:0.92052\tvalidation_1-auc:0.91365\n",
      "[423]\tvalidation_0-auc:0.92071\tvalidation_1-auc:0.91383\n",
      "[424]\tvalidation_0-auc:0.92083\tvalidation_1-auc:0.91396\n",
      "[425]\tvalidation_0-auc:0.92087\tvalidation_1-auc:0.91398\n",
      "[426]\tvalidation_0-auc:0.92088\tvalidation_1-auc:0.91398\n",
      "[427]\tvalidation_0-auc:0.92091\tvalidation_1-auc:0.91402\n",
      "[428]\tvalidation_0-auc:0.92093\tvalidation_1-auc:0.91403\n",
      "[429]\tvalidation_0-auc:0.92095\tvalidation_1-auc:0.91406\n",
      "[430]\tvalidation_0-auc:0.92100\tvalidation_1-auc:0.91409\n",
      "[431]\tvalidation_0-auc:0.92115\tvalidation_1-auc:0.91424\n",
      "[432]\tvalidation_0-auc:0.92128\tvalidation_1-auc:0.91437\n",
      "[433]\tvalidation_0-auc:0.92137\tvalidation_1-auc:0.91448\n",
      "[434]\tvalidation_0-auc:0.92139\tvalidation_1-auc:0.91450\n",
      "[435]\tvalidation_0-auc:0.92139\tvalidation_1-auc:0.91451\n",
      "[436]\tvalidation_0-auc:0.92140\tvalidation_1-auc:0.91451\n",
      "[437]\tvalidation_0-auc:0.92143\tvalidation_1-auc:0.91453\n",
      "[438]\tvalidation_0-auc:0.92148\tvalidation_1-auc:0.91459\n",
      "[439]\tvalidation_0-auc:0.92150\tvalidation_1-auc:0.91460\n",
      "[440]\tvalidation_0-auc:0.92152\tvalidation_1-auc:0.91462\n",
      "[441]\tvalidation_0-auc:0.92159\tvalidation_1-auc:0.91464\n",
      "[442]\tvalidation_0-auc:0.92178\tvalidation_1-auc:0.91488\n",
      "[443]\tvalidation_0-auc:0.92179\tvalidation_1-auc:0.91489\n",
      "[444]\tvalidation_0-auc:0.92190\tvalidation_1-auc:0.91495\n",
      "[445]\tvalidation_0-auc:0.92194\tvalidation_1-auc:0.91503\n",
      "[446]\tvalidation_0-auc:0.92201\tvalidation_1-auc:0.91509\n",
      "[447]\tvalidation_0-auc:0.92203\tvalidation_1-auc:0.91510\n",
      "[448]\tvalidation_0-auc:0.92212\tvalidation_1-auc:0.91523\n",
      "[449]\tvalidation_0-auc:0.92223\tvalidation_1-auc:0.91541\n",
      "[450]\tvalidation_0-auc:0.92235\tvalidation_1-auc:0.91551\n",
      "[451]\tvalidation_0-auc:0.92238\tvalidation_1-auc:0.91555\n",
      "[452]\tvalidation_0-auc:0.92239\tvalidation_1-auc:0.91558\n",
      "[453]\tvalidation_0-auc:0.92249\tvalidation_1-auc:0.91567\n",
      "[454]\tvalidation_0-auc:0.92250\tvalidation_1-auc:0.91568\n",
      "[455]\tvalidation_0-auc:0.92252\tvalidation_1-auc:0.91570\n",
      "[456]\tvalidation_0-auc:0.92255\tvalidation_1-auc:0.91576\n",
      "[457]\tvalidation_0-auc:0.92257\tvalidation_1-auc:0.91577\n",
      "[458]\tvalidation_0-auc:0.92257\tvalidation_1-auc:0.91577\n",
      "[459]\tvalidation_0-auc:0.92261\tvalidation_1-auc:0.91582\n",
      "[460]\tvalidation_0-auc:0.92272\tvalidation_1-auc:0.91595\n",
      "[461]\tvalidation_0-auc:0.92277\tvalidation_1-auc:0.91600\n",
      "[462]\tvalidation_0-auc:0.92279\tvalidation_1-auc:0.91601\n",
      "[463]\tvalidation_0-auc:0.92286\tvalidation_1-auc:0.91610\n",
      "[464]\tvalidation_0-auc:0.92290\tvalidation_1-auc:0.91613\n",
      "[465]\tvalidation_0-auc:0.92299\tvalidation_1-auc:0.91623\n",
      "[466]\tvalidation_0-auc:0.92300\tvalidation_1-auc:0.91623\n",
      "[467]\tvalidation_0-auc:0.92300\tvalidation_1-auc:0.91626\n",
      "[468]\tvalidation_0-auc:0.92309\tvalidation_1-auc:0.91633\n",
      "[469]\tvalidation_0-auc:0.92311\tvalidation_1-auc:0.91635\n",
      "[470]\tvalidation_0-auc:0.92313\tvalidation_1-auc:0.91637\n",
      "[471]\tvalidation_0-auc:0.92332\tvalidation_1-auc:0.91661\n",
      "[472]\tvalidation_0-auc:0.92346\tvalidation_1-auc:0.91683\n",
      "[473]\tvalidation_0-auc:0.92369\tvalidation_1-auc:0.91701\n",
      "[474]\tvalidation_0-auc:0.92376\tvalidation_1-auc:0.91709\n",
      "[475]\tvalidation_0-auc:0.92378\tvalidation_1-auc:0.91712\n",
      "[476]\tvalidation_0-auc:0.92389\tvalidation_1-auc:0.91731\n",
      "[477]\tvalidation_0-auc:0.92399\tvalidation_1-auc:0.91747\n",
      "[478]\tvalidation_0-auc:0.92428\tvalidation_1-auc:0.91769\n",
      "[479]\tvalidation_0-auc:0.92429\tvalidation_1-auc:0.91770\n",
      "[480]\tvalidation_0-auc:0.92432\tvalidation_1-auc:0.91777\n",
      "[481]\tvalidation_0-auc:0.92434\tvalidation_1-auc:0.91778\n",
      "[482]\tvalidation_0-auc:0.92442\tvalidation_1-auc:0.91791\n",
      "[483]\tvalidation_0-auc:0.92443\tvalidation_1-auc:0.91792\n",
      "[484]\tvalidation_0-auc:0.92449\tvalidation_1-auc:0.91803\n",
      "[485]\tvalidation_0-auc:0.92458\tvalidation_1-auc:0.91813\n",
      "[486]\tvalidation_0-auc:0.92474\tvalidation_1-auc:0.91824\n",
      "[487]\tvalidation_0-auc:0.92491\tvalidation_1-auc:0.91842\n",
      "[488]\tvalidation_0-auc:0.92497\tvalidation_1-auc:0.91850\n",
      "[489]\tvalidation_0-auc:0.92505\tvalidation_1-auc:0.91861\n",
      "[490]\tvalidation_0-auc:0.92511\tvalidation_1-auc:0.91873\n",
      "[491]\tvalidation_0-auc:0.92531\tvalidation_1-auc:0.91891\n",
      "[492]\tvalidation_0-auc:0.92534\tvalidation_1-auc:0.91900\n",
      "[493]\tvalidation_0-auc:0.92536\tvalidation_1-auc:0.91901\n",
      "[494]\tvalidation_0-auc:0.92539\tvalidation_1-auc:0.91907\n",
      "[495]\tvalidation_0-auc:0.92544\tvalidation_1-auc:0.91915\n",
      "[496]\tvalidation_0-auc:0.92545\tvalidation_1-auc:0.91916\n",
      "[497]\tvalidation_0-auc:0.92551\tvalidation_1-auc:0.91924\n",
      "[498]\tvalidation_0-auc:0.92558\tvalidation_1-auc:0.91929\n",
      "[499]\tvalidation_0-auc:0.92562\tvalidation_1-auc:0.91937\n",
      "ROC AUC: 0.8381\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "xgb_clf = XGBClassifier(n_estimators=500, learning_rate=0.05, random_state=156)\n",
    "\n",
    "# 학습 / early_stopping_rounds -> 조기중단 / 조기 중단은 검증 세트의 평가 지표가 지정된 라운드 동안 개선되지 않을 때 발생\n",
    "xgb_clf.fit(x_train, y_train, early_stopping_rounds=100,eval_metric='auc',\n",
    "           eval_set=[(x_tr,y_tr),(x_val,y_val)])\n",
    "\n",
    "# 예측 / 평가\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(x_test)[:,1])\n",
    "print(f'ROC AUC: {xgb_roc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검색 공간 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "xgb_search_space = {'max_depth': hp.quniform('max_depth',5,15,1),\n",
    "                   'min_child_weight':hp.quniform('min_child_weight',1,6,1),\n",
    "                   'colsample_bytree':hp.uniform('colsample_bytree',0.5,0.95),\n",
    "                   'learning_rate':hp.uniform('learning_rate',0.01,0.2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 목적 함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "def objective_func(search_space):\n",
    "    xgb_clf = XGBClassifier(n_estimators=100,\n",
    "                            max_depth= int(search_space['max_depth']),\n",
    "               min_child_weight= int(search_space['min_child_weight']),\n",
    "                    learning_rate=search_space['learning_rate'],\n",
    "               colsample_bytree=search_space['colsample_bytree'],\n",
    "                           eval_metric='logloss')\n",
    "    roc_auc = cross_val_score(xgb_clf, x_train, y_train, cv=3, scoring='roc_auc')\n",
    "\n",
    "    return {'loss': -1*np.mean(roc_auc), 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fmin()함수를 사용하여 최적 파라미터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 50/50 [22:51<00:00, 27.43s/trial, best loss: -0.83811339720573]\n",
      "best: {'colsample_bytree': 0.6989263607502518, 'learning_rate': 0.07116170670082439, 'max_depth': 5.0, 'min_child_weight': 6.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "trial_val = Trials()\n",
    "\n",
    "best = fmin(fn=objective_func, space=xgb_search_space,algo=tpe.suggest,\n",
    "           max_evals= 50,trials= trial_val,rstate= np.random.default_rng(seed=9))\n",
    "print(f'best: {best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>ROC_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.576711</td>\n",
       "      <td>0.033688</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.827991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704468</td>\n",
       "      <td>0.105956</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.836135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.913950</td>\n",
       "      <td>0.154804</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.830620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.905011</td>\n",
       "      <td>0.120686</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.832904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.656903</td>\n",
       "      <td>0.142392</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.817836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.827397</td>\n",
       "      <td>0.106579</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.832388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.911769</td>\n",
       "      <td>0.079111</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.827460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.675516</td>\n",
       "      <td>0.095213</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.822152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.665998</td>\n",
       "      <td>0.147520</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.828043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.582904</td>\n",
       "      <td>0.081179</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.834650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.603319</td>\n",
       "      <td>0.076255</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.837606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.749064</td>\n",
       "      <td>0.089624</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.835303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.513295</td>\n",
       "      <td>0.092214</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.834852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.904804</td>\n",
       "      <td>0.083983</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.821780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.883509</td>\n",
       "      <td>0.112477</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.828278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.563891</td>\n",
       "      <td>0.064663</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.837382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.846095</td>\n",
       "      <td>0.042766</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.835817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.543471</td>\n",
       "      <td>0.184028</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.810250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.869251</td>\n",
       "      <td>0.133006</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.833311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.529251</td>\n",
       "      <td>0.091771</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.831873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.608943</td>\n",
       "      <td>0.048157</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.756211</td>\n",
       "      <td>0.012234</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.831431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.610801</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.836858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.634851</td>\n",
       "      <td>0.013307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.827765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.500184</td>\n",
       "      <td>0.067249</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.830405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.554668</td>\n",
       "      <td>0.029037</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.831941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.719904</td>\n",
       "      <td>0.063466</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.837265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.769570</td>\n",
       "      <td>0.052350</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.833886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.575741</td>\n",
       "      <td>0.035658</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.828040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.616265</td>\n",
       "      <td>0.026915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.830771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.700335</td>\n",
       "      <td>0.070661</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.837587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.799441</td>\n",
       "      <td>0.174791</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.827900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.694514</td>\n",
       "      <td>0.128315</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.820476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.698926</td>\n",
       "      <td>0.071162</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.838113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.785720</td>\n",
       "      <td>0.111871</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.825951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.646177</td>\n",
       "      <td>0.019505</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.832543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.730507</td>\n",
       "      <td>0.102652</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.831877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.815545</td>\n",
       "      <td>0.168356</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.826919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.945090</td>\n",
       "      <td>0.076401</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.836101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.678046</td>\n",
       "      <td>0.121577</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.594846</td>\n",
       "      <td>0.100918</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.833907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.632943</td>\n",
       "      <td>0.057468</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.738995</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.837198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.669934</td>\n",
       "      <td>0.199747</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.828434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.524006</td>\n",
       "      <td>0.147743</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.820267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.711810</td>\n",
       "      <td>0.075597</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.833111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.683749</td>\n",
       "      <td>0.086250</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.821942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.653482</td>\n",
       "      <td>0.113407</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.832381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.582590</td>\n",
       "      <td>0.135491</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.832229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.841971</td>\n",
       "      <td>0.037399</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.836306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    colsample_bytree  learning_rate  max_depth  min_child_weight   ROC_AUC\n",
       "0           0.576711       0.033688       14.0               4.0  0.827991\n",
       "1           0.704468       0.105956        5.0               4.0  0.836135\n",
       "2           0.913950       0.154804        5.0               5.0  0.830620\n",
       "3           0.905011       0.120686        6.0               6.0  0.832904\n",
       "4           0.656903       0.142392       12.0               5.0  0.817836\n",
       "5           0.827397       0.106579        7.0               4.0  0.832388\n",
       "6           0.911769       0.079111       11.0               4.0  0.827460\n",
       "7           0.675516       0.095213       14.0               4.0  0.822152\n",
       "8           0.665998       0.147520        8.0               6.0  0.828043\n",
       "9           0.582904       0.081179        7.0               2.0  0.834650\n",
       "10          0.603319       0.076255        5.0               5.0  0.837606\n",
       "11          0.749064       0.089624        6.0               4.0  0.835303\n",
       "12          0.513295       0.092214        7.0               4.0  0.834852\n",
       "13          0.904804       0.083983       15.0               3.0  0.821780\n",
       "14          0.883509       0.112477        8.0               3.0  0.828278\n",
       "15          0.563891       0.064663        5.0               4.0  0.837382\n",
       "16          0.846095       0.042766        7.0               5.0  0.835817\n",
       "17          0.543471       0.184028       14.0               5.0  0.810250\n",
       "18          0.869251       0.133006        5.0               4.0  0.833311\n",
       "19          0.529251       0.091771        8.0               4.0  0.831873\n",
       "20          0.608943       0.048157       10.0               1.0  0.828777\n",
       "21          0.756211       0.012234        9.0               6.0  0.831431\n",
       "22          0.610801       0.060832        6.0               3.0  0.836858\n",
       "23          0.634851       0.013307        5.0               5.0  0.827765\n",
       "24          0.500184       0.067249       12.0               6.0  0.830405\n",
       "25          0.554668       0.029037        9.0               2.0  0.831941\n",
       "26          0.719904       0.063466        6.0               5.0  0.837265\n",
       "27          0.769570       0.052350        9.0               3.0  0.833886\n",
       "28          0.575741       0.035658       12.0               2.0  0.828040\n",
       "29          0.616265       0.026915       10.0               5.0  0.830771\n",
       "30          0.700335       0.070661        5.0               6.0  0.837587\n",
       "31          0.799441       0.174791        6.0               6.0  0.827900\n",
       "32          0.694514       0.128315       13.0               6.0  0.820476\n",
       "33          0.698926       0.071162        5.0               6.0  0.838113\n",
       "34          0.785720       0.111871       11.0               6.0  0.825951\n",
       "35          0.646177       0.019505        6.0               5.0  0.832543\n",
       "36          0.730507       0.102652        8.0               5.0  0.831877\n",
       "37          0.815545       0.168356        7.0               6.0  0.826919\n",
       "38          0.945090       0.076401        5.0               5.0  0.836101\n",
       "39          0.678046       0.121577       11.0               1.0  0.818352\n",
       "40          0.594846       0.100918        7.0               6.0  0.833907\n",
       "41          0.632943       0.057468        8.0               5.0  0.835600\n",
       "42          0.738995       0.044238        5.0               6.0  0.837198\n",
       "43          0.669934       0.199747        6.0               5.0  0.828434\n",
       "44          0.524006       0.147743       13.0               6.0  0.820267\n",
       "45          0.711810       0.075597        9.0               4.0  0.833111\n",
       "46          0.683749       0.086250       15.0               3.0  0.821942\n",
       "47          0.653482       0.113407        7.0               4.0  0.832381\n",
       "48          0.582590       0.135491        6.0               5.0  0.832229\n",
       "49          0.841971       0.037399        5.0               5.0  0.836306"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc = [loss_dict['loss']*(-1) for loss_dict in trial_val.results]\n",
    "result_df = pd.DataFrame(trial_val.vals)\n",
    "result_df['ROC_AUC'] = roc_auc\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적으로 찾은 하이퍼파라미터로 학습과 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_clf = XGBClassifier(n_estimators=500,\n",
    "                            max_depth= int(best['max_depth']),\n",
    "               min_child_weight= int(best['min_child_weight']),\n",
    "                    learning_rate=round(best['learning_rate'],5),\n",
    "               colsample_bytree=round(best['colsample_bytree'],5))\n",
    "\n",
    "xgb_best_clf.fit(x_tr, y_tr, early_stopping_rounds=100,\n",
    "                eval_metric='auc', eval_set=[(x_tr,y_tr),(x_val, y_val)]) \n",
    "\n",
    "pred_proba = xgb_best_clf.predict_proba(x_test)[:,1]\n",
    "print(f'ROC_AUC: {roc_auc_score(y_test, pred_proba):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피처 중요도 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "plot_importance(xgb_clf, ax=ax, max_num_features=20, height=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 목적 함수 설정\n",
    "\n",
    "- 조기 중단을 위해 KFold 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold를 이용하여 조기중단 적용하기\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def objective_func2(search_space):\n",
    "    xgb_clf = XGBClassifier(n_estimators=100,\n",
    "                            max_depth= int(search_space['max_depth']),\n",
    "               min_child_weight= int(search_space['min_child_weight']),\n",
    "                    learning_rate=search_space['learning_rate'],\n",
    "               colsample_bytree=search_space['colsample_bytree'])\n",
    "    scores = []\n",
    "    kf = KFold(n_splits=3)\n",
    "    for tr_idx, val_idx in kf.split(x_train):\n",
    "        x_tr, y_tr = x_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "        x_val, y_val = x_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "        xgb_clf.fit(x_tr, y_tr, eval_metric='auc', eval_set=[(x_tr,y_tr),(x_val,y_val)],\n",
    "                   early_stopping_rounds=30)\n",
    "        score = roc_auc_score(y_val, xgb_clf.predict_proba(x_val)[:,1])\n",
    "        scores.append(score)\n",
    "        \n",
    "    return -1 * np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.76925\tvalidation_1-auc:0.73368                                                                  \n",
      "[1]\tvalidation_0-auc:0.77931\tvalidation_1-auc:0.74099                                                                  \n",
      "[2]\tvalidation_0-auc:0.78206\tvalidation_1-auc:0.74516                                                                  \n",
      "[3]\tvalidation_0-auc:0.82632\tvalidation_1-auc:0.79329                                                                  \n",
      "[4]\tvalidation_0-auc:0.82410\tvalidation_1-auc:0.79134                                                                  \n",
      "[5]\tvalidation_0-auc:0.83679\tvalidation_1-auc:0.80348                                                                  \n",
      "[6]\tvalidation_0-auc:0.84272\tvalidation_1-auc:0.80982                                                                  \n",
      "[7]\tvalidation_0-auc:0.84774\tvalidation_1-auc:0.81687                                                                  \n",
      "[8]\tvalidation_0-auc:0.84653\tvalidation_1-auc:0.81238                                                                  \n",
      "[9]\tvalidation_0-auc:0.85115\tvalidation_1-auc:0.81855                                                                  \n",
      "[10]\tvalidation_0-auc:0.85084\tvalidation_1-auc:0.81767                                                                 \n",
      "[11]\tvalidation_0-auc:0.85443\tvalidation_1-auc:0.82027                                                                 \n",
      "[12]\tvalidation_0-auc:0.85277\tvalidation_1-auc:0.81820                                                                 \n",
      "[13]\tvalidation_0-auc:0.85278\tvalidation_1-auc:0.81682                                                                 \n",
      "[14]\tvalidation_0-auc:0.85604\tvalidation_1-auc:0.82069                                                                 \n",
      "[15]\tvalidation_0-auc:0.85590\tvalidation_1-auc:0.81956                                                                 \n",
      "[16]\tvalidation_0-auc:0.85906\tvalidation_1-auc:0.82247                                                                 \n",
      "[17]\tvalidation_0-auc:0.85852\tvalidation_1-auc:0.82120                                                                 \n",
      "[18]\tvalidation_0-auc:0.86150\tvalidation_1-auc:0.82497                                                                 \n",
      "[19]\tvalidation_0-auc:0.86325\tvalidation_1-auc:0.82700                                                                 \n",
      "[20]\tvalidation_0-auc:0.86564\tvalidation_1-auc:0.82877                                                                 \n",
      "[21]\tvalidation_0-auc:0.86539\tvalidation_1-auc:0.82758                                                                 \n",
      "[22]\tvalidation_0-auc:0.86756\tvalidation_1-auc:0.82920                                                                 \n",
      "[23]\tvalidation_0-auc:0.86735\tvalidation_1-auc:0.82831                                                                 \n",
      "[24]\tvalidation_0-auc:0.86883\tvalidation_1-auc:0.82944                                                                 \n",
      "[25]\tvalidation_0-auc:0.87003\tvalidation_1-auc:0.83050                                                                 \n",
      "[26]\tvalidation_0-auc:0.87133\tvalidation_1-auc:0.83189                                                                 \n",
      "[27]\tvalidation_0-auc:0.87245\tvalidation_1-auc:0.83363                                                                 \n",
      "[28]\tvalidation_0-auc:0.87314\tvalidation_1-auc:0.83343                                                                 \n",
      "[29]\tvalidation_0-auc:0.87420\tvalidation_1-auc:0.83290                                                                 \n",
      "[30]\tvalidation_0-auc:0.87454\tvalidation_1-auc:0.83237                                                                 \n",
      "[31]\tvalidation_0-auc:0.87577\tvalidation_1-auc:0.83352                                                                 \n",
      "[32]\tvalidation_0-auc:0.87699\tvalidation_1-auc:0.83429                                                                 \n",
      "[33]\tvalidation_0-auc:0.87796\tvalidation_1-auc:0.83390                                                                 \n",
      "[34]\tvalidation_0-auc:0.87893\tvalidation_1-auc:0.83436                                                                 \n",
      "[35]\tvalidation_0-auc:0.87983\tvalidation_1-auc:0.83465                                                                 \n",
      "[36]\tvalidation_0-auc:0.88062\tvalidation_1-auc:0.83493                                                                 \n",
      "[37]\tvalidation_0-auc:0.88172\tvalidation_1-auc:0.83490                                                                 \n",
      "[38]\tvalidation_0-auc:0.88263\tvalidation_1-auc:0.83465                                                                 \n",
      "[39]\tvalidation_0-auc:0.88311\tvalidation_1-auc:0.83443                                                                 \n",
      "[40]\tvalidation_0-auc:0.88361\tvalidation_1-auc:0.83458                                                                 \n",
      "[41]\tvalidation_0-auc:0.88405\tvalidation_1-auc:0.83485                                                                 \n",
      "[42]\tvalidation_0-auc:0.88479\tvalidation_1-auc:0.83431                                                                 \n",
      "[43]\tvalidation_0-auc:0.88506\tvalidation_1-auc:0.83448                                                                 \n",
      "[44]\tvalidation_0-auc:0.88566\tvalidation_1-auc:0.83470                                                                 \n",
      "[45]\tvalidation_0-auc:0.88596\tvalidation_1-auc:0.83475                                                                 \n",
      "[46]\tvalidation_0-auc:0.88637\tvalidation_1-auc:0.83473                                                                 \n",
      "[47]\tvalidation_0-auc:0.88684\tvalidation_1-auc:0.83456                                                                 \n",
      "[48]\tvalidation_0-auc:0.88724\tvalidation_1-auc:0.83434                                                                 \n",
      "[49]\tvalidation_0-auc:0.88761\tvalidation_1-auc:0.83435                                                                 \n",
      "[50]\tvalidation_0-auc:0.88835\tvalidation_1-auc:0.83463                                                                 \n",
      "[51]\tvalidation_0-auc:0.88882\tvalidation_1-auc:0.83484                                                                 \n",
      "[52]\tvalidation_0-auc:0.88902\tvalidation_1-auc:0.83472                                                                 \n",
      "[53]\tvalidation_0-auc:0.88917\tvalidation_1-auc:0.83463                                                                 \n",
      "[54]\tvalidation_0-auc:0.88945\tvalidation_1-auc:0.83458                                                                 \n",
      "[55]\tvalidation_0-auc:0.88954\tvalidation_1-auc:0.83462                                                                 \n",
      "[56]\tvalidation_0-auc:0.88980\tvalidation_1-auc:0.83456                                                                 \n",
      "[57]\tvalidation_0-auc:0.88999\tvalidation_1-auc:0.83452                                                                 \n",
      "[58]\tvalidation_0-auc:0.89041\tvalidation_1-auc:0.83446                                                                 \n",
      "[59]\tvalidation_0-auc:0.89063\tvalidation_1-auc:0.83446                                                                 \n",
      "[60]\tvalidation_0-auc:0.89103\tvalidation_1-auc:0.83448                                                                 \n",
      "[61]\tvalidation_0-auc:0.89113\tvalidation_1-auc:0.83437                                                                 \n",
      "[62]\tvalidation_0-auc:0.89131\tvalidation_1-auc:0.83444                                                                 \n",
      "[63]\tvalidation_0-auc:0.89184\tvalidation_1-auc:0.83418                                                                 \n",
      "[64]\tvalidation_0-auc:0.89190\tvalidation_1-auc:0.83424                                                                 \n",
      "[65]\tvalidation_0-auc:0.89236\tvalidation_1-auc:0.83395                                                                 \n",
      "[66]\tvalidation_0-auc:0.89248\tvalidation_1-auc:0.83406                                                                 \n",
      "[0]\tvalidation_0-auc:0.76909\tvalidation_1-auc:0.74196                                                                  \n",
      "[1]\tvalidation_0-auc:0.77588\tvalidation_1-auc:0.74488                                                                  \n",
      "[2]\tvalidation_0-auc:0.78379\tvalidation_1-auc:0.74875                                                                  \n",
      "[3]\tvalidation_0-auc:0.82915\tvalidation_1-auc:0.79699                                                                  \n",
      "[4]\tvalidation_0-auc:0.82930\tvalidation_1-auc:0.79394                                                                  \n",
      "[5]\tvalidation_0-auc:0.84020\tvalidation_1-auc:0.80805                                                                  \n",
      "[6]\tvalidation_0-auc:0.84502\tvalidation_1-auc:0.81382                                                                  \n",
      "[7]\tvalidation_0-auc:0.84943\tvalidation_1-auc:0.81869                                                                  \n",
      "[8]\tvalidation_0-auc:0.84924\tvalidation_1-auc:0.81616                                                                  \n",
      "[9]\tvalidation_0-auc:0.85312\tvalidation_1-auc:0.82108                                                                  \n",
      "[10]\tvalidation_0-auc:0.85400\tvalidation_1-auc:0.81725                                                                 \n",
      "[11]\tvalidation_0-auc:0.85678\tvalidation_1-auc:0.81981                                                                 \n",
      "[12]\tvalidation_0-auc:0.85582\tvalidation_1-auc:0.81852                                                                 \n",
      "[13]\tvalidation_0-auc:0.85542\tvalidation_1-auc:0.81589                                                                 \n",
      "[14]\tvalidation_0-auc:0.85890\tvalidation_1-auc:0.81930                                                                 \n",
      "[15]\tvalidation_0-auc:0.85802\tvalidation_1-auc:0.81814                                                                 \n",
      "[16]\tvalidation_0-auc:0.86113\tvalidation_1-auc:0.82094                                                                 \n",
      "[17]\tvalidation_0-auc:0.86089\tvalidation_1-auc:0.82023                                                                 \n",
      "[18]\tvalidation_0-auc:0.86337\tvalidation_1-auc:0.82243                                                                 \n",
      "[19]\tvalidation_0-auc:0.86480\tvalidation_1-auc:0.82399                                                                 \n",
      "[20]\tvalidation_0-auc:0.86712\tvalidation_1-auc:0.82546                                                                 \n",
      "[21]\tvalidation_0-auc:0.86714\tvalidation_1-auc:0.82422                                                                 \n",
      "[22]\tvalidation_0-auc:0.86939\tvalidation_1-auc:0.82648                                                                 \n",
      "[23]\tvalidation_0-auc:0.86962\tvalidation_1-auc:0.82520                                                                 \n",
      "[24]\tvalidation_0-auc:0.87147\tvalidation_1-auc:0.82720                                                                 \n",
      "[25]\tvalidation_0-auc:0.87304\tvalidation_1-auc:0.82874                                                                 \n",
      "[26]\tvalidation_0-auc:0.87415\tvalidation_1-auc:0.82977                                                                 \n",
      "[27]\tvalidation_0-auc:0.87560\tvalidation_1-auc:0.83086                                                                 \n",
      "[28]\tvalidation_0-auc:0.87598\tvalidation_1-auc:0.83060                                                                 \n",
      "[29]\tvalidation_0-auc:0.87677\tvalidation_1-auc:0.83025                                                                 \n",
      "[30]\tvalidation_0-auc:0.87710\tvalidation_1-auc:0.82940                                                                 \n",
      "[31]\tvalidation_0-auc:0.87815\tvalidation_1-auc:0.83034                                                                 \n",
      "[32]\tvalidation_0-auc:0.87920\tvalidation_1-auc:0.83107                                                                 \n",
      "[33]\tvalidation_0-auc:0.87941\tvalidation_1-auc:0.83077                                                                 \n",
      "[34]\tvalidation_0-auc:0.88070\tvalidation_1-auc:0.83169                                                                 \n",
      "[35]\tvalidation_0-auc:0.88210\tvalidation_1-auc:0.83250                                                                 \n",
      "[36]\tvalidation_0-auc:0.88343\tvalidation_1-auc:0.83318                                                                 \n",
      "[37]\tvalidation_0-auc:0.88420\tvalidation_1-auc:0.83360                                                                 \n",
      "[38]\tvalidation_0-auc:0.88495\tvalidation_1-auc:0.83386                                                                 \n",
      "[39]\tvalidation_0-auc:0.88588\tvalidation_1-auc:0.83356                                                                 \n",
      "[40]\tvalidation_0-auc:0.88669\tvalidation_1-auc:0.83377                                                                 \n",
      "[41]\tvalidation_0-auc:0.88704\tvalidation_1-auc:0.83415                                                                 \n",
      "[42]\tvalidation_0-auc:0.88752\tvalidation_1-auc:0.83448                                                                 \n",
      "[43]\tvalidation_0-auc:0.88797\tvalidation_1-auc:0.83465                                                                 \n",
      "[44]\tvalidation_0-auc:0.88845\tvalidation_1-auc:0.83472                                                                 \n",
      "[45]\tvalidation_0-auc:0.88898\tvalidation_1-auc:0.83510                                                                 \n",
      "[46]\tvalidation_0-auc:0.88929\tvalidation_1-auc:0.83541                                                                 \n",
      "[47]\tvalidation_0-auc:0.88995\tvalidation_1-auc:0.83538                                                                 \n",
      "[48]\tvalidation_0-auc:0.89044\tvalidation_1-auc:0.83563                                                                 \n",
      "[49]\tvalidation_0-auc:0.89071\tvalidation_1-auc:0.83586                                                                 \n",
      "[50]\tvalidation_0-auc:0.89094\tvalidation_1-auc:0.83605                                                                 \n",
      "[51]\tvalidation_0-auc:0.89138\tvalidation_1-auc:0.83602                                                                 \n",
      "[52]\tvalidation_0-auc:0.89205\tvalidation_1-auc:0.83599                                                                 \n",
      "[53]\tvalidation_0-auc:0.89227\tvalidation_1-auc:0.83629                                                                 \n",
      "[54]\tvalidation_0-auc:0.89255\tvalidation_1-auc:0.83635                                                                 \n",
      "[55]\tvalidation_0-auc:0.89294\tvalidation_1-auc:0.83624                                                                 \n",
      "[56]\tvalidation_0-auc:0.89353\tvalidation_1-auc:0.83602                                                                 \n",
      "[57]\tvalidation_0-auc:0.89386\tvalidation_1-auc:0.83600                                                                 \n",
      "[58]\tvalidation_0-auc:0.89455\tvalidation_1-auc:0.83614                                                                 \n",
      "[59]\tvalidation_0-auc:0.89487\tvalidation_1-auc:0.83610                                                                 \n",
      "[60]\tvalidation_0-auc:0.89516\tvalidation_1-auc:0.83629                                                                 \n",
      "[61]\tvalidation_0-auc:0.89528\tvalidation_1-auc:0.83634                                                                 \n",
      "[62]\tvalidation_0-auc:0.89542\tvalidation_1-auc:0.83635                                                                 \n",
      "[63]\tvalidation_0-auc:0.89573\tvalidation_1-auc:0.83640                                                                 \n",
      "[64]\tvalidation_0-auc:0.89586\tvalidation_1-auc:0.83651                                                                 \n",
      "[65]\tvalidation_0-auc:0.89613\tvalidation_1-auc:0.83626                                                                 \n",
      "[66]\tvalidation_0-auc:0.89632\tvalidation_1-auc:0.83626                                                                 \n",
      "[67]\tvalidation_0-auc:0.89654\tvalidation_1-auc:0.83630                                                                 \n",
      "[68]\tvalidation_0-auc:0.89684\tvalidation_1-auc:0.83614                                                                 \n",
      "[69]\tvalidation_0-auc:0.89735\tvalidation_1-auc:0.83622                                                                 \n",
      "[70]\tvalidation_0-auc:0.89761\tvalidation_1-auc:0.83636                                                                 \n",
      "[71]\tvalidation_0-auc:0.89787\tvalidation_1-auc:0.83633                                                                 \n",
      "[72]\tvalidation_0-auc:0.89815\tvalidation_1-auc:0.83629                                                                 \n",
      "[73]\tvalidation_0-auc:0.89841\tvalidation_1-auc:0.83617                                                                 \n",
      "[74]\tvalidation_0-auc:0.89866\tvalidation_1-auc:0.83619                                                                 \n",
      "[75]\tvalidation_0-auc:0.89878\tvalidation_1-auc:0.83624                                                                 \n",
      "[76]\tvalidation_0-auc:0.89894\tvalidation_1-auc:0.83634                                                                 \n",
      "[77]\tvalidation_0-auc:0.89913\tvalidation_1-auc:0.83630                                                                 \n",
      "[78]\tvalidation_0-auc:0.89951\tvalidation_1-auc:0.83627                                                                 \n",
      "[79]\tvalidation_0-auc:0.89957\tvalidation_1-auc:0.83630                                                                 \n",
      "[80]\tvalidation_0-auc:0.90039\tvalidation_1-auc:0.83625                                                                 \n",
      "[81]\tvalidation_0-auc:0.90066\tvalidation_1-auc:0.83623                                                                 \n",
      "[82]\tvalidation_0-auc:0.90084\tvalidation_1-auc:0.83615                                                                 \n",
      "[83]\tvalidation_0-auc:0.90115\tvalidation_1-auc:0.83604                                                                 \n",
      "[84]\tvalidation_0-auc:0.90121\tvalidation_1-auc:0.83602                                                                 \n",
      "[85]\tvalidation_0-auc:0.90121\tvalidation_1-auc:0.83608                                                                 \n",
      "[86]\tvalidation_0-auc:0.90131\tvalidation_1-auc:0.83616                                                                 \n",
      "[87]\tvalidation_0-auc:0.90155\tvalidation_1-auc:0.83606                                                                 \n",
      "[88]\tvalidation_0-auc:0.90164\tvalidation_1-auc:0.83594                                                                 \n",
      "[89]\tvalidation_0-auc:0.90183\tvalidation_1-auc:0.83587                                                                 \n",
      "[90]\tvalidation_0-auc:0.90197\tvalidation_1-auc:0.83581                                                                 \n",
      "[91]\tvalidation_0-auc:0.90242\tvalidation_1-auc:0.83576                                                                 \n",
      "[92]\tvalidation_0-auc:0.90256\tvalidation_1-auc:0.83582                                                                 \n",
      "[93]\tvalidation_0-auc:0.90267\tvalidation_1-auc:0.83582                                                                 \n",
      "[94]\tvalidation_0-auc:0.90294\tvalidation_1-auc:0.83580                                                                 \n",
      "[0]\tvalidation_0-auc:0.77283\tvalidation_1-auc:0.75372                                                                  \n",
      "[1]\tvalidation_0-auc:0.77933\tvalidation_1-auc:0.75940                                                                  \n",
      "[2]\tvalidation_0-auc:0.78395\tvalidation_1-auc:0.76293                                                                  \n",
      "[3]\tvalidation_0-auc:0.83011\tvalidation_1-auc:0.79852                                                                  \n",
      "[4]\tvalidation_0-auc:0.82457\tvalidation_1-auc:0.79414                                                                  \n",
      "[5]\tvalidation_0-auc:0.83791\tvalidation_1-auc:0.80653                                                                  \n",
      "[6]\tvalidation_0-auc:0.84532\tvalidation_1-auc:0.81308                                                                  \n",
      "[7]\tvalidation_0-auc:0.84890\tvalidation_1-auc:0.81579                                                                  \n",
      "[8]\tvalidation_0-auc:0.84863\tvalidation_1-auc:0.81621                                                                  \n",
      "[9]\tvalidation_0-auc:0.85372\tvalidation_1-auc:0.81853                                                                  \n",
      "[10]\tvalidation_0-auc:0.85363\tvalidation_1-auc:0.81827                                                                 \n",
      "[11]\tvalidation_0-auc:0.85727\tvalidation_1-auc:0.82133                                                                 \n",
      "[12]\tvalidation_0-auc:0.85634\tvalidation_1-auc:0.82163                                                                 \n",
      "[13]\tvalidation_0-auc:0.85625\tvalidation_1-auc:0.82026                                                                 \n",
      "[14]\tvalidation_0-auc:0.85972\tvalidation_1-auc:0.82277                                                                 \n",
      "[15]\tvalidation_0-auc:0.85947\tvalidation_1-auc:0.82228                                                                 \n",
      "[16]\tvalidation_0-auc:0.86179\tvalidation_1-auc:0.82463                                                                 \n",
      "[17]\tvalidation_0-auc:0.86166\tvalidation_1-auc:0.82444                                                                 \n",
      "[18]\tvalidation_0-auc:0.86395\tvalidation_1-auc:0.82568                                                                 \n",
      "[19]\tvalidation_0-auc:0.86573\tvalidation_1-auc:0.82740                                                                 \n",
      "[20]\tvalidation_0-auc:0.86763\tvalidation_1-auc:0.82924                                                                 \n",
      "[21]\tvalidation_0-auc:0.86774\tvalidation_1-auc:0.82845                                                                 \n",
      "[22]\tvalidation_0-auc:0.86928\tvalidation_1-auc:0.82981                                                                 \n",
      "[23]\tvalidation_0-auc:0.86924\tvalidation_1-auc:0.82933                                                                 \n",
      "[24]\tvalidation_0-auc:0.87097\tvalidation_1-auc:0.83130                                                                 \n",
      "[25]\tvalidation_0-auc:0.87282\tvalidation_1-auc:0.83266                                                                 \n",
      "[26]\tvalidation_0-auc:0.87472\tvalidation_1-auc:0.83358                                                                 \n",
      "[27]\tvalidation_0-auc:0.87557\tvalidation_1-auc:0.83432                                                                 \n",
      "[28]\tvalidation_0-auc:0.87599\tvalidation_1-auc:0.83367                                                                 \n",
      "[29]\tvalidation_0-auc:0.87638\tvalidation_1-auc:0.83298                                                                 \n",
      "[30]\tvalidation_0-auc:0.87687\tvalidation_1-auc:0.83249                                                                 \n",
      "[31]\tvalidation_0-auc:0.87809\tvalidation_1-auc:0.83319                                                                 \n",
      "[32]\tvalidation_0-auc:0.87913\tvalidation_1-auc:0.83408                                                                 \n",
      "[33]\tvalidation_0-auc:0.87951\tvalidation_1-auc:0.83351                                                                 \n",
      "[34]\tvalidation_0-auc:0.88034\tvalidation_1-auc:0.83416                                                                 \n",
      "[35]\tvalidation_0-auc:0.88116\tvalidation_1-auc:0.83430                                                                 \n",
      "[36]\tvalidation_0-auc:0.88188\tvalidation_1-auc:0.83447                                                                 \n",
      "[37]\tvalidation_0-auc:0.88245\tvalidation_1-auc:0.83496                                                                 \n",
      "[38]\tvalidation_0-auc:0.88363\tvalidation_1-auc:0.83537                                                                 \n",
      "[39]\tvalidation_0-auc:0.88429\tvalidation_1-auc:0.83477                                                                 \n",
      "[40]\tvalidation_0-auc:0.88476\tvalidation_1-auc:0.83547                                                                 \n",
      "[41]\tvalidation_0-auc:0.88519\tvalidation_1-auc:0.83593                                                                 \n",
      "[42]\tvalidation_0-auc:0.88557\tvalidation_1-auc:0.83598                                                                 \n",
      "[43]\tvalidation_0-auc:0.88605\tvalidation_1-auc:0.83629                                                                 \n",
      "[44]\tvalidation_0-auc:0.88661\tvalidation_1-auc:0.83658                                                                 \n",
      "[45]\tvalidation_0-auc:0.88714\tvalidation_1-auc:0.83675                                                                 \n",
      "  0%|                                                                           | 0/50 [01:08<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# kfold를 이용하여 조기중단 적용하기\u001b[39;00m\n\u001b[0;32m      2\u001b[0m trial_val2 \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[1;32m----> 4\u001b[0m best2 \u001b[38;5;241m=\u001b[39m fmin(fn\u001b[38;5;241m=\u001b[39mobjective_func2, space\u001b[38;5;241m=\u001b[39mxgb_search_space,algo\u001b[38;5;241m=\u001b[39mtpe\u001b[38;5;241m.\u001b[39msuggest,\n\u001b[0;32m      5\u001b[0m            max_evals\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,trials\u001b[38;5;241m=\u001b[39m trial_val2,rstate\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m조기중단 적용 best: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hyperopt\\fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trials\u001b[38;5;241m.\u001b[39mfmin(\n\u001b[0;32m    541\u001b[0m         fn,\n\u001b[0;32m    542\u001b[0m         space,\n\u001b[0;32m    543\u001b[0m         algo\u001b[38;5;241m=\u001b[39malgo,\n\u001b[0;32m    544\u001b[0m         max_evals\u001b[38;5;241m=\u001b[39mmax_evals,\n\u001b[0;32m    545\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    546\u001b[0m         loss_threshold\u001b[38;5;241m=\u001b[39mloss_threshold,\n\u001b[0;32m    547\u001b[0m         max_queue_len\u001b[38;5;241m=\u001b[39mmax_queue_len,\n\u001b[0;32m    548\u001b[0m         rstate\u001b[38;5;241m=\u001b[39mrstate,\n\u001b[0;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[38;5;241m=\u001b[39mpass_expr_memo_ctrl,\n\u001b[0;32m    550\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    551\u001b[0m         catch_eval_exceptions\u001b[38;5;241m=\u001b[39mcatch_eval_exceptions,\n\u001b[0;32m    552\u001b[0m         return_argmin\u001b[38;5;241m=\u001b[39mreturn_argmin,\n\u001b[0;32m    553\u001b[0m         show_progressbar\u001b[38;5;241m=\u001b[39mshow_progressbar,\n\u001b[0;32m    554\u001b[0m         early_stop_fn\u001b[38;5;241m=\u001b[39mearly_stop_fn,\n\u001b[0;32m    555\u001b[0m         trials_save_file\u001b[38;5;241m=\u001b[39mtrials_save_file,\n\u001b[0;32m    556\u001b[0m     )\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hyperopt\\base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fmin(\n\u001b[0;32m    672\u001b[0m     fn,\n\u001b[0;32m    673\u001b[0m     space,\n\u001b[0;32m    674\u001b[0m     algo\u001b[38;5;241m=\u001b[39malgo,\n\u001b[0;32m    675\u001b[0m     max_evals\u001b[38;5;241m=\u001b[39mmax_evals,\n\u001b[0;32m    676\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    677\u001b[0m     loss_threshold\u001b[38;5;241m=\u001b[39mloss_threshold,\n\u001b[0;32m    678\u001b[0m     trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    679\u001b[0m     rstate\u001b[38;5;241m=\u001b[39mrstate,\n\u001b[0;32m    680\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    681\u001b[0m     max_queue_len\u001b[38;5;241m=\u001b[39mmax_queue_len,\n\u001b[0;32m    682\u001b[0m     allow_trials_fmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# -- prevent recursion\u001b[39;00m\n\u001b[0;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[38;5;241m=\u001b[39mpass_expr_memo_ctrl,\n\u001b[0;32m    684\u001b[0m     catch_eval_exceptions\u001b[38;5;241m=\u001b[39mcatch_eval_exceptions,\n\u001b[0;32m    685\u001b[0m     return_argmin\u001b[38;5;241m=\u001b[39mreturn_argmin,\n\u001b[0;32m    686\u001b[0m     show_progressbar\u001b[38;5;241m=\u001b[39mshow_progressbar,\n\u001b[0;32m    687\u001b[0m     early_stop_fn\u001b[38;5;241m=\u001b[39mearly_stop_fn,\n\u001b[0;32m    688\u001b[0m     trials_save_file\u001b[38;5;241m=\u001b[39mtrials_save_file,\n\u001b[0;32m    689\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m rval\u001b[38;5;241m.\u001b[39mexhaust()\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_evals \u001b[38;5;241m-\u001b[39m n_done, block_until_done\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masynchronous)\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserial_evaluate()\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mevaluate(spec, ctrl)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(pyll_rval)\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[1;32mIn[59], line 15\u001b[0m, in \u001b[0;36mobjective_func2\u001b[1;34m(search_space)\u001b[0m\n\u001b[0;32m     13\u001b[0m x_tr, y_tr \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39miloc[tr_idx], y_train\u001b[38;5;241m.\u001b[39miloc[tr_idx]\n\u001b[0;32m     14\u001b[0m x_val, y_val \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39miloc[val_idx], y_train\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[1;32m---> 15\u001b[0m xgb_clf\u001b[38;5;241m.\u001b[39mfit(x_tr, y_tr, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m, eval_set\u001b[38;5;241m=\u001b[39m[(x_tr,y_tr),(x_val,y_val)],\n\u001b[0;32m     16\u001b[0m            early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     17\u001b[0m score \u001b[38;5;241m=\u001b[39m roc_auc_score(y_val, xgb_clf\u001b[38;5;241m.\u001b[39mpredict_proba(x_val)[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     18\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\sklearn.py:1519\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1491\u001b[0m (\n\u001b[0;32m   1492\u001b[0m     model,\n\u001b[0;32m   1493\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1499\u001b[0m )\n\u001b[0;32m   1500\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1501\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1502\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1517\u001b[0m )\n\u001b[1;32m-> 1519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1520\u001b[0m     params,\n\u001b[0;32m   1521\u001b[0m     train_dmatrix,\n\u001b[0;32m   1522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[0;32m   1523\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[0;32m   1524\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds,\n\u001b[0;32m   1525\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[0;32m   1526\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[0;32m   1527\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m   1528\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1529\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1530\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1531\u001b[0m )\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    185\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mafter_training(bst)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:238\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[1;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m name\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset name should not contain `-`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 238\u001b[0m score: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval_set(evals, epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_margin)\n\u001b[0;32m    239\u001b[0m metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:2126\u001b[0m, in \u001b[0;36mBooster.eval_set\u001b[1;34m(self, evals, iteration, feval, output_margin)\u001b[0m\n\u001b[0;32m   2123\u001b[0m evnames \u001b[38;5;241m=\u001b[39m c_array(ctypes\u001b[38;5;241m.\u001b[39mc_char_p, [c_str(d[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evals])\n\u001b[0;32m   2124\u001b[0m msg \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p()\n\u001b[0;32m   2125\u001b[0m _check_call(\n\u001b[1;32m-> 2126\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mXGBoosterEvalOneIter(\n\u001b[0;32m   2127\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   2128\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration),\n\u001b[0;32m   2129\u001b[0m         dmats,\n\u001b[0;32m   2130\u001b[0m         evnames,\n\u001b[0;32m   2131\u001b[0m         c_bst_ulong(\u001b[38;5;28mlen\u001b[39m(evals)),\n\u001b[0;32m   2132\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(msg),\n\u001b[0;32m   2133\u001b[0m     )\n\u001b[0;32m   2134\u001b[0m )\n\u001b[0;32m   2135\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m msg\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m res \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mdecode()  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# kfold를 이용하여 조기중단 적용하기\n",
    "trial_val2 = Trials()\n",
    "\n",
    "best2 = fmin(fn=objective_func2, space=xgb_search_space,algo=tpe.suggest,\n",
    "           max_evals= 50,trials= trial_val2,rstate= np.random.default_rng(seed=30))\n",
    "print(f'조기중단 적용 best: {best2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = [loss_dict['loss']*(-1) for loss_dict in trial_val.results]\n",
    "result_df2 = pd.DataFrame(trial_val2.vals)\n",
    "result_df2['ROC_AUC'] = roc_auc\n",
    "result_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(best2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최적으로 찾은 하이퍼파라미터로 학습과 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.75616\tvalidation_1-auc:0.72423\n",
      "[1]\tvalidation_0-auc:0.76454\tvalidation_1-auc:0.73411\n",
      "[2]\tvalidation_0-auc:0.77096\tvalidation_1-auc:0.74296\n",
      "[3]\tvalidation_0-auc:0.81583\tvalidation_1-auc:0.79552\n",
      "[4]\tvalidation_0-auc:0.81450\tvalidation_1-auc:0.79190\n",
      "[5]\tvalidation_0-auc:0.82393\tvalidation_1-auc:0.80398\n",
      "[6]\tvalidation_0-auc:0.82918\tvalidation_1-auc:0.81072\n",
      "[7]\tvalidation_0-auc:0.83345\tvalidation_1-auc:0.81515\n",
      "[8]\tvalidation_0-auc:0.83204\tvalidation_1-auc:0.81335\n",
      "[9]\tvalidation_0-auc:0.83725\tvalidation_1-auc:0.81638\n",
      "[10]\tvalidation_0-auc:0.83566\tvalidation_1-auc:0.81427\n",
      "[11]\tvalidation_0-auc:0.83985\tvalidation_1-auc:0.81786\n",
      "[12]\tvalidation_0-auc:0.83782\tvalidation_1-auc:0.81511\n",
      "[13]\tvalidation_0-auc:0.83709\tvalidation_1-auc:0.81494\n",
      "[14]\tvalidation_0-auc:0.84035\tvalidation_1-auc:0.81903\n",
      "[15]\tvalidation_0-auc:0.83945\tvalidation_1-auc:0.81775\n",
      "[16]\tvalidation_0-auc:0.84203\tvalidation_1-auc:0.82006\n",
      "[17]\tvalidation_0-auc:0.84198\tvalidation_1-auc:0.81887\n",
      "[18]\tvalidation_0-auc:0.84488\tvalidation_1-auc:0.82239\n",
      "[19]\tvalidation_0-auc:0.84667\tvalidation_1-auc:0.82423\n",
      "[20]\tvalidation_0-auc:0.84897\tvalidation_1-auc:0.82692\n",
      "[21]\tvalidation_0-auc:0.84835\tvalidation_1-auc:0.82582\n",
      "[22]\tvalidation_0-auc:0.85007\tvalidation_1-auc:0.82799\n",
      "[23]\tvalidation_0-auc:0.85067\tvalidation_1-auc:0.82752\n",
      "[24]\tvalidation_0-auc:0.85248\tvalidation_1-auc:0.82963\n",
      "[25]\tvalidation_0-auc:0.85360\tvalidation_1-auc:0.83026\n",
      "[26]\tvalidation_0-auc:0.85478\tvalidation_1-auc:0.83077\n",
      "[27]\tvalidation_0-auc:0.85600\tvalidation_1-auc:0.83134\n",
      "[28]\tvalidation_0-auc:0.85598\tvalidation_1-auc:0.83120\n",
      "[29]\tvalidation_0-auc:0.85592\tvalidation_1-auc:0.83074\n",
      "[30]\tvalidation_0-auc:0.85576\tvalidation_1-auc:0.83070\n",
      "[31]\tvalidation_0-auc:0.85720\tvalidation_1-auc:0.83154\n",
      "[32]\tvalidation_0-auc:0.85830\tvalidation_1-auc:0.83212\n",
      "[33]\tvalidation_0-auc:0.85821\tvalidation_1-auc:0.83183\n",
      "[34]\tvalidation_0-auc:0.85917\tvalidation_1-auc:0.83262\n",
      "[35]\tvalidation_0-auc:0.86005\tvalidation_1-auc:0.83301\n",
      "[36]\tvalidation_0-auc:0.86185\tvalidation_1-auc:0.83375\n",
      "[37]\tvalidation_0-auc:0.86247\tvalidation_1-auc:0.83366\n",
      "[38]\tvalidation_0-auc:0.86302\tvalidation_1-auc:0.83364\n",
      "[39]\tvalidation_0-auc:0.86321\tvalidation_1-auc:0.83349\n",
      "[40]\tvalidation_0-auc:0.86393\tvalidation_1-auc:0.83378\n",
      "[41]\tvalidation_0-auc:0.86448\tvalidation_1-auc:0.83387\n",
      "[42]\tvalidation_0-auc:0.86466\tvalidation_1-auc:0.83414\n",
      "[43]\tvalidation_0-auc:0.86531\tvalidation_1-auc:0.83403\n",
      "[44]\tvalidation_0-auc:0.86616\tvalidation_1-auc:0.83432\n",
      "[45]\tvalidation_0-auc:0.86654\tvalidation_1-auc:0.83485\n",
      "[46]\tvalidation_0-auc:0.86688\tvalidation_1-auc:0.83491\n",
      "[47]\tvalidation_0-auc:0.86732\tvalidation_1-auc:0.83514\n",
      "[48]\tvalidation_0-auc:0.86750\tvalidation_1-auc:0.83527\n",
      "[49]\tvalidation_0-auc:0.86792\tvalidation_1-auc:0.83537\n",
      "[50]\tvalidation_0-auc:0.86818\tvalidation_1-auc:0.83558\n",
      "[51]\tvalidation_0-auc:0.86848\tvalidation_1-auc:0.83561\n",
      "[52]\tvalidation_0-auc:0.86874\tvalidation_1-auc:0.83583\n",
      "[53]\tvalidation_0-auc:0.86914\tvalidation_1-auc:0.83598\n",
      "[54]\tvalidation_0-auc:0.86944\tvalidation_1-auc:0.83592\n",
      "[55]\tvalidation_0-auc:0.86981\tvalidation_1-auc:0.83603\n",
      "[56]\tvalidation_0-auc:0.87011\tvalidation_1-auc:0.83600\n",
      "[57]\tvalidation_0-auc:0.87033\tvalidation_1-auc:0.83603\n",
      "[58]\tvalidation_0-auc:0.87055\tvalidation_1-auc:0.83598\n",
      "[59]\tvalidation_0-auc:0.87094\tvalidation_1-auc:0.83599\n",
      "[60]\tvalidation_0-auc:0.87108\tvalidation_1-auc:0.83599\n",
      "[61]\tvalidation_0-auc:0.87123\tvalidation_1-auc:0.83619\n",
      "[62]\tvalidation_0-auc:0.87150\tvalidation_1-auc:0.83607\n",
      "[63]\tvalidation_0-auc:0.87173\tvalidation_1-auc:0.83621\n",
      "[64]\tvalidation_0-auc:0.87189\tvalidation_1-auc:0.83612\n",
      "[65]\tvalidation_0-auc:0.87214\tvalidation_1-auc:0.83619\n",
      "[66]\tvalidation_0-auc:0.87226\tvalidation_1-auc:0.83619\n",
      "[67]\tvalidation_0-auc:0.87252\tvalidation_1-auc:0.83638\n",
      "[68]\tvalidation_0-auc:0.87269\tvalidation_1-auc:0.83660\n",
      "[69]\tvalidation_0-auc:0.87283\tvalidation_1-auc:0.83652\n",
      "[70]\tvalidation_0-auc:0.87309\tvalidation_1-auc:0.83654\n",
      "[71]\tvalidation_0-auc:0.87349\tvalidation_1-auc:0.83662\n",
      "[72]\tvalidation_0-auc:0.87372\tvalidation_1-auc:0.83667\n",
      "[73]\tvalidation_0-auc:0.87395\tvalidation_1-auc:0.83673\n",
      "[74]\tvalidation_0-auc:0.87415\tvalidation_1-auc:0.83680\n",
      "[75]\tvalidation_0-auc:0.87430\tvalidation_1-auc:0.83674\n",
      "[76]\tvalidation_0-auc:0.87451\tvalidation_1-auc:0.83666\n",
      "[77]\tvalidation_0-auc:0.87463\tvalidation_1-auc:0.83671\n",
      "[78]\tvalidation_0-auc:0.87488\tvalidation_1-auc:0.83682\n",
      "[79]\tvalidation_0-auc:0.87496\tvalidation_1-auc:0.83681\n",
      "[80]\tvalidation_0-auc:0.87511\tvalidation_1-auc:0.83694\n",
      "[81]\tvalidation_0-auc:0.87531\tvalidation_1-auc:0.83699\n",
      "[82]\tvalidation_0-auc:0.87540\tvalidation_1-auc:0.83707\n",
      "[83]\tvalidation_0-auc:0.87561\tvalidation_1-auc:0.83714\n",
      "[84]\tvalidation_0-auc:0.87572\tvalidation_1-auc:0.83714\n",
      "[85]\tvalidation_0-auc:0.87581\tvalidation_1-auc:0.83705\n",
      "[86]\tvalidation_0-auc:0.87596\tvalidation_1-auc:0.83713\n",
      "[87]\tvalidation_0-auc:0.87617\tvalidation_1-auc:0.83718\n",
      "[88]\tvalidation_0-auc:0.87628\tvalidation_1-auc:0.83727\n",
      "[89]\tvalidation_0-auc:0.87659\tvalidation_1-auc:0.83709\n",
      "[90]\tvalidation_0-auc:0.87687\tvalidation_1-auc:0.83703\n",
      "[91]\tvalidation_0-auc:0.87703\tvalidation_1-auc:0.83720\n",
      "[92]\tvalidation_0-auc:0.87727\tvalidation_1-auc:0.83719\n",
      "[93]\tvalidation_0-auc:0.87743\tvalidation_1-auc:0.83709\n",
      "[94]\tvalidation_0-auc:0.87751\tvalidation_1-auc:0.83713\n",
      "[95]\tvalidation_0-auc:0.87755\tvalidation_1-auc:0.83713\n",
      "[96]\tvalidation_0-auc:0.87800\tvalidation_1-auc:0.83725\n",
      "[97]\tvalidation_0-auc:0.87811\tvalidation_1-auc:0.83725\n",
      "[98]\tvalidation_0-auc:0.87828\tvalidation_1-auc:0.83731\n",
      "[99]\tvalidation_0-auc:0.87837\tvalidation_1-auc:0.83736\n",
      "[100]\tvalidation_0-auc:0.87842\tvalidation_1-auc:0.83735\n",
      "[101]\tvalidation_0-auc:0.87863\tvalidation_1-auc:0.83731\n",
      "[102]\tvalidation_0-auc:0.87873\tvalidation_1-auc:0.83735\n",
      "[103]\tvalidation_0-auc:0.87890\tvalidation_1-auc:0.83747\n",
      "[104]\tvalidation_0-auc:0.87900\tvalidation_1-auc:0.83747\n",
      "[105]\tvalidation_0-auc:0.87908\tvalidation_1-auc:0.83741\n",
      "[106]\tvalidation_0-auc:0.87921\tvalidation_1-auc:0.83739\n",
      "[107]\tvalidation_0-auc:0.87941\tvalidation_1-auc:0.83731\n",
      "[108]\tvalidation_0-auc:0.87942\tvalidation_1-auc:0.83729\n",
      "[109]\tvalidation_0-auc:0.87956\tvalidation_1-auc:0.83732\n",
      "[110]\tvalidation_0-auc:0.87962\tvalidation_1-auc:0.83733\n",
      "[111]\tvalidation_0-auc:0.87971\tvalidation_1-auc:0.83729\n",
      "[112]\tvalidation_0-auc:0.87980\tvalidation_1-auc:0.83743\n",
      "[113]\tvalidation_0-auc:0.88011\tvalidation_1-auc:0.83742\n",
      "[114]\tvalidation_0-auc:0.88038\tvalidation_1-auc:0.83737\n",
      "[115]\tvalidation_0-auc:0.88045\tvalidation_1-auc:0.83737\n",
      "[116]\tvalidation_0-auc:0.88065\tvalidation_1-auc:0.83729\n",
      "[117]\tvalidation_0-auc:0.88089\tvalidation_1-auc:0.83718\n",
      "[118]\tvalidation_0-auc:0.88096\tvalidation_1-auc:0.83717\n",
      "[119]\tvalidation_0-auc:0.88112\tvalidation_1-auc:0.83711\n",
      "[120]\tvalidation_0-auc:0.88118\tvalidation_1-auc:0.83711\n",
      "[121]\tvalidation_0-auc:0.88127\tvalidation_1-auc:0.83709\n",
      "[122]\tvalidation_0-auc:0.88139\tvalidation_1-auc:0.83711\n",
      "[123]\tvalidation_0-auc:0.88154\tvalidation_1-auc:0.83711\n",
      "[124]\tvalidation_0-auc:0.88174\tvalidation_1-auc:0.83715\n",
      "[125]\tvalidation_0-auc:0.88195\tvalidation_1-auc:0.83709\n",
      "[126]\tvalidation_0-auc:0.88210\tvalidation_1-auc:0.83704\n",
      "[127]\tvalidation_0-auc:0.88214\tvalidation_1-auc:0.83707\n",
      "[128]\tvalidation_0-auc:0.88232\tvalidation_1-auc:0.83697\n",
      "[129]\tvalidation_0-auc:0.88241\tvalidation_1-auc:0.83685\n",
      "[130]\tvalidation_0-auc:0.88257\tvalidation_1-auc:0.83689\n",
      "[131]\tvalidation_0-auc:0.88269\tvalidation_1-auc:0.83692\n",
      "[132]\tvalidation_0-auc:0.88290\tvalidation_1-auc:0.83678\n",
      "[133]\tvalidation_0-auc:0.88296\tvalidation_1-auc:0.83677\n",
      "[134]\tvalidation_0-auc:0.88299\tvalidation_1-auc:0.83684\n",
      "[135]\tvalidation_0-auc:0.88312\tvalidation_1-auc:0.83672\n",
      "[136]\tvalidation_0-auc:0.88317\tvalidation_1-auc:0.83670\n",
      "[137]\tvalidation_0-auc:0.88360\tvalidation_1-auc:0.83660\n",
      "[138]\tvalidation_0-auc:0.88400\tvalidation_1-auc:0.83658\n",
      "[139]\tvalidation_0-auc:0.88413\tvalidation_1-auc:0.83647\n",
      "[140]\tvalidation_0-auc:0.88425\tvalidation_1-auc:0.83631\n",
      "[141]\tvalidation_0-auc:0.88432\tvalidation_1-auc:0.83622\n",
      "[142]\tvalidation_0-auc:0.88439\tvalidation_1-auc:0.83622\n",
      "[143]\tvalidation_0-auc:0.88453\tvalidation_1-auc:0.83621\n",
      "[144]\tvalidation_0-auc:0.88491\tvalidation_1-auc:0.83615\n",
      "[145]\tvalidation_0-auc:0.88505\tvalidation_1-auc:0.83605\n",
      "[146]\tvalidation_0-auc:0.88506\tvalidation_1-auc:0.83603\n",
      "[147]\tvalidation_0-auc:0.88536\tvalidation_1-auc:0.83592\n",
      "[148]\tvalidation_0-auc:0.88539\tvalidation_1-auc:0.83595\n",
      "[149]\tvalidation_0-auc:0.88569\tvalidation_1-auc:0.83585\n",
      "[150]\tvalidation_0-auc:0.88578\tvalidation_1-auc:0.83587\n",
      "[151]\tvalidation_0-auc:0.88584\tvalidation_1-auc:0.83587\n",
      "[152]\tvalidation_0-auc:0.88594\tvalidation_1-auc:0.83593\n",
      "[153]\tvalidation_0-auc:0.88621\tvalidation_1-auc:0.83594\n",
      "[154]\tvalidation_0-auc:0.88627\tvalidation_1-auc:0.83593\n",
      "[155]\tvalidation_0-auc:0.88648\tvalidation_1-auc:0.83600\n",
      "[156]\tvalidation_0-auc:0.88662\tvalidation_1-auc:0.83600\n",
      "[157]\tvalidation_0-auc:0.88669\tvalidation_1-auc:0.83591\n",
      "[158]\tvalidation_0-auc:0.88702\tvalidation_1-auc:0.83607\n",
      "[159]\tvalidation_0-auc:0.88705\tvalidation_1-auc:0.83603\n",
      "[160]\tvalidation_0-auc:0.88735\tvalidation_1-auc:0.83584\n",
      "[161]\tvalidation_0-auc:0.88742\tvalidation_1-auc:0.83581\n",
      "[162]\tvalidation_0-auc:0.88741\tvalidation_1-auc:0.83582\n",
      "[163]\tvalidation_0-auc:0.88748\tvalidation_1-auc:0.83582\n",
      "[164]\tvalidation_0-auc:0.88779\tvalidation_1-auc:0.83564\n",
      "[165]\tvalidation_0-auc:0.88783\tvalidation_1-auc:0.83554\n",
      "[166]\tvalidation_0-auc:0.88785\tvalidation_1-auc:0.83552\n",
      "[167]\tvalidation_0-auc:0.88804\tvalidation_1-auc:0.83549\n",
      "[168]\tvalidation_0-auc:0.88814\tvalidation_1-auc:0.83544\n",
      "[169]\tvalidation_0-auc:0.88815\tvalidation_1-auc:0.83542\n",
      "[170]\tvalidation_0-auc:0.88835\tvalidation_1-auc:0.83550\n",
      "[171]\tvalidation_0-auc:0.88844\tvalidation_1-auc:0.83537\n",
      "[172]\tvalidation_0-auc:0.88849\tvalidation_1-auc:0.83533\n",
      "[173]\tvalidation_0-auc:0.88859\tvalidation_1-auc:0.83529\n",
      "[174]\tvalidation_0-auc:0.88898\tvalidation_1-auc:0.83533\n",
      "[175]\tvalidation_0-auc:0.88904\tvalidation_1-auc:0.83533\n",
      "[176]\tvalidation_0-auc:0.88916\tvalidation_1-auc:0.83525\n",
      "[177]\tvalidation_0-auc:0.88938\tvalidation_1-auc:0.83521\n",
      "[178]\tvalidation_0-auc:0.88949\tvalidation_1-auc:0.83527\n",
      "[179]\tvalidation_0-auc:0.88965\tvalidation_1-auc:0.83520\n",
      "[180]\tvalidation_0-auc:0.88965\tvalidation_1-auc:0.83518\n",
      "[181]\tvalidation_0-auc:0.88988\tvalidation_1-auc:0.83509\n",
      "[182]\tvalidation_0-auc:0.89015\tvalidation_1-auc:0.83494\n",
      "[183]\tvalidation_0-auc:0.89057\tvalidation_1-auc:0.83494\n",
      "[184]\tvalidation_0-auc:0.89082\tvalidation_1-auc:0.83500\n",
      "[185]\tvalidation_0-auc:0.89102\tvalidation_1-auc:0.83489\n",
      "[186]\tvalidation_0-auc:0.89121\tvalidation_1-auc:0.83493\n",
      "[187]\tvalidation_0-auc:0.89126\tvalidation_1-auc:0.83491\n",
      "[188]\tvalidation_0-auc:0.89146\tvalidation_1-auc:0.83481\n",
      "[189]\tvalidation_0-auc:0.89164\tvalidation_1-auc:0.83494\n",
      "[190]\tvalidation_0-auc:0.89186\tvalidation_1-auc:0.83493\n",
      "[191]\tvalidation_0-auc:0.89208\tvalidation_1-auc:0.83484\n",
      "[192]\tvalidation_0-auc:0.89210\tvalidation_1-auc:0.83481\n",
      "[193]\tvalidation_0-auc:0.89215\tvalidation_1-auc:0.83475\n",
      "[194]\tvalidation_0-auc:0.89225\tvalidation_1-auc:0.83477\n",
      "[195]\tvalidation_0-auc:0.89236\tvalidation_1-auc:0.83477\n",
      "[196]\tvalidation_0-auc:0.89263\tvalidation_1-auc:0.83473\n",
      "[197]\tvalidation_0-auc:0.89271\tvalidation_1-auc:0.83471\n",
      "[198]\tvalidation_0-auc:0.89277\tvalidation_1-auc:0.83473\n",
      "[199]\tvalidation_0-auc:0.89285\tvalidation_1-auc:0.83475\n",
      "[200]\tvalidation_0-auc:0.89290\tvalidation_1-auc:0.83475\n",
      "[201]\tvalidation_0-auc:0.89304\tvalidation_1-auc:0.83465\n",
      "[202]\tvalidation_0-auc:0.89309\tvalidation_1-auc:0.83467\n",
      "ROC_AUC: 0.8461\n"
     ]
    }
   ],
   "source": [
    "xgb_best_clf2 = XGBClassifier(n_estimators=500,\n",
    "                            max_depth= int(best['max_depth']),\n",
    "               min_child_weight= int(best['min_child_weight']),\n",
    "                    learning_rate=round(best['learning_rate'],5),\n",
    "               colsample_bytree=round(best['colsample_bytree'],5))\n",
    "\n",
    "xgb_best_clf2.fit(x_tr, y_tr, early_stopping_rounds=100,\n",
    "                eval_metric='auc', eval_set=[(x_tr,y_tr),(x_val, y_val)]) \n",
    "\n",
    "pred_proba2 = xgb_best_clf2.predict_proba(x_test)[:,1]\n",
    "print(f'ROC_AUC: {roc_auc_score(y_test, pred_proba2):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 피처 중요도 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "plot_importance(xgb_clf, ax=ax, max_num_features=20, height=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM 모델 학습과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1658, number of negative: 40913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13308\n",
      "[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 242\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038947 -> initscore=-3.205836\n",
      "[LightGBM] [Info] Start training from score -3.205836\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's auc: 0.91059\ttraining's binary_logloss: 0.112183\tvalid_1's auc: 0.831787\tvalid_1's binary_logloss: 0.13527\n",
      "ROC_AUC: 0.8384\n"
     ]
    }
   ],
   "source": [
    "# x_train을 x_tr과 x_val로 나누는 이유 -> tr로 학습을 하면서 val로 조기중단 조건이 되는지 확인\n",
    "# -> 조기중단을 하기 위한 조건이 되는지 확인하기 위해 _val이 필요하다.\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=500)\n",
    "\n",
    "lgbm_clf.fit(x_tr, y_tr, callbacks=[early_stopping(stopping_rounds=100)],\n",
    "            eval_metric='auc', eval_set=[(x_tr,y_tr),(x_val,y_val)])\n",
    "\n",
    "lgbm_roc_auc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(x_test)[:,1])\n",
    "print(f'ROC_AUC: {lgbm_roc_auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검색 공간 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_search_space = {'max_depth': hp.quniform('max_depth',100, 160, 1),\n",
    "                   'min_child_samples':hp.quniform('min_child_samples', 60, 100, 1),\n",
    "                   'num_leaves':hp.quniform('num_leaves',32, 64, 1),\n",
    "                   'subsample':hp.uniform('subsample', 0.7, 1),\n",
    "                    'learning_rate':hp.uniform('learning_rate', 0.01, 0.2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 목적 함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func_lgbm(search_space):\n",
    "    lgbm_clf = LGBMClassifier(n_estimators=100, \n",
    "                              max_depth= int(search_space['max_depth']),\n",
    "                           min_child_samples= int(search_space['min_child_samples']),\n",
    "                           num_leaves= int(search_space['num_leaves']),\n",
    "                           subsample= search_space['subsample'],\n",
    "                            learning_rate= search_space['learning_rate'])\n",
    "\n",
    "    scores = []\n",
    "    kf = KFold(n_splits=3)\n",
    "    for tr_idx, val_idx in kf.split(x_train):\n",
    "        x_tr, y_tr = x_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "        x_val, y_val = x_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "        lgbm_clf.fit(x_tr, y_tr, eval_metric='auc', eval_set=[(x_tr,y_tr),(x_val,y_val)],\n",
    "                   callbacks=[early_stopping(stopping_rounds=30)])\n",
    "        score = roc_auc_score(y_val, lgbm_clf.predict_proba(x_val)[:,1])\n",
    "        scores.append(score)\n",
    "        \n",
    "    return -1 * np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fmin()함수로 최적 파라미터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073289 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[98]\ttraining's auc: 0.88765\ttraining's binary_logloss: 0.121808\tvalid_1's auc: 0.831519\tvalid_1's binary_logloss: 0.135868\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067777 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13008                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[32]\ttraining's auc: 0.860623\ttraining's binary_logloss: 0.137851\tvalid_1's auc: 0.835706\tvalid_1's binary_logloss: 0.137481\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111624 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[98]\ttraining's auc: 0.885445\ttraining's binary_logloss: 0.121577\tvalid_1's auc: 0.837538\tvalid_1's binary_logloss: 0.136446\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090153 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12885                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[8]\ttraining's auc: 0.876014\ttraining's binary_logloss: 0.128648\tvalid_1's auc: 0.830458\tvalid_1's binary_logloss: 0.139332\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086250 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13008                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[9]\ttraining's auc: 0.876004\ttraining's binary_logloss: 0.129519\tvalid_1's auc: 0.835219\tvalid_1's binary_logloss: 0.134005\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083614 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12915                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[17]\ttraining's auc: 0.897685\ttraining's binary_logloss: 0.117316\tvalid_1's auc: 0.832056\tvalid_1's binary_logloss: 0.137878\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086099 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[21]\ttraining's auc: 0.880303\ttraining's binary_logloss: 0.126597\tvalid_1's auc: 0.830448\tvalid_1's binary_logloss: 0.137992\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101266 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[47]\ttraining's auc: 0.903653\ttraining's binary_logloss: 0.11734\tvalid_1's auc: 0.837037\tvalid_1's binary_logloss: 0.130606\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109877 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[37]\ttraining's auc: 0.896492\ttraining's binary_logloss: 0.118202\tvalid_1's auc: 0.836576\tvalid_1's binary_logloss: 0.136794\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124143 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12895                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[15]\ttraining's auc: 0.891713\ttraining's binary_logloss: 0.122226\tvalid_1's auc: 0.830818\tvalid_1's binary_logloss: 0.137184\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081328 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13046                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[38]\ttraining's auc: 0.92456\ttraining's binary_logloss: 0.108961\tvalid_1's auc: 0.836448\tvalid_1's binary_logloss: 0.130846\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089723 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12915                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[13]\ttraining's auc: 0.886101\ttraining's binary_logloss: 0.123542\tvalid_1's auc: 0.832306\tvalid_1's binary_logloss: 0.139844\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081909 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[46]\ttraining's auc: 0.905084\ttraining's binary_logloss: 0.116011\tvalid_1's auc: 0.832359\tvalid_1's binary_logloss: 0.135612\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095950 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[46]\ttraining's auc: 0.902133\ttraining's binary_logloss: 0.118038\tvalid_1's auc: 0.835118\tvalid_1's binary_logloss: 0.130885\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077373 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[26]\ttraining's auc: 0.884765\ttraining's binary_logloss: 0.12408\tvalid_1's auc: 0.836094\tvalid_1's binary_logloss: 0.138359\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074844 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[98]\ttraining's auc: 0.875162\ttraining's binary_logloss: 0.127906\tvalid_1's auc: 0.83007\tvalid_1's binary_logloss: 0.137948\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075873 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13008                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[58]\ttraining's auc: 0.863996\ttraining's binary_logloss: 0.136878\tvalid_1's auc: 0.835763\tvalid_1's binary_logloss: 0.137074\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101767 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\ttraining's auc: 0.875291\ttraining's binary_logloss: 0.127136\tvalid_1's auc: 0.836234\tvalid_1's binary_logloss: 0.138892\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133322 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[14]\ttraining's auc: 0.883983\ttraining's binary_logloss: 0.122742\tvalid_1's auc: 0.829439\tvalid_1's binary_logloss: 0.136732\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083886 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[23]\ttraining's auc: 0.898643\ttraining's binary_logloss: 0.118736\tvalid_1's auc: 0.834012\tvalid_1's binary_logloss: 0.131262\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109210 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[14]\ttraining's auc: 0.882212\ttraining's binary_logloss: 0.122057\tvalid_1's auc: 0.834167\tvalid_1's binary_logloss: 0.1375\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090988 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12885                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[6]\ttraining's auc: 0.877909\ttraining's binary_logloss: 0.129022\tvalid_1's auc: 0.831019\tvalid_1's binary_logloss: 0.139605\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101907 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13008                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[8]\ttraining's auc: 0.882764\ttraining's binary_logloss: 0.126909\tvalid_1's auc: 0.834978\tvalid_1's binary_logloss: 0.133306\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121271 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12915                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[10]\ttraining's auc: 0.888474\ttraining's binary_logloss: 0.121393\tvalid_1's auc: 0.831905\tvalid_1's binary_logloss: 0.13881\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069514 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[16]\ttraining's auc: 0.883463\ttraining's binary_logloss: 0.122487\tvalid_1's auc: 0.828811\tvalid_1's binary_logloss: 0.136226\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117560 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[26]\ttraining's auc: 0.897739\ttraining's binary_logloss: 0.118696\tvalid_1's auc: 0.835408\tvalid_1's binary_logloss: 0.131068\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074890 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[16]\ttraining's auc: 0.882981\ttraining's binary_logloss: 0.121866\tvalid_1's auc: 0.83719\tvalid_1's binary_logloss: 0.136608\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071333 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[25]\ttraining's auc: 0.889418\ttraining's binary_logloss: 0.12278\tvalid_1's auc: 0.830877\tvalid_1's binary_logloss: 0.136842\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082702 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[29]\ttraining's auc: 0.891395\ttraining's binary_logloss: 0.122542\tvalid_1's auc: 0.836749\tvalid_1's binary_logloss: 0.131362\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092357 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[29]\ttraining's auc: 0.893513\ttraining's binary_logloss: 0.120113\tvalid_1's auc: 0.836608\tvalid_1's binary_logloss: 0.137057\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075191 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12922                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[12]\ttraining's auc: 0.906435\ttraining's binary_logloss: 0.114779\tvalid_1's auc: 0.82774\tvalid_1's binary_logloss: 0.136755\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096642 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13094                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[8]\ttraining's auc: 0.888557\ttraining's binary_logloss: 0.123077\tvalid_1's auc: 0.829973\tvalid_1's binary_logloss: 0.13395\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067456 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12950                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[10]\ttraining's auc: 0.900951\ttraining's binary_logloss: 0.116292\tvalid_1's auc: 0.830367\tvalid_1's binary_logloss: 0.139063\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076811 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[24]\ttraining's auc: 0.890348\ttraining's binary_logloss: 0.123792\tvalid_1's auc: 0.831582\tvalid_1's binary_logloss: 0.137387\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139734 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[13]\ttraining's auc: 0.874758\ttraining's binary_logloss: 0.134829\tvalid_1's auc: 0.834646\tvalid_1's binary_logloss: 0.137045\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172377 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[33]\ttraining's auc: 0.899419\ttraining's binary_logloss: 0.11843\tvalid_1's auc: 0.834678\tvalid_1's binary_logloss: 0.137567\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071686 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12922                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[13]\ttraining's auc: 0.892057\ttraining's binary_logloss: 0.11902\tvalid_1's auc: 0.830649\tvalid_1's binary_logloss: 0.136321\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145130 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13050                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[19]\ttraining's auc: 0.904309\ttraining's binary_logloss: 0.11568\tvalid_1's auc: 0.835229\tvalid_1's binary_logloss: 0.131373\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112064 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12950                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[12]\ttraining's auc: 0.88868\ttraining's binary_logloss: 0.119275\tvalid_1's auc: 0.833478\tvalid_1's binary_logloss: 0.137726\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125074 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12895                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[32]\ttraining's auc: 0.888068\ttraining's binary_logloss: 0.124061\tvalid_1's auc: 0.83111\tvalid_1's binary_logloss: 0.137324\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110224 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13046                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[15]\ttraining's auc: 0.868803\ttraining's binary_logloss: 0.137033\tvalid_1's auc: 0.835942\tvalid_1's binary_logloss: 0.138006\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084849 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12915                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[49]\ttraining's auc: 0.899627\ttraining's binary_logloss: 0.117231\tvalid_1's auc: 0.836838\tvalid_1's binary_logloss: 0.136736\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063695 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12885                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[17]\ttraining's auc: 0.880608\ttraining's binary_logloss: 0.123408\tvalid_1's auc: 0.831875\tvalid_1's binary_logloss: 0.136083\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076258 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13008                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[21]\ttraining's auc: 0.886836\ttraining's binary_logloss: 0.122805\tvalid_1's auc: 0.835644\tvalid_1's binary_logloss: 0.13133\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100618 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12915                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[17]\ttraining's auc: 0.880862\ttraining's binary_logloss: 0.122781\tvalid_1's auc: 0.834309\tvalid_1's binary_logloss: 0.137458\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080416 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[27]\ttraining's auc: 0.893158\ttraining's binary_logloss: 0.122967\tvalid_1's auc: 0.83139\tvalid_1's binary_logloss: 0.137345\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118834 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[39]\ttraining's auc: 0.902827\ttraining's binary_logloss: 0.118997\tvalid_1's auc: 0.833863\tvalid_1's binary_logloss: 0.131786\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088959 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[29]\ttraining's auc: 0.89403\ttraining's binary_logloss: 0.121202\tvalid_1's auc: 0.834426\tvalid_1's binary_logloss: 0.138472\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092680 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12835                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[84]\ttraining's auc: 0.887929\ttraining's binary_logloss: 0.122272\tvalid_1's auc: 0.831646\tvalid_1's binary_logloss: 0.136149\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117048 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13008                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[28]\ttraining's auc: 0.862102\ttraining's binary_logloss: 0.138724\tvalid_1's auc: 0.836016\tvalid_1's binary_logloss: 0.138418\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068714 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12863                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[67]\ttraining's auc: 0.880004\ttraining's binary_logloss: 0.124743\tvalid_1's auc: 0.836627\tvalid_1's binary_logloss: 0.137962\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092703 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[16]\ttraining's auc: 0.891733\ttraining's binary_logloss: 0.122281\tvalid_1's auc: 0.831215\tvalid_1's binary_logloss: 0.136879\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085940 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13008                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[27]\ttraining's auc: 0.907788\ttraining's binary_logloss: 0.116045\tvalid_1's auc: 0.833893\tvalid_1's binary_logloss: 0.131288\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067312 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[17]\ttraining's auc: 0.892339\ttraining's binary_logloss: 0.120825\tvalid_1's auc: 0.835105\tvalid_1's binary_logloss: 0.137923\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068024 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12885                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[12]\ttraining's auc: 0.890809\ttraining's binary_logloss: 0.120382\tvalid_1's auc: 0.830185\tvalid_1's binary_logloss: 0.1365\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061232 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13008                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[6]\ttraining's auc: 0.869003\ttraining's binary_logloss: 0.131621\tvalid_1's auc: 0.833731\tvalid_1's binary_logloss: 0.134972\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063235 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12915                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[18]\ttraining's auc: 0.90523\ttraining's binary_logloss: 0.113791\tvalid_1's auc: 0.8331\tvalid_1's binary_logloss: 0.137537\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084574 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\ttraining's auc: 0.893787\ttraining's binary_logloss: 0.119715\tvalid_1's auc: 0.83254\tvalid_1's binary_logloss: 0.135597\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075312 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13008                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[53]\ttraining's auc: 0.874685\ttraining's binary_logloss: 0.130084\tvalid_1's auc: 0.836271\tvalid_1's binary_logloss: 0.133382\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095522 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[91]\ttraining's auc: 0.890704\ttraining's binary_logloss: 0.120373\tvalid_1's auc: 0.837451\tvalid_1's binary_logloss: 0.136615\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076413 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[76]\ttraining's auc: 0.894472\ttraining's binary_logloss: 0.119534\tvalid_1's auc: 0.831524\tvalid_1's binary_logloss: 0.135887\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081893 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[90]\ttraining's auc: 0.89836\ttraining's binary_logloss: 0.119315\tvalid_1's auc: 0.836027\tvalid_1's binary_logloss: 0.130725\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067894 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[51]\ttraining's auc: 0.882403\ttraining's binary_logloss: 0.12424\tvalid_1's auc: 0.836045\tvalid_1's binary_logloss: 0.137957\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082447 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12926                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[36]\ttraining's auc: 0.903883\ttraining's binary_logloss: 0.115785\tvalid_1's auc: 0.830096\tvalid_1's binary_logloss: 0.135968\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143226 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13094                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[9]\ttraining's auc: 0.865784\ttraining's binary_logloss: 0.136632\tvalid_1's auc: 0.834575\tvalid_1's binary_logloss: 0.137741\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085655 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12950                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[41]\ttraining's auc: 0.907124\ttraining's binary_logloss: 0.113368\tvalid_1's auc: 0.836712\tvalid_1's binary_logloss: 0.13644\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079985 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[98]\ttraining's auc: 0.869963\ttraining's binary_logloss: 0.129022\tvalid_1's auc: 0.83026\tvalid_1's binary_logloss: 0.137915\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122300 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12954                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[99]\ttraining's auc: 0.869146\ttraining's binary_logloss: 0.131159\tvalid_1's auc: 0.83565\tvalid_1's binary_logloss: 0.133162\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070884 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\ttraining's auc: 0.87013\ttraining's binary_logloss: 0.128267\tvalid_1's auc: 0.835836\tvalid_1's binary_logloss: 0.138809\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086493 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[76]\ttraining's auc: 0.888926\ttraining's binary_logloss: 0.120919\tvalid_1's auc: 0.831889\tvalid_1's binary_logloss: 0.135827\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069003 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[21]\ttraining's auc: 0.858707\ttraining's binary_logloss: 0.138373\tvalid_1's auc: 0.835384\tvalid_1's binary_logloss: 0.137473\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068810 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[55]\ttraining's auc: 0.877993\ttraining's binary_logloss: 0.124335\tvalid_1's auc: 0.837539\tvalid_1's binary_logloss: 0.136975\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083558 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[25]\ttraining's auc: 0.888493\ttraining's binary_logloss: 0.12167\tvalid_1's auc: 0.830873\tvalid_1's binary_logloss: 0.136089\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077651 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[41]\ttraining's auc: 0.904748\ttraining's binary_logloss: 0.116897\tvalid_1's auc: 0.836547\tvalid_1's binary_logloss: 0.130579\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064645 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[30]\ttraining's auc: 0.894066\ttraining's binary_logloss: 0.118717\tvalid_1's auc: 0.836698\tvalid_1's binary_logloss: 0.136591\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069360 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[73]\ttraining's auc: 0.888177\ttraining's binary_logloss: 0.121236\tvalid_1's auc: 0.832469\tvalid_1's binary_logloss: 0.135663\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094131 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[92]\ttraining's auc: 0.893164\ttraining's binary_logloss: 0.12084\tvalid_1's auc: 0.836808\tvalid_1's binary_logloss: 0.130493\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076734 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[53]\ttraining's auc: 0.878518\ttraining's binary_logloss: 0.124543\tvalid_1's auc: 0.837753\tvalid_1's binary_logloss: 0.137\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091403 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[67]\ttraining's auc: 0.892989\ttraining's binary_logloss: 0.119666\tvalid_1's auc: 0.831573\tvalid_1's binary_logloss: 0.135636\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091246 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[87]\ttraining's auc: 0.89997\ttraining's binary_logloss: 0.118624\tvalid_1's auc: 0.838406\tvalid_1's binary_logloss: 0.130134\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076568 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[62]\ttraining's auc: 0.889571\ttraining's binary_logloss: 0.120034\tvalid_1's auc: 0.836798\tvalid_1's binary_logloss: 0.136463\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068371 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[29]\ttraining's auc: 0.887071\ttraining's binary_logloss: 0.121546\tvalid_1's auc: 0.831452\tvalid_1's binary_logloss: 0.135825\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092242 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[45]\ttraining's auc: 0.901124\ttraining's binary_logloss: 0.118084\tvalid_1's auc: 0.837408\tvalid_1's binary_logloss: 0.130491\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075352 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[28]\ttraining's auc: 0.886292\ttraining's binary_logloss: 0.121144\tvalid_1's auc: 0.837828\tvalid_1's binary_logloss: 0.136417\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078607 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[64]\ttraining's auc: 0.891285\ttraining's binary_logloss: 0.120261\tvalid_1's auc: 0.832229\tvalid_1's binary_logloss: 0.135598\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065694 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[19]\ttraining's auc: 0.862446\ttraining's binary_logloss: 0.137272\tvalid_1's auc: 0.835449\tvalid_1's binary_logloss: 0.137017\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064512 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[60]\ttraining's auc: 0.888165\ttraining's binary_logloss: 0.120612\tvalid_1's auc: 0.837055\tvalid_1's binary_logloss: 0.136432\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075447 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\ttraining's auc: 0.869631\ttraining's binary_logloss: 0.129534\tvalid_1's auc: 0.829346\tvalid_1's binary_logloss: 0.138232\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086446 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[57]\ttraining's auc: 0.858867\ttraining's binary_logloss: 0.138623\tvalid_1's auc: 0.834543\tvalid_1's binary_logloss: 0.13773\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089437 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\ttraining's auc: 0.867898\ttraining's binary_logloss: 0.129098\tvalid_1's auc: 0.835723\tvalid_1's binary_logloss: 0.139077\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.163931 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[33]\ttraining's auc: 0.89628\ttraining's binary_logloss: 0.118554\tvalid_1's auc: 0.832107\tvalid_1's binary_logloss: 0.135623\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108889 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[43]\ttraining's auc: 0.904605\ttraining's binary_logloss: 0.116756\tvalid_1's auc: 0.83804\tvalid_1's binary_logloss: 0.130441\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077284 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[31]\ttraining's auc: 0.895219\ttraining's binary_logloss: 0.118745\tvalid_1's auc: 0.835749\tvalid_1's binary_logloss: 0.136972\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066751 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[29]\ttraining's auc: 0.89138\ttraining's binary_logloss: 0.120194\tvalid_1's auc: 0.830716\tvalid_1's binary_logloss: 0.135883\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068627 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[30]\ttraining's auc: 0.890302\ttraining's binary_logloss: 0.121845\tvalid_1's auc: 0.835779\tvalid_1's binary_logloss: 0.13078\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059176 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[24]\ttraining's auc: 0.88255\ttraining's binary_logloss: 0.122328\tvalid_1's auc: 0.835161\tvalid_1's binary_logloss: 0.137151\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069351 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[21]\ttraining's auc: 0.893953\ttraining's binary_logloss: 0.11914\tvalid_1's auc: 0.829057\tvalid_1's binary_logloss: 0.13618\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066225 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[30]\ttraining's auc: 0.908126\ttraining's binary_logloss: 0.115502\tvalid_1's auc: 0.834479\tvalid_1's binary_logloss: 0.131194\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094783 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[29]\ttraining's auc: 0.906639\ttraining's binary_logloss: 0.113813\tvalid_1's auc: 0.835706\tvalid_1's binary_logloss: 0.136692\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075413 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[34]\ttraining's auc: 0.889555\ttraining's binary_logloss: 0.123409\tvalid_1's auc: 0.831636\tvalid_1's binary_logloss: 0.136865\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071370 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[54]\ttraining's auc: 0.901515\ttraining's binary_logloss: 0.118508\tvalid_1's auc: 0.835245\tvalid_1's binary_logloss: 0.131015\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094507 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[35]\ttraining's auc: 0.888981\ttraining's binary_logloss: 0.122376\tvalid_1's auc: 0.835745\tvalid_1's binary_logloss: 0.13802\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061777 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\ttraining's auc: 0.865673\ttraining's binary_logloss: 0.132381\tvalid_1's auc: 0.827493\tvalid_1's binary_logloss: 0.140039\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077243 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[70]\ttraining's auc: 0.85939\ttraining's binary_logloss: 0.13914\tvalid_1's auc: 0.835253\tvalid_1's binary_logloss: 0.138196\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118115 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\ttraining's auc: 0.862398\ttraining's binary_logloss: 0.131836\tvalid_1's auc: 0.833291\tvalid_1's binary_logloss: 0.141133\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073839 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[72]\ttraining's auc: 0.890116\ttraining's binary_logloss: 0.120786\tvalid_1's auc: 0.832287\tvalid_1's binary_logloss: 0.135738\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093547 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[97]\ttraining's auc: 0.898387\ttraining's binary_logloss: 0.119163\tvalid_1's auc: 0.836743\tvalid_1's binary_logloss: 0.130496\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066917 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[45]\ttraining's auc: 0.876047\ttraining's binary_logloss: 0.126142\tvalid_1's auc: 0.836795\tvalid_1's binary_logloss: 0.138217\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081782 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[41]\ttraining's auc: 0.897675\ttraining's binary_logloss: 0.117975\tvalid_1's auc: 0.831779\tvalid_1's binary_logloss: 0.135624\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061738 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[56]\ttraining's auc: 0.909107\ttraining's binary_logloss: 0.115603\tvalid_1's auc: 0.836025\tvalid_1's binary_logloss: 0.130585\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065278 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[36]\ttraining's auc: 0.89275\ttraining's binary_logloss: 0.119111\tvalid_1's auc: 0.835937\tvalid_1's binary_logloss: 0.136647\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072035 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\ttraining's auc: 0.885778\ttraining's binary_logloss: 0.122456\tvalid_1's auc: 0.83141\tvalid_1's binary_logloss: 0.135921\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079979 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[29]\ttraining's auc: 0.857385\ttraining's binary_logloss: 0.139595\tvalid_1's auc: 0.835057\tvalid_1's binary_logloss: 0.138489\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077479 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[74]\ttraining's auc: 0.875425\ttraining's binary_logloss: 0.125649\tvalid_1's auc: 0.837658\tvalid_1's binary_logloss: 0.13748\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117270 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[26]\ttraining's auc: 0.902858\ttraining's binary_logloss: 0.116184\tvalid_1's auc: 0.830246\tvalid_1's binary_logloss: 0.136101\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062599 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[23]\ttraining's auc: 0.897194\ttraining's binary_logloss: 0.119629\tvalid_1's auc: 0.835902\tvalid_1's binary_logloss: 0.13119\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062850 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[22]\ttraining's auc: 0.896262\ttraining's binary_logloss: 0.117787\tvalid_1's auc: 0.835465\tvalid_1's binary_logloss: 0.137079\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089582 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[63]\ttraining's auc: 0.899646\ttraining's binary_logloss: 0.117096\tvalid_1's auc: 0.831433\tvalid_1's binary_logloss: 0.135529\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064758 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[48]\ttraining's auc: 0.888797\ttraining's binary_logloss: 0.122445\tvalid_1's auc: 0.837543\tvalid_1's binary_logloss: 0.130775\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074025 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[72]\ttraining's auc: 0.90459\ttraining's binary_logloss: 0.114652\tvalid_1's auc: 0.837471\tvalid_1's binary_logloss: 0.136067\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073537 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[25]\ttraining's auc: 0.899438\ttraining's binary_logloss: 0.11808\tvalid_1's auc: 0.831287\tvalid_1's binary_logloss: 0.13608\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076792 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[7]\ttraining's auc: 0.867526\ttraining's binary_logloss: 0.137319\tvalid_1's auc: 0.833761\tvalid_1's binary_logloss: 0.138222\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082677 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[12]\ttraining's auc: 0.875878\ttraining's binary_logloss: 0.127684\tvalid_1's auc: 0.83395\tvalid_1's binary_logloss: 0.140162\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124126 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[12]\ttraining's auc: 0.87886\ttraining's binary_logloss: 0.125777\tvalid_1's auc: 0.828991\tvalid_1's binary_logloss: 0.137275\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087743 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[20]\ttraining's auc: 0.890323\ttraining's binary_logloss: 0.121447\tvalid_1's auc: 0.835596\tvalid_1's binary_logloss: 0.13143\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067427 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[25]\ttraining's auc: 0.900864\ttraining's binary_logloss: 0.115675\tvalid_1's auc: 0.835757\tvalid_1's binary_logloss: 0.136628\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070951 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[50]\ttraining's auc: 0.909049\ttraining's binary_logloss: 0.113801\tvalid_1's auc: 0.830427\tvalid_1's binary_logloss: 0.13583\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065984 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[44]\ttraining's auc: 0.903144\ttraining's binary_logloss: 0.117668\tvalid_1's auc: 0.836499\tvalid_1's binary_logloss: 0.130804\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074984 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[27]\ttraining's auc: 0.887742\ttraining's binary_logloss: 0.122241\tvalid_1's auc: 0.836168\tvalid_1's binary_logloss: 0.137611\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095716 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[8]\ttraining's auc: 0.873503\ttraining's binary_logloss: 0.127602\tvalid_1's auc: 0.828237\tvalid_1's binary_logloss: 0.138342\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078224 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13008                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[19]\ttraining's auc: 0.897574\ttraining's binary_logloss: 0.118524\tvalid_1's auc: 0.835902\tvalid_1's binary_logloss: 0.131176\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142573 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[14]\ttraining's auc: 0.888474\ttraining's binary_logloss: 0.120027\tvalid_1's auc: 0.834591\tvalid_1's binary_logloss: 0.137392\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075761 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12827                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[64]\ttraining's auc: 0.890585\ttraining's binary_logloss: 0.124906\tvalid_1's auc: 0.831592\tvalid_1's binary_logloss: 0.137863\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086503 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12946                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[98]\ttraining's auc: 0.900064\ttraining's binary_logloss: 0.120225\tvalid_1's auc: 0.834152\tvalid_1's binary_logloss: 0.131623\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081819 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12855                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[79]\ttraining's auc: 0.8945\ttraining's binary_logloss: 0.121017\tvalid_1's auc: 0.835051\tvalid_1's binary_logloss: 0.138241\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072479 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12895                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[85]\ttraining's auc: 0.904325\ttraining's binary_logloss: 0.115466\tvalid_1's auc: 0.832696\tvalid_1's binary_logloss: 0.135594\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072919 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13046                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[71]\ttraining's auc: 0.895229\ttraining's binary_logloss: 0.119958\tvalid_1's auc: 0.83829\tvalid_1's binary_logloss: 0.13037\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069262 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12915                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[42]\ttraining's auc: 0.880406\ttraining's binary_logloss: 0.123703\tvalid_1's auc: 0.83768\tvalid_1's binary_logloss: 0.136956\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094112 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12895                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[22]\ttraining's auc: 0.903536\ttraining's binary_logloss: 0.116334\tvalid_1's auc: 0.829045\tvalid_1's binary_logloss: 0.13634\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092351 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13046                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[26]\ttraining's auc: 0.907771\ttraining's binary_logloss: 0.115433\tvalid_1's auc: 0.835926\tvalid_1's binary_logloss: 0.131222\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061233 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12915                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[18]\ttraining's auc: 0.897709\ttraining's binary_logloss: 0.11834\tvalid_1's auc: 0.834657\tvalid_1's binary_logloss: 0.137588\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142876 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12885                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[25]\ttraining's auc: 0.895377\ttraining's binary_logloss: 0.118275\tvalid_1's auc: 0.830974\tvalid_1's binary_logloss: 0.135881\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075459 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13037                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[30]\ttraining's auc: 0.899723\ttraining's binary_logloss: 0.11817\tvalid_1's auc: 0.837348\tvalid_1's binary_logloss: 0.130793\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082685 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12915                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[29]\ttraining's auc: 0.900515\ttraining's binary_logloss: 0.115523\tvalid_1's auc: 0.835878\tvalid_1's binary_logloss: 0.136581\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098285 seconds.                \n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12926                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[40]\ttraining's auc: 0.918157\ttraining's binary_logloss: 0.110446\tvalid_1's auc: 0.830875\tvalid_1's binary_logloss: 0.136074\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114807 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13094                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[17]\ttraining's auc: 0.886925\ttraining's binary_logloss: 0.126151\tvalid_1's auc: 0.835208\tvalid_1's binary_logloss: 0.133511\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081338 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12950                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[43]\ttraining's auc: 0.920927\ttraining's binary_logloss: 0.108508\tvalid_1's auc: 0.835286\tvalid_1's binary_logloss: 0.136897\n",
      "[LightGBM] [Info] Number of positive: 1579, number of negative: 38965                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120640 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12922                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038945 -> initscore=-3.205872                                        \n",
      "[LightGBM] [Info] Start training from score -3.205872                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[43]\ttraining's auc: 0.897475\ttraining's binary_logloss: 0.118376\tvalid_1's auc: 0.831328\tvalid_1's binary_logloss: 0.135789\n",
      "[LightGBM] [Info] Number of positive: 1609, number of negative: 38935                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084650 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13094                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039685 -> initscore=-3.186281                                        \n",
      "[LightGBM] [Info] Start training from score -3.186281                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[55]\ttraining's auc: 0.903379\ttraining's binary_logloss: 0.117121\tvalid_1's auc: 0.838443\tvalid_1's binary_logloss: 0.130262\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 38984                                                  \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089482 seconds.                \n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12950                                                                                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202                          \n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038477 -> initscore=-3.218465                                        \n",
      "[LightGBM] [Info] Start training from score -3.218465                                                                  \n",
      "Training until validation scores don't improve for 30 rounds                                                           \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[59]\ttraining's auc: 0.90819\ttraining's binary_logloss: 0.113368\tvalid_1's auc: 0.837301\tvalid_1's binary_logloss: 0.136206\n",
      "100%|███████████████████████████████████████████████| 50/50 [11:50<00:00, 14.21s/trial, best loss: -0.8362219933743916]\n",
      "LGBM best: {'learning_rate': 0.04589753644367517, 'max_depth': 119.0, 'min_child_samples': 69.0, 'num_leaves': 33.0, 'subsample': 0.9332360463629378}\n"
     ]
    }
   ],
   "source": [
    "trial_val_lgbm = Trials()\n",
    "\n",
    "best_lgbm = fmin(fn=objective_func_lgbm, \n",
    "                 space=lgbm_search_space,\n",
    "                 algo=tpe.suggest,\n",
    "                   max_evals= 50,\n",
    "                 trials= trial_val_lgbm,\n",
    "                 rstate= np.random.default_rng(seed=30))\n",
    "print(f'LGBM best: {best_lgbm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최적의 하이퍼파라미터로 학습 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1658, number of negative: 40913\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13001\n",
      "[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 202\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038947 -> initscore=-3.205836\n",
      "[LightGBM] [Info] Start training from score -3.205836\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[77]\ttraining's auc: 0.899118\ttraining's binary_logloss: 0.117171\tvalid_1's auc: 0.833934\tvalid_1's binary_logloss: 0.13474\n",
      "ROC_AUC: 0.8412\n"
     ]
    }
   ],
   "source": [
    "lgbm_best_clf = LGBMClassifier(n_estimators=100, \n",
    "                              max_depth= int(best_lgbm['max_depth']),\n",
    "                           min_child_samples= int(best_lgbm['min_child_samples']),\n",
    "                           num_leaves= int(best_lgbm['num_leaves']),\n",
    "                           subsample= best_lgbm['subsample'],\n",
    "                            learning_rate= best_lgbm['learning_rate'])\n",
    "\n",
    "lgbm_best_clf.fit(x_tr, y_tr, eval_metric='auc', \n",
    "                  eval_set=[(x_tr,y_tr),(x_val,y_val)],\n",
    "               callbacks=[early_stopping(stopping_rounds=100)])\n",
    "lgbm_best_score = roc_auc_score(y_test, lgbm_best_clf.predict_proba(x_test)[:,1])\n",
    "print(f'ROC_AUC: {lgbm_best_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
