{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 분류 실습 : 20 뉴스그룹 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 분류\n",
    "\n",
    ": 특정 문서의 분류를 학습 데이터를 통해 학습 모델을 생성한 뒤 이 모델을 통해 다른 문서의 분류를 예측하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습 예제**\n",
    "- 사이킷런 내부에 가지고 있는 20 뉴스그룹 예제데이터\n",
    "    - fetch_20newsgroups() API 이용\n",
    "    - 뉴스그룹 분류를 위한 데이터\n",
    "    \n",
    "\n",
    "**실습 예제 분석 방법**\n",
    "- 텍스트의 피처 벡터화 후 생성된 희소행렬을 이용해 분류 알고리즘 적용\n",
    "    - 분류 알고리즘 : 로지스틱 회귀, 선형 서포트 벡터 머신, 나이브 베이즈 등\n",
    "    \n",
    "\n",
    "- 카운트 기반과 TF-IDF 기반의 벡터화를 차례로 적용하여 예측 성능 비교\n",
    "\n",
    "\n",
    "- 피처 벡터화를 위한 파라미터와 GridSearchCV 기반의 하이퍼 파라미터 튜닝\n",
    "\n",
    "\n",
    "- Pipeline 객체를 통한 피처 벡터화 파라미터와 GridSearchCV 기반의 하이퍼 파라미터 튜닝을 한번에 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fetch_20newsgroups()\n",
    "- 인터넷에서 로컬 컴퓨터로 데이터를 먼저 내려받은 후 메모리로 데이터를 로딩\n",
    "- 내컴퓨터의 [C:\\Users\\사용자폴더명\\scikit_learn_data]에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C:\\\\Users\\\\gillhk\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\rec.motorcycles\\\\104321',\n",
       "       'C:\\\\Users\\\\gillhk\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\rec.motorcycles\\\\103229',\n",
       "       'C:\\\\Users\\\\gillhk\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-test\\\\sci.electronics\\\\54286',\n",
       "       ...,\n",
       "       'C:\\\\Users\\\\gillhk\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\rec.autos\\\\102799',\n",
       "       'C:\\\\Users\\\\gillhk\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.ibm.pc.hardware\\\\60175',\n",
       "       'C:\\\\Users\\\\gillhk\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\rec.sport.baseball\\\\104387'],\n",
       "      dtype='<U96')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load_xxx() API와 유사한 Key 값을 가짐\n",
    "    - filenames : 로컬 컴퓨터에 저장하는 디렉터리와 파일명\n",
    "    - target_names : target 클래스들의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  8, 12, ...,  7,  3,  9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         8\n",
       "1         8\n",
       "2        12\n",
       "3        10\n",
       "4         6\n",
       "         ..\n",
       "18841    19\n",
       "18842     3\n",
       "18843     7\n",
       "18844     3\n",
       "18845     9\n",
       "Length: 18846, dtype: int32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(news_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 개별 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 뉴스그룹 데이터 구성\n",
    "    - 제목\n",
    "    - 작성자\n",
    "    - 소속\n",
    "    - 이메일\n",
    "    - 뉴스그룹 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기사 내용만 가지고 텍스트 분석 진행\n",
    "- 제목, 소속, 이메일 등의 헤더(header)와 푸터(footer) 정보들은 뉴스 그룹 분류에 높은 예측 성능을 가질 수 있으므로 제거함\n",
    "    - remove 파라미터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: 11314\n"
     ]
    }
   ],
   "source": [
    "train_news = fetch_20newsgroups(subset='train', remove=('header','footer','quotes'), random_state= 156)\n",
    "\n",
    "x_train = train_news.data\n",
    "y_train = train_news.target\n",
    "print(f'train dataset: {len(x_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset: 7532\n"
     ]
    }
   ],
   "source": [
    "test_news = fetch_20newsgroups(subset='test', remove=('header','footer','quotes'), random_state= 156)\n",
    "\n",
    "x_test = test_news.data\n",
    "y_test = test_news.target\n",
    "print(f'test dataset: {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: cpage@two-step.seas.upenn.edu (Carter C. Page)\n",
      "Subject: Re: Prayer in Jesus' Name\n",
      "Organization: University of Pennsylvania\n",
      "Lines: 46\n",
      "\n",
      "\n",
      "\t\"And in that day you will ask Me no question.  Truly, truly, I say to \n",
      "\tyou, if you shall ask the Father for anything, He will give it to you \n",
      "\tin my name.  Until now you have asked for nothing in My name; ask, and \n",
      "\tyou will receive, that your joy may be made full.\"\n",
      "\t\t\t\t-John 16:23-24\n",
      "\n",
      "I don't believe that we necessarily have to say \" . . . In Christ's name.  \n",
      "Amen,\" for our prayers to be heard, but it glorifies the Son, when we \n",
      "acknowledge that our prayer is made possible by Him.  I believe that just as \n",
      "those who were saved in the OT, could only be saved because Jesus would one day\n",
      "reconcile God to man, He is the only reason their prayers would be heard by \n",
      "God.\n",
      "\n",
      "\tFor all of us have become like one who is unclean,\n",
      "\tAnd all our righteous deeds are like a filthy garment;\n",
      "\tAnd all of us wither like a leaf,\n",
      "\tand our iniquities, like the wind, take us away.\n",
      "\t\t\t\t-Isaiah 64:6, NAS\n",
      "\n",
      "Our prayers like the rest of our deeds are too unholy to go directly to the\n",
      "Father because they are tainted by our sin.  Only by washing these prayers with\n",
      "Christ's blood are they worthy to be lifted to to the Father.\n",
      "\n",
      "\t\"First, I thank my God through Christ Jesus . . .\"\n",
      "\t\t\t\t-Romans 1:8, NAS\n",
      "\n",
      "Some scholars believe that this is Paul recognizing that even his thanks are \n",
      "too unholy for the Father.\n",
      "\tBasically, prayer is a gift of grace, I believe that only through Jesus\n",
      "do our prayers have any power; thus, praying in His name glorifies and praises \n",
      "Jesus for this beautiful and powerful gift He has given us.\n",
      "\n",
      "+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=\n",
      "Carter C. Page           | Of happiness the crown and chiefest part is wisdom,\n",
      "A Carpenter's Apprentice | and to hold God in awe.  This is the law that,\n",
      "cpage@seas.upenn.edu     | seeing the stricken heart of pride brought down,\n",
      "                         | we learn when we are old.  -Adapted from Sophocles\n",
      "+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=-+-=+-=-+-=+-=-+=-+-=-+-=-+=-+-=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CountVectorizer를 이용해 텍스트데이터 피처 벡터화\n",
    "\n",
    "\n",
    "**테스트 데이터에서 CountVectorizer를 적용할 때 주의점**: \n",
    "- 학습 데이터를 이용해 fit()이 수행된 CountVectorizer 객체를 이용해 테스트 데이터를 변환(transform)해야 함\n",
    "    - 학습 시 설정된 CountVectorizer의 피처 개수와 테스트 데이터를 CountVectorizer로 변환할 피처 개수가 같아짐\n",
    "- 테스트 데이터 피처 벡터화할 때 fit_transform()을 사용하면 안됨\n",
    "    - 테스트 데이터를 fit_transform()을 이용해 벡터화하면 피처개수가 학습데이터와 달라지므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 120756), (7532, 120756)\n"
     ]
    }
   ],
   "source": [
    "cnt = CountVectorizer()\n",
    "\n",
    "# 학습\n",
    "cnt.fit(x_train)\n",
    "\n",
    "# 학습 데이터 피처 벡터화\n",
    "x_train_cnt = cnt.transform(x_train)\n",
    "\n",
    "# 테스트 데이터 피처 벡테화\n",
    "x_test_cnt = cnt.transform(x_test)\n",
    "\n",
    "print(f'{x_train_cnt.shape}, {x_test_cnt.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 로지스틱 회귀를 적용해 뉴스그룹 분류 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count 기반으로 피처 벡터화가 적용된 데이터 세트에 대한 로지스틱 회귀 모델 적용하여 예측**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression(random_state= 0)\n",
    "lr.fit(x_train_cnt, y_train)\n",
    "pred = lr.predict(x_test_cnt)\n",
    "print(f'정확도: {accuracy_score(y_test, pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TF-IDF를 이용한 피처벡터화 및 로지스틱 회귀로 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF 기반으로 벡터화를 변경한 데이터 세트에 대해 로지스틱 회귀 적용하여 예측**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 120756), (7532, 120756)\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "\n",
    "# 학습\n",
    "tf.fit(x_train)\n",
    "\n",
    "# 학습 데이터 피처 벡터화\n",
    "x_train_tf = tf.transform(x_train)\n",
    "\n",
    "# 테스트 데이터 피처 벡테화\n",
    "x_test_tf = tf.transform(x_test)\n",
    "\n",
    "print(f'{x_train_tf.shape}, {x_test_tf.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7841\n"
     ]
    }
   ],
   "source": [
    "lr_tf = LogisticRegression(random_state= 0)\n",
    "lr_tf.fit(x_train_tf, y_train)\n",
    "pred_tf = lr_tf.predict(x_test_tf)\n",
    "print(f'정확도: {accuracy_score(y_test, pred_tf):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텍스트 분석에서 머신러닝 모델 성능을 향상시키는 주요 방법 2가지**\n",
    "- 최적의 ML 알고리즘 선택하는 것\n",
    "- 최상의 피처 전처리를 수행하는 것\n",
    "\n",
    "\n",
    "=> 텍스트 정규화나 Count/TF-IDF 기반 피처 벡터화를 어떻게 효과적으로 적용했는지가 텍스트 기반 머신러닝 성능에  큰 영향을 미칠 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF 벡터화에서 좀 더 다양한 파라미터를 적용하여 예측 수행**\n",
    "\n",
    ": TfidfVectorizer 클래스의 파라미터 변경\n",
    "\n",
    "- stop_words : None에서 'english'로 변경\n",
    "- ngram_range : (1,1)에서 (1,2)로 변경\n",
    "- max_df=300으로 변경 (전체 문서에서 300개 이하로 나타나는 단어만 피처로 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7744\n"
     ]
    }
   ],
   "source": [
    "# ngram -> 연속된 단어로 추출 ex)  \"This is a pen\" -> 2-그램을 추출하면 \"This is\", \"is a\", \"a pen\"\n",
    "tf = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df= 300)\n",
    "\n",
    "tf.fit(x_train)\n",
    "x_train_tf = tf.transform(x_train)\n",
    "x_test_tf = tf.transform(x_test)\n",
    "\n",
    "lr_tf = LogisticRegression(random_state= 0)\n",
    "lr_tf.fit(x_train_tf, y_train)\n",
    "pred_tf = lr_tf.predict(x_test_tf)\n",
    "print(f'정확도: {accuracy_score(y_test, pred_tf):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 GridSearchCV를 이용해 로지스틱 회귀의 하이퍼파라미터 최적화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    }
   ],
   "source": [
    "params = {'C': [0.01, 0.1, 1,5,10]}\n",
    "grid = GridSearchCV(estimator= lr, param_grid= params, cv=3, scoring='accuracy', verbose= 1)\n",
    "\n",
    "grid.fit(x_train_tf, y_train)\n",
    "\n",
    "print(f'최적의 하이퍼 파라미터: {grid.best_params_}')\n",
    "print(f'최적의 성능: {grid.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 C값으로 학습된 grid로 예측\n",
    "params = {'C': [0.01, 0.1, 1,5,10]}\n",
    "grid_best = GridSearchCV(estimator= lr, param_grid= params, cv=3, scoring='accuracy', verbose= 1)\n",
    "pred = grid_best.predict(x_test_tf)\n",
    "print(f'최적의 모델 예측성능: {accuracy_score(y_test, pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 사이킷런 파이프라인(Pipeline) 사용 및 GridSearchCV와의 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사이킷런의 Pipeline 클래스를 사용하여 피처 벡터화와 ML 알고리즘 학습/예측을 위한 코드 작성을 한 번에 진행**\n",
    "\n",
    "**Pipeline이란?**\n",
    "- 데이터의 가공, 변환 등의 전처리와 알고리즘 적용을 '수도관(pipe)에서 물이 흐르듯' 한꺼번에 스트림 기반으로 처리한다는 의미\n",
    "\n",
    "**파이프라인 이용의 장점**\n",
    "- 데이터의 전처리와 머신러닝 학습 과정을 통일된 API 기반에서 처리할 수 있어 더 직관적인 ML 모델 코드를 생성할 수 있음\n",
    "- 대용량 데이터의 피처 벡터화 결과를 별도 데이터로 저장하지 않고 스트림 기반에서 바로 머신러닝 알고리즘의 데이터로 입력할 수 있어 수행시간을 절약할 수 있음\n",
    "\n",
    "**사이킷런의 파이프라인은 모든 데이터 전처리 작업과 Estimator를 결합할 수 있음**\n",
    "- 예. 스케일링 또는 벡터 정규화 + PCA 등의 변환작업 +  분류, 회귀 등의 Estimator를 한 번에 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 텍스트 분류 예제 코드를 Pipeline을 이용하여 다시 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline 객체를 사용하여 TfidfVectorizer와 LogisticRegressor를 위한 파라미터 지정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('TF-IDF', TfidfVectorizer(stop_words='english'), ngram_range=(1,2), max_df= 300),\n",
    "('LR', LogisticRegression(c=10, max_iter= 150))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(x_train, y_train)\n",
    "pred = pipe.predict(x_test)\n",
    "print(f'Pipeline을 이용한 예측성능: {accuracy_score(y_test, pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GridSearchCV에 Pipeline을 입력하여 파라미터 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('TF-IDF', TfidfVectorizer(stop_words='english')), ('LR', LogisticRegression())])\n",
    "\n",
    "params = {'TF-IDF__ngram_range':[(1,1),(1,2)],\n",
    "         'TF-IDF__max_df':[100,300],\n",
    "         'LR__C': [5,10]}\n",
    "\n",
    "grid_pipe = GridSearchCV(pipe, param_grid= params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_pipe.fit(x_train, y_train)\n",
    "print(f'최적의 하이퍼 파라미터: {grid_pipe.best_params_}')\n",
    "print(f'최적의 성능: {grid_pipe.best_score_}')\n",
    "\n",
    "pred = grid_pipe.predict(x_test)\n",
    "print(f'Pipeline 이용한 최적모델 예측 정확도: {accuracy_score(y_test, pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
