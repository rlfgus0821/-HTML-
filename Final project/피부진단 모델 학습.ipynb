{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyP+jQ+ocgB8LzwMjWzKwalE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"metadata":{"id":"5ehyxxf3nZGy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models, transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import pickle\n","import ast"],"metadata":{"id":"u6c47wUwv7W7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 런타임 오류 방지 함수\n","# 이 함수는 Colab 연결을 유지하기 위해 60초마다 연결 버튼을 자동으로 클릭\n","def keep_alive():\n","    display(Javascript('''\n","        function ClickConnect(){\n","            console.log(\"클릭 연결 버튼\");\n","            document.querySelector(\"colab-connect-button\").click()\n","        }\n","        setInterval(ClickConnect, 60000)\n","    '''))\n"],"metadata":{"id":"eZHNMFQguayg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이 클래스는 데이터를 불러오고 처리하는 방법을 정의\n","# img_dir은 이미지가 저장된 폴더 경로, df는 데이터프레임, facepart는 얼굴 부위 번호\n","# 데이터셋 클래스 정의\n","class CachedDataset(Dataset):\n","    def __init__(self, img_dir, df, facepart, task, transform=None, cache_dir='/gdrive/MyDrive/Final project/1_Red/5_분석모델링/피부진단/cache'):\n","        self.transform = transform\n","        self.facepart = facepart\n","        self.task = task\n","        os.makedirs(cache_dir, exist_ok=True)\n","        self.cache_file = os.path.join(cache_dir, f'cache_facepart_{facepart}_{task}.pkl')\n","\n","        if os.path.exists(self.cache_file):\n","            print(f\"facepart {facepart}의 {task} 캐시된 데이터를 불러옵니다...\")\n","            with open(self.cache_file, 'rb') as f:\n","                self.cache = pickle.load(f)\n","        else:\n","            print(f\"facepart {facepart}의 {task} 캐시를 생성합니다...\")\n","            self.cache = []\n","            df_facepart = df[df['images'].apply(lambda x: x['facepart'] in ([facepart] if facepart not in [3, 4, 5, 6] else [3, 4] if facepart == 3 else [5, 6]))]\n","\n","            for idx, row in df_facepart.iterrows():\n","                try:\n","                    bbox = row['images']['bbox']\n","                    if (bbox == ['None', 'None', 'None', 'None']) or not all(isinstance(b, (int, float)) and b is not None for b in bbox):\n","                        continue\n","\n","                    img_name = row['info']['filename']\n","                    if facepart == 0:\n","                        img_path = os.path.join(img_dir, img_name)\n","                    else:\n","                        img_path = os.path.join(img_dir, f\"{os.path.splitext(img_name)[0]}_{facepart}.jpg\")\n","\n","                    if not os.path.exists(img_path):\n","                        continue\n","\n","                    try:\n","                        image = Image.open(img_path).convert('RGB')\n","                    except (IOError, OSError):\n","                        continue\n","\n","                    if self.transform:\n","                        image = self.transform(image)\n","\n","                    labels = self._prepare_labels(row['annotations'], row['equipment'], row['info'])\n","                    if labels:\n","                        self.cache.append((image, torch.tensor(labels, dtype=torch.float if task == 'regression' else torch.long)))\n","\n","                except Exception as e:\n","                    print(f\"데이터 처리 중 오류 발생: {str(e)}\")\n","                    continue\n","\n","            with open(self.cache_file, 'wb') as f:\n","                pickle.dump(self.cache, f)\n","            print(f\"facepart {facepart}의 {task} 캐시가 생성되고 저장되었습니다\")\n","\n","    def _prepare_labels(self, annotations, equipment, info):\n","        labels = []\n","        try:\n","            if self.task == 'classification':\n","                if self.facepart == 0:\n","                    labels = [info['skin_type'], info['sensitive']]\n","                if annotations:\n","                    labels.extend(self._process_annotations(annotations))\n","            elif self.task == 'regression':\n","                if equipment:\n","                    labels.extend(list(equipment.values()))\n","        except Exception as e:\n","            print(f\"레이블 준비 중 오류 발생: {str(e)}\")\n","        return labels\n","\n","    def _process_annotations(self, annotations):\n","        processed = []\n","        for key, value in annotations.items():\n","            if isinstance(value, (int, float)):\n","                processed.append(value)\n","        return processed\n","\n","    def __len__(self):\n","        return len(self.cache)\n","\n","    def __getitem__(self, idx):\n","        return self.cache[idx]\n"],"metadata":{"id":"Wqxfi3o8uYqV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ResNet50 모델 생성 함수\n","# 사전 학습된 ResNet50 모델을 로드, 마지막 fully connected 층을 num_outputs에 맞게 수정\n","# dropout 층을 추가하여 과적합을 방지\n","def create_resnet_model(num_outputs, task):\n","    model = models.resnet50(pretrained=True)\n","    num_features = model.fc.in_features\n","    if task == 'classification':\n","        model.fc = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(num_features, num_outputs)\n","        )\n","    else:  # regression\n","        model.fc = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(num_features, num_outputs),\n","            nn.ReLU() # ReLU를 사용하여 음수 값 방지\n","        )\n","    return model\n","\n","# 현재 iteration(현재 모델이 몇 번째 반복을 수행 중인지)에 따라 학습률을 감소\n","# 학습이 진행됨에 따라 학습률을 점진적으로 줄여나가는 역할\n","# 학습 초기에는 큰 학습률로 빠르게 학습하다가, 학습이 진행될수록 작은 학습률로 세밀하게 조정\n","# 모델이 더 안정적으로 수렴하도록 도움\n","def lr_poly(base_lr, iter, max_iter, power):\n","    return base_lr * ((1 - float(iter) / max_iter) ** power)\n"],"metadata":{"id":"1RelNzafuXPF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델을 학습, 검증, 체크포인트 저장, 학습 과정 시각화\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, facepart, task):\n","    best_val_loss = float('inf')\n","    train_losses = []\n","    val_losses = []\n","    train_metrics = []\n","    val_metrics = []\n","    save_path = '/gdrive/MyDrive/Final project/1_Red/5_분석모델링/피부진단/model'\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        running_metric = 0.0\n","        for i, (images, labels) in enumerate(train_loader):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # 학습률 조정\n","            lr = lr_poly(1e-3, epoch * len(train_loader) + i, num_epochs * len(train_loader), 0.9)\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = lr\n","\n","            running_loss += loss.item()\n","\n","            # 평가 지표 계산\n","            if task == 'classification':\n","                _, predicted = torch.max(outputs.data, 1)\n","                running_metric += (predicted == labels).sum().item() / labels.size(0)\n","            else:  # regression\n","                running_metric += nn.L1Loss()(outputs, labels).item()\n","\n","        epoch_loss = running_loss / len(train_loader)\n","        epoch_metric = running_metric / len(train_loader)\n","        train_losses.append(epoch_loss)\n","        train_metrics.append(epoch_metric)\n","        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, {'Accuracy' if task == 'classification' else 'MAE'}: {epoch_metric:.4f}\")\n","\n","        # 검증\n","        model.eval()\n","        val_loss = 0.0\n","        val_metric = 0.0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","                # 평가 지표 계산\n","                if task == 'classification':\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    val_metric += (predicted == labels).sum().item() / labels.size(0)\n","                else:  # regression\n","                    val_metric += nn.L1Loss()(outputs, labels).item()\n","\n","        val_loss /= len(val_loader)\n","        val_metric /= len(val_loader)\n","        val_losses.append(val_loss)\n","        val_metrics.append(val_metric)\n","        print(f\"Validation Loss: {val_loss:.4f}, {'Accuracy' if task == 'classification' else 'MAE'}: {val_metric:.4f}\")\n","\n","        # 최고의 모델 저장\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), os.path.join(save_path, f'best_model_resnet50_facepart_{facepart}_{task}.pth'))\n","            print(f\"facepart {facepart}의 {task} 새로운 최고 모델을 저장했습니다\")\n","\n","        # 체크포인트 저장\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': epoch_loss,\n","        }, os.path.join(save_path, f'checkpoint_resnet50_facepart_{facepart}_{task}_epoch_{epoch+1}.pth'))\n","\n","    # 학습 과정 시각화 및 저장\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n","    plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title(f'Training and Validation Loss for ResNet50 Facepart {facepart} {task}')\n","    plt.legend()\n","    plt.xticks(range(0, num_epochs+1, 5))\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(range(1, num_epochs+1), train_metrics, label='Train Metric')\n","    plt.plot(range(1, num_epochs+1), val_metrics, label='Validation Metric')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy' if task == 'classification' else 'MAE')\n","    plt.title(f'Training and Validation Metric for ResNet50 Facepart {facepart} {task}')\n","    plt.legend()\n","    plt.xticks(range(0, num_epochs+1, 5))\n","\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(save_path, f'plot_resnet50_facepart_{facepart}_{task}.png'))\n","    plt.close()\n","\n","    return model"],"metadata":{"id":"Dc1GCwsNuT-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 메인함수\n","def main(facepart_range):\n","    base_path = '/gdrive/MyDrive/Final project/1_Red/3_데이터수집_저장/0_데이터수집폴더/피부 데이터'\n","\n","    try:\n","        df = pd.read_csv(os.path.join(base_path, 'json to df.csv'))\n","    except FileNotFoundError:\n","        print(\"CSV 파일을 찾을 수 없습니다. 경로를 확인해주세요.\")\n","        return\n","    except pd.errors.EmptyDataError:\n","        print(\"CSV 파일이 비어있습니다.\")\n","        return\n","    except pd.errors.ParserError:\n","        print(\"CSV 파일 파싱 중 오류가 발생했습니다. 파일 형식을 확인해주세요.\")\n","        return\n","\n","    # 문자열 딕셔너리를 실제 딕셔너리로 변환\n","    for col in ['info', 'images', 'annotations', 'equipment']:\n","        df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n","\n","    # Training 데이터만 선택\n","    df_train = df[df['split'] == 'Training']\n","\n","    # 이미지 전처리 정의\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    for facepart in facepart_range:\n","        for task in ['classification', 'regression']:\n","            if task == 'regression' and facepart in [2, 7]:\n","                continue\n","\n","            print(f\"facepart {facepart} {task} 처리 중\")\n","\n","            if facepart == 0:\n","                img_dir = '/gdrive/MyDrive/Final project/1_Red/3_데이터수집_저장/0_데이터수집폴더/피부 데이터/Training/01.원천데이터'\n","            else:\n","                img_dir = f'/gdrive/MyDrive/Final project/1_Red/4_데이터탐색_전처리/facepart별 피부 이미지/Training_cropped/{facepart}'\n","\n","            if not os.path.exists(img_dir):\n","                print(f\"이미지 디렉토리를 찾을 수 없습니다: {img_dir}\")\n","                continue\n","\n","            # 데이터셋 생성\n","            try:\n","                dataset = CachedDataset(img_dir, df_train, facepart, task, transform)\n","            except Exception as e:\n","                print(f\"데이터셋 생성 중 오류 발생: {str(e)}\")\n","                continue\n","\n","            # 데이터셋을 train과 validation으로 분할\n","            train_data, val_data = train_test_split(dataset, test_size=0.1, random_state=42)\n","\n","            # 데이터 로더 생성\n","            train_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=4)\n","            val_loader = DataLoader(val_data, batch_size=16, shuffle=False, num_workers=4)\n","\n","            # 모델 생성\n","            num_outputs = len(dataset[0][1])\n","            model = create_resnet_model(num_outputs, task).to(device)\n","\n","            # 손실 함수와 최적화 알고리즘 정의\n","            criterion = nn.CrossEntropyLoss() if task == 'classification' else nn.MSELoss()\n","            optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n","\n","            # ResNet50 모델 학습\n","            try:\n","                model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=30, device=device, facepart=facepart, task=task)\n","            except Exception as e:\n","                print(f\"ResNet50 모델 학습 중 오류 발생: {str(e)}\")\n","\n","            # 최종 모델 저장\n","            save_path = '/gdrive/MyDrive/Final project/1_Red/5_분석모델링/피부진단'\n","            try:\n","                torch.save(model.state_dict(), os.path.join(save_path, f'final_model_resnet50_facepart_{facepart}_{task}.pth'))\n","            except Exception as e:\n","                print(f\"모델 저장 중 오류 발생: {str(e)}\")"],"metadata":{"id":"itBIW8yDuRc2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1번 강다율 2번 문수정 3번 김길현"],"metadata":{"id":"youCCBpFxfEO"}},{"cell_type":"code","source":["# 메인 실행\n","if __name__ == \"__main__\":\n","    keep_alive() # 연결 유지 함수 실행\n","    # 사용자 입력을 받아 facepart 범위 설정\n","    user_input = input(\"처리할 facepart 범위를 선택하세요 (1: 0-2, 2: 3-6, 3: 7-8): \")\n","    if user_input == '1':\n","        facepart_range = range(0, 3)\n","    elif user_input == '2':\n","        facepart_range = range(3, 7)\n","    elif user_input == '3':\n","        facepart_range = range(7, 9)\n","    else:\n","        print(\"잘못된 입력입니다. 프로그램을 종료합니다.\")\n","        exit()\n","\n","    main(facepart_range)"],"metadata":{"id":"_muE8fQXuNO-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 코드 설명\n","- class CachedDataset(Dataset):\n","    - 이 클래스는 데이터셋을 생성하고 캐시합니다.\n","    - '/gdrive/MyDrive/Final project/1_Red/3_데이터수집_저장/0_데이터수집폴더/피부 데이터'에서 이미지를 로드하고\n","    - '/gdrive/MyDrive/Final project/1_Red/5_분석모델링/피부진단/cache'에 캐시를 저장합니다.\n","    - 각 얼굴 부위(facepart)별로 별도의 캐시를 생성합니다.\n","\n","- def create_resnet_model(num_outputs):\n","    - ResNet50 모델을 생성하고 마지막 레이어를 수정하여 원하는 출력 수에 맞춥니다.\n","\n","- def lr_poly(base_lr, iter, max_iter, power):\n","    - 학습률을 동적으로 조정하는 함수입니다.\n","\n","- def train_model(model, train_loader, val_loader, criterion, optimizer,num_epochs, device, facepart):\n","    - 모델을 훈련시키는 함수입니다.\n","    - 훈련 중 최고의 모델을 '/gdrive/MyDrive/Final project/1_Red/5_분석모델링/피부진단/model'에 저장합니다.\n","    - 훈련 과정을 시각화하여 같은 경로에 저장합니다.\n","\n","- def main():\n","    - 메인 함수로, 전체 훈련 과정을 제어합니다.\n","    - 1. CSV 파일을 로드하고 전처리합니다.\n","    - 2. 각 얼굴 부위(facepart)에 대해 반복:\n","    -    a. 데이터셋을 생성합니다.\n","    -    b. 데이터를 훈련 세트와 검증 세트로 나눕니다.\n","    -    c. 모델을 생성하고 훈련시킵니다.\n","    -    d. 최종 모델을 저장합니다.\n"],"metadata":{"id":"9ibIWD6abZGz"}}]}