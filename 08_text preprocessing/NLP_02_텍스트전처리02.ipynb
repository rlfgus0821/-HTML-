{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클렌징(cleansing)\n",
    "- 토큰화(Tokenization)\n",
    "- 필터링/ 스톱 워드(불용어) 제거 / 철자 수정\n",
    "- 어간 추출(Stemming & Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 토큰화(Text Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서에서 문장을 분리하는 **`문장 토큰화`**\n",
    "- 문장에서 단어를 토큰으로 분리하는 **`단어 토큰화`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장의 마침표(.), 개행문자(\\n), ? 등 문장의 마지막을 뜻하는 기호에 따라 분리\n",
    "- 문장 분류(sentence segmentation)\n",
    "- 코퍼스가 정제되지 않은 상태(문장 단위로 구분되어 있지 않는 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `!`와 `?`는 확실하게 문장 구분역할을 하나 `.`는 경우에 따라 문장을 구분하지 못하는 경우가 있음\n",
    "- 예1.\n",
    "```python\n",
    "'''IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서\n",
    "aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.'''\n",
    "```\r",
    "- 예2.\n",
    "```python\n",
    "\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정규 표현식에 따른 문장 토큰화\n",
    "- NLTK의 **`sent_tokenize` API**를 많이 사용\n",
    "- 단어사전과 같이 참조가 필요한 데이터 세트는 인터넷으로 다운로드 가능\n",
    "    - 예. nltk.download('punkt') : 마침표, 개행문자 등의 데이터 세트 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예. 마침표, 개행문자 등의 데이터세트 다운로드 및 문장 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = 'NLTK is a leading platform for building Python programs to work with human language data. \\\n",
    "It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,\\\n",
    "along with a suite of text processing libraries for classification, tokenization, stemming,\\\n",
    "tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries,\\\n",
    "and an active discussion forum.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK is a leading platform for building Python programs to work with human language data.',\n",
       " 'It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,along with a suite of text processing libraries for classification, tokenization, stemming,tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries,and an active discussion forum.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = sent_tokenize(text_sample)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample2 = \"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = sent_tokenize(text_sample2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서\\naaa@gmail.com로 결과 좀 보내줘.',\n",
       " '그 후 점심 먹으러 가자.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sample3 = '''IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서\n",
    "aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.'''\n",
    "sent_tokenize(text_sample3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장을 단어로 토큰화\n",
    "- 공백, 콤마(,), 마침표(.), 개행문자 등으로 단어를 분리\n",
    "- 정규표현식을 이용해 다양한 토큰화 수행\n",
    "- Bag of Word와 같이 단어의 순서가 중요하지 않은 경우는 문장 토큰화를 수행하지 않고 단어 토큰화만 사용해도 충분함\n",
    "- 문장 토큰화는 각 문장이 가지는 시맨틱적 의미가 중요한 요소로 사용될 때 이용\n",
    "- 단어 토큰화를 위해 NLTK의 **`word_tokenize` API** 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'leading',\n",
       " 'platform',\n",
       " 'for',\n",
       " 'building',\n",
       " 'Python',\n",
       " 'programs',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'NLTK is a leading platform for building Python programs to work with human language data.'\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Since',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'actively',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'Ph.D.',\n",
       " 'students',\n",
       " ',',\n",
       " 'I',\n",
       " 'get',\n",
       " 'the',\n",
       " 'same',\n",
       " 'question',\n",
       " 'a',\n",
       " 'dozen',\n",
       " 'times',\n",
       " 'every',\n",
       " 'year',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = \"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"\n",
    "word_tokenize(sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아포스트로피(')가 있는 문장에 대한 단어 토큰화\n",
    "\n",
    "- NLTK의 WordPunctTokenizer를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize결과 ['It', \"'s\", 'nothing', 'that', 'you', 'do', \"n't\", 'already', 'know', 'except', 'most', 'people', 'are', \"n't\", 'aware', 'of', 'how', 'their', 'inner', 'world', 'works', '.']\n",
      "\n",
      "WordPunctTokenizer결과 ['It', \"'\", 's', 'nothing', 'that', 'you', 'don', \"'\", 't', 'already', 'know', 'except', 'most', 'people', 'aren', \"'\", 't', 'aware', 'of', 'how', 'their', 'inner', 'world', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "sentence3 = \"It's nothing that you don't already know except most people aren't aware of how their inner world works.\"\n",
    "print(f'word_tokenize결과',word_tokenize(sentence3))\n",
    "print()\n",
    "print(f'WordPunctTokenizer결과',WordPunctTokenizer().tokenize(sentence3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예. 문장 토큰화와 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    result = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.'], ['It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'NLP', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_text(text_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고. 케라스의 text_to_word_sequence를 이용한 토큰화\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = \"It's nothing that you don't already know except most people aren't aware of how their inner world works.\"\n",
    "\n",
    "words = text_to_word_sequence(sentence)\n",
    "print(words)\n",
    "```\n",
    "\n",
    "- 모든 알파벳을 소문자로 바꾸면서 마침표나 컴마, 느낌표 등의 구두점을 제거함\n",
    "- 아포스트로피를 보존함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화에서 고려할 사항\n",
    "\n",
    "1. 구두점이나 특수 문자를 단순 제외해서는 안된다\n",
    "- 마침표(`.`)가 문장의 경계를 구분할 경우\n",
    "- 단어 자체에 구두점을 가지고 있는 경우 : 예. Ph.D,  AT&T\n",
    "- 특수문자의 `$`나 `/` : $45.55,  01/02/06 날짜\n",
    "- 숫자 사이에 `,` : 123,456,789\n",
    "\n",
    "2. 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
    "- 영어의 아포스트로피(') : 예. what're -> what are,  we're -> we are (re를 접어라고 함)\n",
    "- 하나의 단어인데 중간에 띄어쓰기가 있는 경우는 하나의 토큰으로 구분해야 함\n",
    "    - 예. rock 'n' roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표준 토큰화 예 : Penn Treebank Tokenization 규칙\n",
    "\n",
    "- 규칙1. 하이푼(-)으로 구성된 단어는 하나로 유지한다\n",
    "- 규칙2. dosen't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'nothing', 'that', 'you', 'do', \"n't\", 'already', 'know', 'exceptmost', 'people', 'are', \"n't\", 'aware', 'of', 'how', 'their', 'inner', 'world', 'works', '.']\n",
      "\n",
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n",
      "\n",
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"It's nothing that you don't already know except\\\n",
    "most people aren't aware of how their inner world works.\"\n",
    "sentence2 = \"Starting a home-based restaurant may be an ideal. \\\n",
    "it doesn't have a food chain or restaurant of their own.\"\n",
    "word1 = TreebankWordTokenizer().tokenize(sentence1)\n",
    "word2 = TreebankWordTokenizer().tokenize(sentence2)\n",
    "word3 = word_tokenize(sentence2)\n",
    "\n",
    "print(word1)\n",
    "print()\n",
    "print(word2)\n",
    "print()\n",
    "print(word3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 문장 토큰화\n",
    "- KSS(Korean Sentence Splitter)\n",
    "   - 박상길 개발\n",
    "   - https://github.com/hyunwoongko/kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting kss\n",
      "  Using cached kss-5.2.0.tar.gz (88 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting emoji==1.2.0 (from kss)\n",
      "  Using cached emoji-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from kss) (2023.10.3)\n",
      "Collecting pecab (from kss)\n",
      "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
      "     ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 1.3/26.4 MB 42.3 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 3.0/26.4 MB 7.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 4.0/26.4 MB 9.2 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 4.6/26.4 MB 9.5 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 5.1/26.4 MB 9.4 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 6.1/26.4 MB 10.3 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 7.0/26.4 MB 10.9 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 7.8/26.4 MB 11.3 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 8.9/26.4 MB 11.6 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 9.8/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "     --------------- ----------------------- 10.6/26.4 MB 11.7 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 11.7/26.4 MB 11.7 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 13.2/26.4 MB 17.2 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 14.1/26.4 MB 16.4 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 14.9/26.4 MB 17.7 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 16.8/26.4 MB 18.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 17.7/26.4 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 18.6/26.4 MB 17.7 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 20.0/26.4 MB 18.7 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 21.1/26.4 MB 19.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 21.8/26.4 MB 18.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 22.8/26.4 MB 17.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 23.9/26.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 24.9/26.4 MB 18.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  25.8/26.4 MB 17.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  26.1/26.4 MB 16.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  26.4/26.4 MB 16.0 MB/s eta 0:00:01\n",
      "     --------------------------------------  26.4/26.4 MB 16.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 26.4/26.4 MB 14.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from kss) (3.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from pecab->kss) (1.26.4)\n",
      "Requirement already satisfied: pyarrow in c:\\programdata\\anaconda3\\lib\\site-packages (from pecab->kss) (14.0.2)\n",
      "Requirement already satisfied: pytest in c:\\programdata\\anaconda3\\lib\\site-packages (from pecab->kss) (7.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->pecab->kss) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->pecab->kss) (23.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->pecab->kss) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->pecab->kss) (0.4.6)\n",
      "Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "   ---------------------------------------- 0.0/131.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 131.3/131.3 kB 3.9 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: kss, pecab\n",
      "  Building wheel for kss (setup.py): started\n",
      "  Building wheel for kss (setup.py): finished with status 'done'\n",
      "  Created wheel for kss: filename=kss-5.2.0-py3-none-any.whl size=62589 sha256=b0b68e96c3814e7bfb4134504b02b6b538e184e3b69c68fc4bd14f7c423dacbf\n",
      "  Stored in directory: c:\\users\\gillhk\\appdata\\local\\pip\\cache\\wheels\\6a\\8e\\3b\\088eef586a82349f6c4cf16852268813547c3624f24ca3a480\n",
      "  Building wheel for pecab (setup.py): started\n",
      "  Building wheel for pecab (setup.py): finished with status 'done'\n",
      "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646702 sha256=8c9e4459c23faf20308f327956f011257e8a595d15c9545061e4fa243c635957\n",
      "  Stored in directory: c:\\users\\gillhk\\appdata\\local\\pip\\cache\\wheels\\c9\\0d\\97\\ca2bb361e44a80f4c63efe6f6438ff903fd1ab5640eedabc1b\n",
      "Successfully built kss pecab\n",
      "Installing collected packages: emoji, pecab, kss\n",
      "Successfully installed emoji-1.2.0 kss-5.2.0 pecab-1.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어처리가 재미있기는 합니다.', '그런데 영어보다 한국어로 처리할 때 너무 어려워요']\n",
      "['여러분 힘내요!', '시작이 반이에요.']\n"
     ]
    }
   ],
   "source": [
    "from kss import split_sentences\n",
    "text1 = '자연어처리가 재미있기는 합니다. 그런데 영어보다 한국어로 처리할 때 너무 어려워요'\n",
    "text2 = '여러분 힘내요! 시작이 반이에요.'\n",
    "print(split_sentences(text1))\n",
    "print(split_sentences(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KoNLPy에서 제공하는 한글 형태소 분석기를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['여러분 힘 내요!', '시작이 반이에요.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "Kkma().sentences(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어', '처리', '가', '재미있', '기', '는', '하', 'ㅂ니다', '.', '그런데', '영어', '보다', '한국어', '로', '처리', '하', 'ㄹ', '때', '너무', '어렵', '어요']\n"
     ]
    }
   ],
   "source": [
    "print(Kkma().morphs(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['여러분', '힘', '내', '이', '요', '!', '시작', '이', '반', '이', '에요', '.']\n"
     ]
    }
   ],
   "source": [
    "print(Kkma().morphs(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['여러분', '힘', '힘내', '내', '시작', '반']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kkma().nouns(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어에서 토큰화의 어려움\n",
    "\n",
    "**1. 교착어 특성**\n",
    "- 다양한 조사가 띄어쓰기 없이 바로 붙어 있어 다른 단어로 보임(조사 분리 필요)\n",
    "   - 예. 그(he/him) : 그가, 그에게, 그를, 그와, 그는\n",
    "- 형태소(morpheme) : 뜻을 가진 가장 작은 말의 단위\n",
    "   - 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소\n",
    "       - 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사\n",
    "       - 그 자체로 단어가 됨\n",
    "   - 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소\n",
    "       - 접사, 어미, 조사, 어간\n",
    "   - 예. 나래가 책을 읽었다\n",
    "       - 자립형태소 : 나래, 책\n",
    "       - 의존형태소 : -가, -을, 읽-, -었, -다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 띄어쓰기가 영어보다 잘 지켜지지 않음**\n",
    "- 한국어는 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어\n",
    "- 띄어쓰기 보편화 : 1933년 한글맞춤법통일안\n",
    "- 한국어 모아쓰기 방식, 영어는 풀어쓰기 방식\n",
    "- 예.\n",
    "    - 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.\n",
    "    - Tobeornottobethatisthequestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사 태깅(part-of-speech tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어는 품사에 따라서 의미가 달라짐\n",
    "    - 영어 'fly' : 동사-'날다', 명사-'파리'\n",
    "    - 한국어 '못' : 명사-'망치를 사용해 목재 따위를 고정하는 물건', 부사-'동작 동사를 할 수 없다는 의미'\n",
    "- 품사태깅 : 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지 구분하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영어 품사 태깅\n",
    "- nltk : Penn Treebank POS Tags 기준으로 품사 태깅\n",
    "    - PRP : 인칭대명사\n",
    "    - VBP : 동사\n",
    "    - RB : 부사\n",
    "    - VBG : 현재부사\n",
    "    - IN : 전치사\n",
    "    - NNP : 고유명사\n",
    "    - NNS : 복수형명사\n",
    "    - CC : 접속사\n",
    "    - DT : 관사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 품사 태깅\n",
    "\n",
    "- 한국어 단어 토큰화\n",
    "- KoNLPy의 형태소 분석기\n",
    "    - Okt(Open Korea Text)\n",
    "    - 메캅(Mecab)\n",
    "    - 코모란(Komoran)\n",
    "    - 한나눔(Hannanum)\n",
    "    - 꼬꼬마(Kkma)\n",
    "\n",
    "- 형태소 분석기 메서드\n",
    "    - morphs : 형태소 추출\n",
    "    - pos : 품사 태깅\n",
    "    - nouns : 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 정제와 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거    \n",
    "- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어줌\n",
    "\n",
    "- 토큰화 작업에 방해가 되는 부분들을 배제시키기 위해 토큰화 작업 전과 후에 진행\n",
    "   1. 규칙에 기반한 표기가 다른 단어들 통합\n",
    "       - USA -> US\n",
    "       - uh-huh -> uhhuh\n",
    "   2. 대소문자 통합 : 소문자로 변환\n",
    "   3. 불필요한 단어의 제거\n",
    "       - 등장 빈도가 적은 단어\n",
    "       - 길이가 짧은 단어 : 길이가 2-3이하인 단어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stopwords 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 불용어 제거\n",
    "- 분석에 큰 의미가 없는 단어 제거\n",
    "    - is, the, a, will 등 필수 문법 요소이지만 문맥적으로 큰 의미가 없는 단어\n",
    "- 언어별 스톱 워드가 목록화되어 있음\n",
    "    - NLTK의 **`'stopwords'`** 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**영어의 불용어 목록**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예. 불용어 제거**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문법적 또는 의미적으로 변화하는 **`단어의 원형을 찾는 작업`**\n",
    "- 뿌리 단어를 찾아 단어의 개수를 줄임\n",
    "- 영어의 경우\n",
    "    - be동사 : am, ar, is\n",
    "    - 과거/현재, 3인칭 단수 여부, 진행형 등은 원래 단어가 변형된 것\n",
    "        - 예. work : worked, working, works ...    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어의 원형을 찾아가는 방식\n",
    "- 단어의 형태학(morphology)적 파싱을 진행\n",
    "- 형태소의 종류\n",
    "    - 어간(stem) : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "    - 접사(affix) : 단어에 추가적인 의미를 주는 부분\n",
    "        - 예. cats : cat(어간) + -s(접사)\n",
    "- Stemming(어간 추출)과 Lemmatization(표제어 추출)\n",
    "    - 표제어 추출이 어간 추출 보다 더 정교하며 의미론적 기반에서 단어의 원형을 찾음\n",
    "    - 표제어 추출이 어간 추출 보다 변환에 더 오랜 시간을 필요로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Stemming(어간 추출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 형태학적 분석을 단순화한 버전\n",
    "- 정해진 규칙만 보고 어미를 자르는 어림짐작의 작업\n",
    "- 원형 단어로 변환시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있음\n",
    "- NLTK의 Stemmer : Porter, Lancaster, Snowball Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLTK의 **`LancasterStemmer`** 또는 **`PorterStemmer`** API를 이용\n",
    "    - 진행형, 3인칭 단수, 과거형에 따른 동사, 비교, 최상에 따른 형용사 변화에 대한 더 단순한 원형 단어를 찾아 줌   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stemming 단계    \n",
    "    1. LancasterStemmer() 객체 생성\n",
    "    2. stem('원하는단어') 메서드 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 랭커스터 알고리즘(LancasterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 포터 알고리즘(PorterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> 랭커스터 알고리즘이 포터 알고리즘에 비해 단어 원형을 알아볼 수 없을 정도로 축소 시키므로, 데이터셋을 축소시켜야 하는 특정 상황에서 더 유용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제1. 다음 문장을 nltk의 PorterStemmer 추출 방식을 사용하여 단어 토큰화를 진행해보시오.\n",
    "```python\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제2. 다음 단어들을 nltk의 두 가지 어간 추출 방식을 사용하여 단어 토큰화를 수행하고 그 결과를 비교하시오.\r",
    "```python\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "```.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> 어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, Porter 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Lemmatization(표제어 추출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lemma : 표제어, 기본 사전형 단어\n",
    "- 표제어 추출은 표제어를 찾아가는 과정\n",
    "- 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아 줌\n",
    "- 어간 추출보다 성능이 더 좋음\n",
    "- 품사와 같은 문법 뿐만아니라 문장 내에서 단어 의미도 고려함\n",
    "- 어간 추출보다 시간이 더 걸림\n",
    "- NLTK의 Lemmatizer : WordNetLemmatizer    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`WordNetLemmatizer`** API를 이용한 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어의 품사 정보를 알면 더 정확한 결과를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 한국어 어간 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한국어는 5언 9품사의 구조\n",
    "  - 체언 : 명사, 대명사, 수사\n",
    "  - 수식언 : 관형사, 부사\n",
    "  - 관계언 : 조사\n",
    "  - 독립언 : 감탄사\n",
    "  - 용언 : 동사, 형용사\n",
    "    - 동사와 형용사는 어간(stem)과 어미(ending)의 결합으로 구성되므로 용언이라고 언급할 경우 동사와 형용사를 포함하여 언급한 것임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 활용(conjugation)\n",
    "- 활용이란 용언의 어간(stem)이 어미(ending)을 가지는 일\n",
    "- 한국어, 인도유럽어에서 볼 수 있는 언어적 특징의 통칭적인 개념\n",
    "- 어간(stem) : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분\n",
    "    - 어간의 모양이 바뀔 수 있음(예. 긋다, 긋고, 그어서, 그어라)\n",
    "- 어미(ending) : 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분, 여러 문법적 기능을 수행\n",
    "- 활용의 구분\n",
    "    - 규칙활용\n",
    "    - 불규칙활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**규칙 활용**\n",
    "- 어간이 어미를 취할 때 어간의 모습이 일정한 경우\n",
    "- 어간과 어미를 합칠 때 어간의 형태가 바뀌지 않는 경우\n",
    "- 예. 잡다 : 잡(어간) + 다(어미)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**불규칙 활용**\n",
    "- 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우\n",
    "- 어간의 형식이 달라지는 경우\n",
    "    - 예. 듣/들-, 돕/도우-, 곱/고우-, 잇/이-, 올/올-, 노랗/노라-’\n",
    "- 특수한 어미를 취하는 경우\n",
    "    - 예. 오르+ 아/어→올라, 하+아/어→하여, 이르+아/어→이르러, 푸르+아/어→푸르러 \n",
    "- https://namu.wiki/w/한국어/불규칙%20활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
