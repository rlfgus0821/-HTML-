{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클렌징(cleansing)\n",
    "- 토큰화(Tokenization)\n",
    "- 필터링/ 스톱 워드(불용어) 제거 / 철자 수정\n",
    "- 어간 추출(Stemming & Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 토큰화(Text Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서에서 문장을 분리하는 **`문장 토큰화`**\n",
    "- 문장에서 단어를 토큰으로 분리하는 **`단어 토큰화`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장의 마침표(.), 개행문자(\\n), ? 등 문장의 마지막을 뜻하는 기호에 따라 분리\n",
    "- 문장 분류(sentence segmentation)\n",
    "- 코퍼스가 정제되지 않은 상태(문장 단위로 구분되어 있지 않는 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `!`와 `?`는 확실하게 문장 구분역할을 하나 `.`는 경우에 따라 문장을 구분하지 못하는 경우가 있음\n",
    "- 예1.\n",
    "```python\n",
    "'''IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서\n",
    "aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.'''\n",
    "```\r",
    "- 예2.\n",
    "```python\n",
    "\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정규 표현식에 따른 문장 토큰화\n",
    "- NLTK의 **`sent_tokenize` API**를 많이 사용\n",
    "- 단어사전과 같이 참조가 필요한 데이터 세트는 인터넷으로 다운로드 가능\n",
    "    - 예. nltk.download('punkt') : 마침표, 개행문자 등의 데이터 세트 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예. 마침표, 개행문자 등의 데이터세트 다운로드 및 문장 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = 'NLTK is a leading platform for building Python programs to work with human language data. \\\n",
    "It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,\\\n",
    "along with a suite of text processing libraries for classification, tokenization, stemming,\\\n",
    "tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries,\\\n",
    "and an active discussion forum.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK is a leading platform for building Python programs to work with human language data.',\n",
       " 'It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,along with a suite of text processing libraries for classification, tokenization, stemming,tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries,and an active discussion forum.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = sent_tokenize(text_sample)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample2 = \"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = sent_tokenize(text_sample2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서\\naaa@gmail.com로 결과 좀 보내줘.',\n",
       " '그 후 점심 먹으러 가자.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sample3 = '''IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서\n",
    "aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.'''\n",
    "sent_tokenize(text_sample3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장을 단어로 토큰화\n",
    "- 공백, 콤마(,), 마침표(.), 개행문자 등으로 단어를 분리\n",
    "- 정규표현식을 이용해 다양한 토큰화 수행\n",
    "- Bag of Word와 같이 단어의 순서가 중요하지 않은 경우는 문장 토큰화를 수행하지 않고 단어 토큰화만 사용해도 충분함\n",
    "- 문장 토큰화는 각 문장이 가지는 시맨틱적 의미가 중요한 요소로 사용될 때 이용\n",
    "- 단어 토큰화를 위해 NLTK의 **`word_tokenize` API** 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'leading',\n",
       " 'platform',\n",
       " 'for',\n",
       " 'building',\n",
       " 'Python',\n",
       " 'programs',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'NLTK is a leading platform for building Python programs to work with human language data.'\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Since',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'actively',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'Ph.D.',\n",
       " 'students',\n",
       " ',',\n",
       " 'I',\n",
       " 'get',\n",
       " 'the',\n",
       " 'same',\n",
       " 'question',\n",
       " 'a',\n",
       " 'dozen',\n",
       " 'times',\n",
       " 'every',\n",
       " 'year',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = \"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"\n",
    "word_tokenize(sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아포스트로피(')가 있는 문장에 대한 단어 토큰화\n",
    "\n",
    "- NLTK의 WordPunctTokenizer를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize결과 ['It', \"'s\", 'nothing', 'that', 'you', 'do', \"n't\", 'already', 'know', 'except', 'most', 'people', 'are', \"n't\", 'aware', 'of', 'how', 'their', 'inner', 'world', 'works', '.']\n",
      "\n",
      "WordPunctTokenizer결과 ['It', \"'\", 's', 'nothing', 'that', 'you', 'don', \"'\", 't', 'already', 'know', 'except', 'most', 'people', 'aren', \"'\", 't', 'aware', 'of', 'how', 'their', 'inner', 'world', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "sentence3 = \"It's nothing that you don't already know except most people aren't aware of how their inner world works.\"\n",
    "print(f'word_tokenize결과',word_tokenize(sentence3))\n",
    "print()\n",
    "print(f'WordPunctTokenizer결과',WordPunctTokenizer().tokenize(sentence3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예. 문장 토큰화와 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    result = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.'], ['It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'NLP', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_text(text_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고. 케라스의 text_to_word_sequence를 이용한 토큰화\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = \"It's nothing that you don't already know except most people aren't aware of how their inner world works.\"\n",
    "\n",
    "words = text_to_word_sequence(sentence)\n",
    "print(words)\n",
    "```\n",
    "\n",
    "- 모든 알파벳을 소문자로 바꾸면서 마침표나 컴마, 느낌표 등의 구두점을 제거함\n",
    "- 아포스트로피를 보존함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading grpcio-1.62.1-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading keras-3.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading optree-0.11.0-cp311-cp311-win_amd64.whl.metadata (46 kB)\n",
      "     ---------------------------------------- 0.0/46.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.2/46.2 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.16.1-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl (377.0 MB)\n",
      "   ---------------------------------------- 0.0/377.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.7/377.0 MB 22.8 MB/s eta 0:00:17\n",
      "   ---------------------------------------- 1.0/377.0 MB 8.7 MB/s eta 0:00:44\n",
      "   ---------------------------------------- 1.5/377.0 MB 8.5 MB/s eta 0:00:45\n",
      "   ---------------------------------------- 2.1/377.0 MB 9.5 MB/s eta 0:00:40\n",
      "   ---------------------------------------- 2.5/377.0 MB 8.9 MB/s eta 0:00:43\n",
      "   ---------------------------------------- 2.9/377.0 MB 8.3 MB/s eta 0:00:46\n",
      "   ---------------------------------------- 3.3/377.0 MB 7.8 MB/s eta 0:00:48\n",
      "   ---------------------------------------- 3.8/377.0 MB 8.0 MB/s eta 0:00:47\n",
      "   ---------------------------------------- 4.3/377.0 MB 8.4 MB/s eta 0:00:45\n",
      "    --------------------------------------- 5.0/377.0 MB 8.5 MB/s eta 0:00:44\n",
      "    --------------------------------------- 5.4/377.0 MB 8.5 MB/s eta 0:00:44\n",
      "    --------------------------------------- 5.8/377.0 MB 8.2 MB/s eta 0:00:46\n",
      "    --------------------------------------- 6.1/377.0 MB 7.8 MB/s eta 0:00:48\n",
      "    --------------------------------------- 7.0/377.0 MB 7.7 MB/s eta 0:00:48\n",
      "    --------------------------------------- 7.7/377.0 MB 7.6 MB/s eta 0:00:49\n",
      "    --------------------------------------- 8.4/377.0 MB 7.6 MB/s eta 0:00:49\n",
      "    --------------------------------------- 8.9/377.0 MB 7.5 MB/s eta 0:00:50\n",
      "   - -------------------------------------- 9.6/377.0 MB 7.7 MB/s eta 0:00:48\n",
      "   - -------------------------------------- 10.1/377.0 MB 7.7 MB/s eta 0:00:48\n",
      "   - -------------------------------------- 10.8/377.0 MB 7.6 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 11.6/377.0 MB 8.0 MB/s eta 0:00:46\n",
      "   - -------------------------------------- 12.3/377.0 MB 8.1 MB/s eta 0:00:46\n",
      "   - -------------------------------------- 13.3/377.0 MB 9.0 MB/s eta 0:00:41\n",
      "   - -------------------------------------- 14.2/377.0 MB 9.4 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 15.3/377.0 MB 10.1 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 16.2/377.0 MB 10.9 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 16.8/377.0 MB 11.9 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 16.8/377.0 MB 11.9 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 17.8/377.0 MB 12.4 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 18.7/377.0 MB 13.9 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 19.7/377.0 MB 15.2 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 20.7/377.0 MB 17.3 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 21.6/377.0 MB 17.7 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 22.5/377.0 MB 18.7 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 23.3/377.0 MB 18.7 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 24.1/377.0 MB 18.7 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 25.3/377.0 MB 18.7 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 26.3/377.0 MB 19.3 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 27.2/377.0 MB 21.8 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 28.6/377.0 MB 22.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 29.6/377.0 MB 22.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 30.5/377.0 MB 22.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 31.5/377.0 MB 22.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 32.5/377.0 MB 22.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 33.5/377.0 MB 23.4 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 34.4/377.0 MB 24.2 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 35.5/377.0 MB 23.4 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 36.6/377.0 MB 24.2 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 37.7/377.0 MB 25.1 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 38.5/377.0 MB 24.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 39.7/377.0 MB 24.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 40.9/377.0 MB 24.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 41.7/377.0 MB 24.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 43.0/377.0 MB 25.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 44.2/377.0 MB 26.2 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 45.4/377.0 MB 26.2 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 46.3/377.0 MB 26.2 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 47.6/377.0 MB 26.2 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 48.5/377.0 MB 26.2 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 49.4/377.0 MB 26.2 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 50.6/377.0 MB 26.2 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 51.1/377.0 MB 25.2 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 52.0/377.0 MB 25.2 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 52.8/377.0 MB 23.4 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 54.0/377.0 MB 24.2 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 55.1/377.0 MB 24.2 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 56.3/377.0 MB 24.2 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 57.4/377.0 MB 23.4 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 58.8/377.0 MB 25.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 59.6/377.0 MB 24.2 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 60.6/377.0 MB 23.4 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 61.4/377.0 MB 25.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 62.3/377.0 MB 25.1 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 63.1/377.0 MB 25.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 63.8/377.0 MB 24.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 64.7/377.0 MB 23.4 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 65.5/377.0 MB 22.5 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 66.5/377.0 MB 22.6 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 67.2/377.0 MB 21.8 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 68.4/377.0 MB 21.8 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 69.2/377.0 MB 21.1 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 70.2/377.0 MB 21.8 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 70.8/377.0 MB 20.5 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 71.4/377.0 MB 19.8 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 72.1/377.0 MB 19.8 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 73.4/377.0 MB 20.5 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 74.5/377.0 MB 21.1 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 75.7/377.0 MB 22.6 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 76.9/377.0 MB 22.5 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 78.0/377.0 MB 23.4 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 79.1/377.0 MB 23.4 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 80.2/377.0 MB 23.4 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 81.3/377.0 MB 26.2 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 82.1/377.0 MB 26.2 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 82.7/377.0 MB 25.1 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 83.5/377.0 MB 23.4 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 84.0/377.0 MB 23.4 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 84.6/377.0 MB 21.8 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 85.5/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 86.4/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 87.3/377.0 MB 20.5 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 88.5/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 90.1/377.0 MB 21.8 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 91.4/377.0 MB 21.8 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 92.4/377.0 MB 22.6 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 93.8/377.0 MB 26.2 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 94.4/377.0 MB 27.3 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 94.6/377.0 MB 25.2 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 95.6/377.0 MB 25.1 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 96.4/377.0 MB 25.2 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 97.4/377.0 MB 25.2 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 98.3/377.0 MB 24.2 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 99.1/377.0 MB 23.4 MB/s eta 0:00:12\n",
      "   ---------- ---------------------------- 100.1/377.0 MB 21.9 MB/s eta 0:00:13\n",
      "   ---------- ---------------------------- 101.3/377.0 MB 21.8 MB/s eta 0:00:13\n",
      "   ---------- ---------------------------- 102.4/377.0 MB 21.8 MB/s eta 0:00:13\n",
      "   ---------- ---------------------------- 103.8/377.0 MB 22.6 MB/s eta 0:00:13\n",
      "   ---------- ---------------------------- 105.0/377.0 MB 25.1 MB/s eta 0:00:11\n",
      "   ---------- ---------------------------- 106.2/377.0 MB 26.2 MB/s eta 0:00:11\n",
      "   ----------- --------------------------- 107.2/377.0 MB 26.2 MB/s eta 0:00:11\n",
      "   ----------- --------------------------- 108.4/377.0 MB 27.3 MB/s eta 0:00:10\n",
      "   ----------- --------------------------- 109.6/377.0 MB 28.4 MB/s eta 0:00:10\n",
      "   ----------- --------------------------- 110.6/377.0 MB 29.7 MB/s eta 0:00:09\n",
      "   ----------- --------------------------- 111.1/377.0 MB 27.3 MB/s eta 0:00:10\n",
      "   ----------- --------------------------- 112.4/377.0 MB 26.2 MB/s eta 0:00:11\n",
      "   ----------- --------------------------- 113.7/377.0 MB 27.3 MB/s eta 0:00:10\n",
      "   ----------- --------------------------- 115.5/377.0 MB 28.4 MB/s eta 0:00:10\n",
      "   ------------ -------------------------- 116.7/377.0 MB 28.4 MB/s eta 0:00:10\n",
      "   ------------ -------------------------- 117.4/377.0 MB 27.3 MB/s eta 0:00:10\n",
      "   ------------ -------------------------- 118.3/377.0 MB 26.2 MB/s eta 0:00:10\n",
      "   ------------ -------------------------- 119.2/377.0 MB 24.2 MB/s eta 0:00:11\n",
      "   ------------ -------------------------- 120.2/377.0 MB 24.2 MB/s eta 0:00:11\n",
      "   ------------ -------------------------- 121.0/377.0 MB 23.4 MB/s eta 0:00:11\n",
      "   ------------ -------------------------- 122.4/377.0 MB 25.2 MB/s eta 0:00:11\n",
      "   ------------ -------------------------- 123.4/377.0 MB 25.2 MB/s eta 0:00:11\n",
      "   ------------ -------------------------- 124.8/377.0 MB 24.2 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 126.1/377.0 MB 24.2 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 127.4/377.0 MB 24.2 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 129.0/377.0 MB 28.5 MB/s eta 0:00:09\n",
      "   ------------- ------------------------- 130.0/377.0 MB 29.7 MB/s eta 0:00:09\n",
      "   ------------- ------------------------- 131.2/377.0 MB 31.2 MB/s eta 0:00:08\n",
      "   ------------- ------------------------- 132.5/377.0 MB 29.7 MB/s eta 0:00:09\n",
      "   ------------- ------------------------- 133.8/377.0 MB 31.2 MB/s eta 0:00:08\n",
      "   ------------- ------------------------- 135.0/377.0 MB 31.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 136.2/377.0 MB 29.7 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 137.2/377.0 MB 28.4 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 138.4/377.0 MB 28.5 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 139.5/377.0 MB 27.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 140.5/377.0 MB 27.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 141.6/377.0 MB 27.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 142.7/377.0 MB 27.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 144.1/377.0 MB 27.3 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 144.6/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 145.8/377.0 MB 25.2 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 147.1/377.0 MB 25.2 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 148.3/377.0 MB 25.1 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 149.3/377.0 MB 25.1 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 150.5/377.0 MB 25.2 MB/s eta 0:00:09\n",
      "   --------------- ----------------------- 151.6/377.0 MB 26.2 MB/s eta 0:00:09\n",
      "   --------------- ----------------------- 152.6/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 153.5/377.0 MB 23.4 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 154.4/377.0 MB 23.4 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 155.3/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 156.6/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 158.0/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 158.8/377.0 MB 24.2 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 160.2/377.0 MB 25.2 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 161.4/377.0 MB 25.2 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 162.4/377.0 MB 24.2 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 163.2/377.0 MB 24.2 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 163.6/377.0 MB 22.6 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 164.0/377.0 MB 21.1 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 164.6/377.0 MB 20.5 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 165.1/377.0 MB 19.3 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 165.8/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 166.8/377.0 MB 18.2 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 168.0/377.0 MB 17.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 169.1/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 170.0/377.0 MB 17.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 171.3/377.0 MB 17.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 172.6/377.0 MB 17.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 173.7/377.0 MB 19.8 MB/s eta 0:00:11\n",
      "   ------------------ -------------------- 174.7/377.0 MB 23.4 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 176.1/377.0 MB 27.3 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 177.2/377.0 MB 28.4 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 178.7/377.0 MB 28.5 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 180.0/377.0 MB 29.8 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 180.9/377.0 MB 28.5 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 182.6/377.0 MB 31.2 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 184.2/377.0 MB 31.2 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 185.7/377.0 MB 31.2 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 187.0/377.0 MB 32.8 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 188.2/377.0 MB 32.8 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 188.9/377.0 MB 29.7 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 190.5/377.0 MB 31.2 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 191.6/377.0 MB 32.8 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 193.0/377.0 MB 31.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 194.4/377.0 MB 29.7 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 195.6/377.0 MB 31.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 196.9/377.0 MB 29.7 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 198.2/377.0 MB 31.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 199.2/377.0 MB 31.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 200.5/377.0 MB 29.7 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 201.6/377.0 MB 29.7 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 202.8/377.0 MB 28.5 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 204.0/377.0 MB 28.4 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 205.1/377.0 MB 27.3 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 206.0/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 207.0/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 208.2/377.0 MB 25.1 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 209.2/377.0 MB 25.1 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 210.4/377.0 MB 25.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 211.8/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 213.0/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 214.0/377.0 MB 25.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 215.0/377.0 MB 25.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 216.1/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 217.0/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 218.1/377.0 MB 25.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 219.5/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 220.7/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 222.4/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 223.7/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 224.8/377.0 MB 27.3 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 226.1/377.0 MB 28.5 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 227.0/377.0 MB 27.3 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 228.4/377.0 MB 29.7 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 229.6/377.0 MB 28.4 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 231.0/377.0 MB 29.8 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 232.2/377.0 MB 28.4 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 233.5/377.0 MB 28.4 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 234.9/377.0 MB 29.7 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 236.0/377.0 MB 29.7 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 237.3/377.0 MB 31.1 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 238.2/377.0 MB 28.5 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 239.0/377.0 MB 28.4 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 240.1/377.0 MB 28.4 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 241.4/377.0 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 242.4/377.0 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 243.3/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 244.3/377.0 MB 25.2 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 245.2/377.0 MB 23.4 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 246.3/377.0 MB 24.2 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 247.5/377.0 MB 23.4 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 248.5/377.0 MB 23.4 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 249.3/377.0 MB 23.4 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 250.0/377.0 MB 21.8 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 250.8/377.0 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 251.7/377.0 MB 20.5 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 252.5/377.0 MB 19.9 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 253.5/377.0 MB 20.5 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 254.6/377.0 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 255.8/377.0 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 256.7/377.0 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 257.5/377.0 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 258.2/377.0 MB 20.5 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 259.6/377.0 MB 21.8 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 260.9/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 262.1/377.0 MB 25.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 263.2/377.0 MB 25.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 264.2/377.0 MB 24.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 265.3/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 266.3/377.0 MB 25.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 267.4/377.0 MB 25.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 268.6/377.0 MB 28.5 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 269.7/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 270.9/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 272.2/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 273.2/377.0 MB 25.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 274.7/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 276.0/377.0 MB 28.5 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 277.1/377.0 MB 28.5 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 278.5/377.0 MB 29.7 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 279.7/377.0 MB 29.7 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 280.6/377.0 MB 29.7 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 281.8/377.0 MB 29.7 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 282.8/377.0 MB 28.5 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 284.1/377.0 MB 28.4 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 285.3/377.0 MB 28.5 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 286.3/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 287.6/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 289.1/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 290.1/377.0 MB 28.5 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 291.3/377.0 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 293.1/377.0 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 294.3/377.0 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 295.4/377.0 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 296.6/377.0 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 297.8/377.0 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 298.7/377.0 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 300.0/377.0 MB 28.4 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 301.3/377.0 MB 28.4 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 303.0/377.0 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 304.3/377.0 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 305.9/377.0 MB 31.1 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 307.0/377.0 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 308.3/377.0 MB 32.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 308.3/377.0 MB 32.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 308.3/377.0 MB 32.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 308.3/377.0 MB 32.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 308.7/377.0 MB 21.8 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 310.0/377.0 MB 21.9 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 311.2/377.0 MB 21.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 312.2/377.0 MB 20.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 313.0/377.0 MB 19.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 314.4/377.0 MB 19.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 315.5/377.0 MB 18.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 316.5/377.0 MB 18.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 317.6/377.0 MB 18.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 318.7/377.0 MB 25.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 319.8/377.0 MB 25.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 321.1/377.0 MB 25.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 322.1/377.0 MB 25.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 323.1/377.0 MB 25.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 324.6/377.0 MB 26.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 325.8/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 327.5/377.0 MB 28.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 328.2/377.0 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 328.2/377.0 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 328.2/377.0 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 328.5/377.0 MB 19.9 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 329.7/377.0 MB 19.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 330.5/377.0 MB 19.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 331.3/377.0 MB 18.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 332.2/377.0 MB 18.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 333.4/377.0 MB 19.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 334.4/377.0 MB 18.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 335.2/377.0 MB 17.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 336.4/377.0 MB 17.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 337.6/377.0 MB 17.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 338.8/377.0 MB 23.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 339.9/377.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 341.3/377.0 MB 25.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 342.3/377.0 MB 25.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 343.2/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 344.6/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 345.5/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 346.8/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 348.0/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 349.3/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 350.3/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 351.3/377.0 MB 24.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 352.5/377.0 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 353.4/377.0 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 354.2/377.0 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 355.1/377.0 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 356.2/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 357.4/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 358.6/377.0 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 359.3/377.0 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 360.2/377.0 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 361.7/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 362.5/377.0 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 363.3/377.0 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 364.6/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 365.8/377.0 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 366.9/377.0 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  368.0/377.0 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  369.3/377.0 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  370.9/377.0 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  372.1/377.0 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  372.6/377.0 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  373.5/377.0 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  374.3/377.0 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.3/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  376.5/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 377.0/377.0 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.7/133.7 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.62.1-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.8/3.8 MB 25.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.6/3.8 MB 20.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.7/3.8 MB 21.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.9/3.0 MB 29.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.0/3.0 MB 31.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 31.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 21.1 MB/s eta 0:00:00\n",
      "Downloading keras-3.2.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.1/1.1 MB 33.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 22.8 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.2/26.4 MB 38.1 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 2.4/26.4 MB 30.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.0/26.4 MB 35.9 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 5.2/26.4 MB 32.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.6/26.4 MB 32.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.5/26.4 MB 32.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.9/26.4 MB 31.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 10.0/26.4 MB 30.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 11.6/26.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 13.0/26.4 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 14.1/26.4 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.2/26.4 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.5/26.4 MB 28.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.8/26.4 MB 29.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.2/26.4 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 20.3/26.4 MB 29.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.5/26.4 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.4/26.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.5/26.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.1/26.4 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 25.2 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl (127 kB)\n",
      "   ---------------------------------------- 0.0/127.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 127.7/127.7 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.5/65.5 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.5 MB 31.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.5/5.5 MB 32.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.5/5.5 MB 31.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.5 MB 32.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 31.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 27.0 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 34.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 18.9 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp311-cp311-win_amd64.whl (245 kB)\n",
      "   ---------------------------------------- 0.0/245.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 245.0/245.0 kB 14.7 MB/s eta 0:00:00\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.11.0 keras-3.2.1 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\gillhk\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\gillhk\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"it's\", 'nothing', 'that', 'you', \"don't\", 'already', 'know', 'except', 'most', 'people', \"aren't\", 'aware', 'of', 'how', 'their', 'inner', 'world', 'works']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = \"It's nothing that you don't already know except most people aren't aware of how their inner world works.\"\n",
    "\n",
    "words = text_to_word_sequence(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화에서 고려할 사항\n",
    "\n",
    "1. 구두점이나 특수 문자를 단순 제외해서는 안된다\n",
    "- 마침표(`.`)가 문장의 경계를 구분할 경우\n",
    "- 단어 자체에 구두점을 가지고 있는 경우 : 예. Ph.D,  AT&T\n",
    "- 특수문자의 `$`나 `/` : $45.55,  01/02/06 날짜\n",
    "- 숫자 사이에 `,` : 123,456,789\n",
    "\n",
    "2. 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
    "- 영어의 아포스트로피(') : 예. what're -> what are,  we're -> we are (re를 접어라고 함)\n",
    "- 하나의 단어인데 중간에 띄어쓰기가 있는 경우는 하나의 토큰으로 구분해야 함\n",
    "    - 예. rock 'n' roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표준 토큰화 예 : Penn Treebank Tokenization 규칙\n",
    "\n",
    "- 규칙1. 하이푼(-)으로 구성된 단어는 하나로 유지한다\n",
    "- 규칙2. dosen't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'nothing', 'that', 'you', 'do', \"n't\", 'already', 'know', 'exceptmost', 'people', 'are', \"n't\", 'aware', 'of', 'how', 'their', 'inner', 'world', 'works', '.']\n",
      "\n",
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n",
      "\n",
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"It's nothing that you don't already know except\\\n",
    "most people aren't aware of how their inner world works.\"\n",
    "sentence2 = \"Starting a home-based restaurant may be an ideal. \\\n",
    "it doesn't have a food chain or restaurant of their own.\"\n",
    "word1 = TreebankWordTokenizer().tokenize(sentence1)\n",
    "word2 = TreebankWordTokenizer().tokenize(sentence2)\n",
    "word3 = word_tokenize(sentence2)\n",
    "\n",
    "print(word1)\n",
    "print()\n",
    "print(word2)\n",
    "print()\n",
    "print(word3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 문장 토큰화\n",
    "- KSS(Korean Sentence Splitter)\n",
    "   - 박상길 개발\n",
    "   - https://github.com/hyunwoongko/kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting kss\n",
      "  Using cached kss-5.2.0.tar.gz (88 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting emoji==1.2.0 (from kss)\n",
      "  Using cached emoji-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from kss) (2023.10.3)\n",
      "Collecting pecab (from kss)\n",
      "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
      "     ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 1.3/26.4 MB 42.3 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 2.1/26.4 MB 33.5 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 3.0/26.4 MB 7.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 4.0/26.4 MB 9.2 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 4.6/26.4 MB 9.5 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 5.1/26.4 MB 9.4 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 6.1/26.4 MB 10.3 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 7.0/26.4 MB 10.9 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 7.8/26.4 MB 11.3 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 8.9/26.4 MB 11.6 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 9.8/26.4 MB 11.9 MB/s eta 0:00:02\n",
      "     --------------- ----------------------- 10.6/26.4 MB 11.7 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 11.7/26.4 MB 11.7 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 13.2/26.4 MB 17.2 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 14.1/26.4 MB 16.4 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 14.9/26.4 MB 17.7 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 16.8/26.4 MB 18.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 17.7/26.4 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 18.6/26.4 MB 17.7 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 20.0/26.4 MB 18.7 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 21.1/26.4 MB 19.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 21.8/26.4 MB 18.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 22.8/26.4 MB 17.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 23.9/26.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 24.9/26.4 MB 18.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  25.8/26.4 MB 17.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  26.1/26.4 MB 16.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  26.4/26.4 MB 16.0 MB/s eta 0:00:01\n",
      "     --------------------------------------  26.4/26.4 MB 16.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 26.4/26.4 MB 14.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from kss) (3.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from pecab->kss) (1.26.4)\n",
      "Requirement already satisfied: pyarrow in c:\\programdata\\anaconda3\\lib\\site-packages (from pecab->kss) (14.0.2)\n",
      "Requirement already satisfied: pytest in c:\\programdata\\anaconda3\\lib\\site-packages (from pecab->kss) (7.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->pecab->kss) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->pecab->kss) (23.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->pecab->kss) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->pecab->kss) (0.4.6)\n",
      "Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "   ---------------------------------------- 0.0/131.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 131.3/131.3 kB 3.9 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: kss, pecab\n",
      "  Building wheel for kss (setup.py): started\n",
      "  Building wheel for kss (setup.py): finished with status 'done'\n",
      "  Created wheel for kss: filename=kss-5.2.0-py3-none-any.whl size=62589 sha256=b0b68e96c3814e7bfb4134504b02b6b538e184e3b69c68fc4bd14f7c423dacbf\n",
      "  Stored in directory: c:\\users\\gillhk\\appdata\\local\\pip\\cache\\wheels\\6a\\8e\\3b\\088eef586a82349f6c4cf16852268813547c3624f24ca3a480\n",
      "  Building wheel for pecab (setup.py): started\n",
      "  Building wheel for pecab (setup.py): finished with status 'done'\n",
      "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646702 sha256=8c9e4459c23faf20308f327956f011257e8a595d15c9545061e4fa243c635957\n",
      "  Stored in directory: c:\\users\\gillhk\\appdata\\local\\pip\\cache\\wheels\\c9\\0d\\97\\ca2bb361e44a80f4c63efe6f6438ff903fd1ab5640eedabc1b\n",
      "Successfully built kss pecab\n",
      "Installing collected packages: emoji, pecab, kss\n",
      "Successfully installed emoji-1.2.0 kss-5.2.0 pecab-1.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어처리가 재미있기는 합니다.', '그런데 영어보다 한국어로 처리할 때 너무 어려워요']\n",
      "['여러분 힘내요!', '시작이 반이에요.']\n"
     ]
    }
   ],
   "source": [
    "from kss import split_sentences\n",
    "text1 = '자연어처리가 재미있기는 합니다. 그런데 영어보다 한국어로 처리할 때 너무 어려워요'\n",
    "text2 = '여러분 힘내요! 시작이 반이에요.'\n",
    "print(split_sentences(text1))\n",
    "print(split_sentences(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KoNLPy에서 제공하는 한글 형태소 분석기를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['여러분 힘 내요!', '시작이 반이에요.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "Kkma().sentences(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어', '처리', '가', '재미있', '기', '는', '하', 'ㅂ니다', '.', '그런데', '영어', '보다', '한국어', '로', '처리', '하', 'ㄹ', '때', '너무', '어렵', '어요']\n"
     ]
    }
   ],
   "source": [
    "print(Kkma().morphs(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['여러분', '힘', '내', '이', '요', '!', '시작', '이', '반', '이', '에요', '.']\n"
     ]
    }
   ],
   "source": [
    "print(Kkma().morphs(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['여러분', '힘', '힘내', '내', '시작', '반']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kkma().nouns(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어에서 토큰화의 어려움\n",
    "\n",
    "**1. 교착어 특성**\n",
    "- 다양한 조사가 띄어쓰기 없이 바로 붙어 있어 다른 단어로 보임(조사 분리 필요)\n",
    "   - 예. 그(he/him) : 그가, 그에게, 그를, 그와, 그는\n",
    "- 형태소(morpheme) : 뜻을 가진 가장 작은 말의 단위\n",
    "   - 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소\n",
    "       - 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사\n",
    "       - 그 자체로 단어가 됨\n",
    "   - 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소\n",
    "       - 접사, 어미, 조사, 어간\n",
    "   - 예. 나래가 책을 읽었다\n",
    "       - 자립형태소 : 나래, 책\n",
    "       - 의존형태소 : -가, -을, 읽-, -었, -다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 띄어쓰기가 영어보다 잘 지켜지지 않음**\n",
    "- 한국어는 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어\n",
    "- 띄어쓰기 보편화 : 1933년 한글맞춤법통일안\n",
    "- 한국어 모아쓰기 방식, 영어는 풀어쓰기 방식\n",
    "- 예.\n",
    "    - 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.\n",
    "    - Tobeornottobethatisthequestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사 태깅(part-of-speech tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어는 품사에 따라서 의미가 달라짐\n",
    "    - 영어 'fly' : 동사-'날다', 명사-'파리'\n",
    "    - 한국어 '못' : 명사-'망치를 사용해 목재 따위를 고정하는 물건', 부사-'동작 동사를 할 수 없다는 의미'\n",
    "- 품사태깅 : 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지 구분하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영어 품사 태깅\n",
    "- nltk : Penn Treebank POS Tags 기준으로 품사 태깅\n",
    "    - PRP : 인칭대명사\n",
    "    - VBP : 동사\n",
    "    - RB : 부사\n",
    "    - VBG : 현재부사\n",
    "    - IN : 전치사\n",
    "    - NNP : 고유명사\n",
    "    - NNS : 복수형명사\n",
    "    - CC : 접속사\n",
    "    - DT : 관사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어토큰화: \n",
      "['Since', 'I', \"'m\", 'actively', 'looking', 'for', 'Ph.D.', 'students', ',', 'I', 'get', 'the', 'same', 'question', 'a', 'dozen', 'times', 'every', 'year', '.']\n",
      "품사 태깅: \n",
      "[('Since', 'IN'), ('I', 'PRP'), (\"'m\", 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), (',', ','), ('I', 'PRP'), ('get', 'VBP'), ('the', 'DT'), ('same', 'JJ'), ('question', 'NN'), ('a', 'DT'), ('dozen', 'NN'), ('times', 'NNS'), ('every', 'DT'), ('year', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"\n",
    "token_sentence = word_tokenize(text)\n",
    "\n",
    "print(f'단어토큰화: \\n{token_sentence}')\n",
    "print(f'품사 태깅: \\n{pos_tag(token_sentence)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 품사 태깅\n",
    "\n",
    "- 한국어 단어 토큰화\n",
    "- KoNLPy의 형태소 분석기\n",
    "    - Okt(Open Korea Text)\n",
    "    - 메캅(Mecab)\n",
    "    - 코모란(Komoran)\n",
    "    - 한나눔(Hannanum)\n",
    "    - 꼬꼬마(Kkma)\n",
    "\n",
    "- 형태소 분석기 메서드\n",
    "    - morphs : 형태소 추출\n",
    "    - pos : 품사 태깅\n",
    "    - nouns : 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okt 형태소 분석: ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "Okt 품사 태깅: [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "Okt 명사 추출: ['코딩', '당신', '연휴', '여행']\n",
      "\n",
      "Kkma 형태소 분석: ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "Kkma 품사 태깅: [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "Kkma 명사 추출: ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt, Kkma\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "text1 = '열심히 코딩한 당신, 연휴에는 여행을 가봐요'\n",
    "text2 = '열심히 공부한 당신, 쉼을 가져봐요'\n",
    "\n",
    "print(f'Okt 형태소 분석:', okt.morphs(text1))\n",
    "print(f'Okt 품사 태깅:', okt.pos(text1))\n",
    "print(f'Okt 명사 추출:', okt.nouns(text1))\n",
    "print()\n",
    "print(f'Kkma 형태소 분석:', kkma.morphs(text1))\n",
    "print(f'Kkma 품사 태깅:', kkma.pos(text1))\n",
    "print(f'Kkma 명사 추출:', kkma.nouns(text1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komoran 형태소 분석: ['열심히', '코', '딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가', '아', '보', '아요']\n",
      "Komoran 품사 태깅: [('열심히', 'MAG'), ('코', 'NNG'), ('딩', 'MAG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('당신', 'NNP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가', 'VV'), ('아', 'EC'), ('보', 'VX'), ('아요', 'EC')]\n",
      "Komoran 명사 추출: ['코', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "text1 = '열심히 코딩한 당신, 연휴에는 여행을 가봐요'\n",
    "komoran = Komoran()\n",
    "\n",
    "print(f'Komoran 형태소 분석:', komoran.morphs(text1))\n",
    "print(f'Komoran 품사 태깅:', komoran.pos(text1))\n",
    "print(f'Komoran 명사 추출:', komoran.nouns(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kkma 품사 태그\n",
    "- NNG: 일반 명사\n",
    "- JKS: 주격 조사\n",
    "- JKM: 부사격 조사\n",
    "- VV: 동사\n",
    "- EFN: 평서형 종결 어미\n",
    "- SF: 마침표, 물음표, 느낌표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okt 품사 태그 \n",
    "- Noun: 명사\n",
    "- Verb: 동사\n",
    "- Adjective: 형용사\n",
    "- Determiner: 관형사 (참, 첫, 이, 그)\n",
    "- Adverb: 부사 (매우, 빨리, 반드시)\n",
    "- Conjunction: 접속사\n",
    "- Exclamation: 감탄사\n",
    "- Josa: 조사\n",
    "- PreEmoi: 선어말어미 (있)\n",
    "- Emoi: 어미 (요, 여)\n",
    "- Suffix: 접미사\n",
    "- Punctuation: 구두점,마침표, 물음표, 느낌표 등\n",
    "- Foreign: 외국어, 한자 및 기타 기호"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komoran 품사 태그(42개 품사지원)\n",
    "- NNG: 일반 명사\n",
    "- NNP: 고유 명사\n",
    "- VV: 동사\n",
    "- VA: 형용사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 ['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'this', 'room']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'Matrix'),\n",
       " ('Matrix', 'is'),\n",
       " ('is', 'everywhere'),\n",
       " ('everywhere', 'its'),\n",
       " ('its', 'all'),\n",
       " ('all', 'around'),\n",
       " ('around', 'us'),\n",
       " ('us', ','),\n",
       " (',', 'here'),\n",
       " ('here', 'even'),\n",
       " ('even', 'this'),\n",
       " ('this', 'room')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even this room'\n",
    "words = word_tokenize(sentence)\n",
    "print(f'단어 토큰화',words)\n",
    "all_ngrams = ngrams(words, 2)\n",
    "list(all_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 정제와 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거    \n",
    "- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어줌\n",
    "\n",
    "- 토큰화 작업에 방해가 되는 부분들을 배제시키기 위해 토큰화 작업 전과 후에 진행\n",
    "   1. 규칙에 기반한 표기가 다른 단어들 통합\n",
    "       - USA -> US\n",
    "       - uh-huh -> uhhuh\n",
    "   2. 대소문자 통합 : 소문자로 변환\n",
    "   3. 불필요한 단어의 제거\n",
    "       - 등장 빈도가 적은 단어\n",
    "       - 길이가 짧은 단어 : 길이가 2-3이하인 단어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stopwords 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 불용어 제거\n",
    "- 분석에 큰 의미가 없는 단어 제거\n",
    "    - is, the, a, will 등 필수 문법 요소이지만 문맥적으로 큰 의미가 없는 단어\n",
    "- 언어별 스톱 워드가 목록화되어 있음\n",
    "    - NLTK의 **`'stopwords'`** 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**영어의 불용어 목록**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 목록 수: 179\n",
      "[\"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print('불용어 목록 수:', len(stop_words))\n",
    "print(stop_words[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예. 불용어 제거**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전: ['One', 'of', 'the', 'first', 'things', 'that', 'we', 'ask', 'ourselves', 'is', 'what', 'ar', 'the', 'pros', 'and', 'cons', 'of', 'any', 'task', 'we', 'perform', '.']\n",
      "불용어 제거 후: ['One', 'first', 'things', 'ask', 'ar', 'pros', 'cons', 'task', 'perform', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "text = 'One of the first things that we ask ourselves is \\\n",
    "what ar the pros and cons of any task we perform.'\n",
    "\n",
    "# 토큰화\n",
    "text_token = word_tokenize(text)\n",
    "\n",
    "# 불용어 제거\n",
    "token_sw = []\n",
    "for word in text_token:\n",
    "    if word not in stop_words:\n",
    "        token_sw.append(word)\n",
    "\n",
    "print(f'불용어 제거 전:',text_token)\n",
    "print(f'불용어 제거 후:',token_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정규표현식을 이용한 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One the first things that ask ourselves what the pros and cons any task perform.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이가 1~2개인 단어들 제거\n",
    "import re\n",
    "\n",
    "text = 'One of the first things that we ask ourselves is \\\n",
    "what ar the pros and cons of any task we perform.'\n",
    "\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "shortword.sub('',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 불용어 제거\n",
    "- 토큰화 후에 접속사, 조사 제거\n",
    "- 명사, 형용사 중 제거하고 싶은 단어들을 불용어 목록에 정의하여 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "text = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. \\\n",
    "예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "\n",
    "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
    "stop_words = set(stop_words.split())\n",
    "\n",
    "print(f'불용어 목록: ',stop_words)\n",
    "\n",
    "# 토큰화\n",
    "word_tokens = okt.morphs(text)\n",
    "token_sw = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "print(f'불용어 제거 전:',word_tokens)\n",
    "print(f'불용어 제거 후:',token_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고. 한국어 불용어 리스트 제공 사이트\n",
    "- https://www.ranks.nl/stopwords/korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>휴</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아이구</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아이쿠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아이고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>일곱</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>여덟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>아홉</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>령</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>영</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       휴\n",
       "0    아이구\n",
       "1    아이쿠\n",
       "2    아이고\n",
       "3      어\n",
       "4      나\n",
       "..   ...\n",
       "668   일곱\n",
       "669   여덟\n",
       "670   아홉\n",
       "671    령\n",
       "672    영\n",
       "\n",
       "[673 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stopwords_kr = pd.read_csv('data/한국어 불용어 리스트.csv', encoding='CP949')\n",
    "stopwords_kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전: ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
      "불용어 제거 후: ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. \\\n",
    "예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "\n",
    "stopwords_list = stopwords_kr.values.tolist()\n",
    "stop_words = stopwords_list\n",
    "\n",
    "# 토큰화\n",
    "word_tokens = okt.morphs(text)\n",
    "token_sw = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "print(f'불용어 제거 전:',word_tokens)\n",
    "print(f'불용어 제거 후:',token_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문법적 또는 의미적으로 변화하는 **`단어의 원형을 찾는 작업`**\n",
    "- 뿌리 단어를 찾아 단어의 개수를 줄임\n",
    "- 영어의 경우\n",
    "    - be동사 : am, ar, is\n",
    "    - 과거/현재, 3인칭 단수 여부, 진행형 등은 원래 단어가 변형된 것\n",
    "        - 예. work : worked, working, works ...    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어의 원형을 찾아가는 방식\n",
    "- 단어의 형태학(morphology)적 파싱을 진행\n",
    "- 형태소의 종류\n",
    "    - 어간(stem) : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "    - 접사(affix) : 단어에 추가적인 의미를 주는 부분\n",
    "        - 예. cats : cat(어간) + -s(접사)\n",
    "- Stemming(어간 추출)과 Lemmatization(표제어 추출)\n",
    "    - 표제어 추출이 어간 추출 보다 더 정교하며 의미론적 기반에서 단어의 원형을 찾음\n",
    "    - 표제어 추출이 어간 추출 보다 변환에 더 오랜 시간을 필요로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Stemming(어간 추출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 형태학적 분석을 단순화한 버전\n",
    "- 정해진 규칙만 보고 어미를 자르는 어림짐작의 작업\n",
    "- 원형 단어로 변환시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있음\n",
    "- NLTK의 Stemmer : Porter, Lancaster, Snowball Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLTK의 **`LancasterStemmer`** 또는 **`PorterStemmer`** API를 이용\n",
    "    - 진행형, 3인칭 단수, 과거형에 따른 동사, 비교, 최상에 따른 형용사 변화에 대한 더 단순한 원형 단어를 찾아 줌   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stemming 단계    \n",
    "    1. LancasterStemmer() 객체 생성\n",
    "    2. stem('원하는단어') 메서드 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 랭커스터 알고리즘(LancasterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "happy happy\n",
      "nat nat\n",
      "amus amus amus\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# stemmer객체 생성\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# 어간추출: stem()\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happies'))\n",
    "print(stemmer.stem('national'),stemmer.stem('nation'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuse'),stemmer.stem('amused'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 포터 알고리즘(PorterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "happier happi\n",
      "nation nation\n",
      "amus amus amus\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# stemmer객체 생성\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# 어간추출: stem()\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happies'))\n",
    "print(stemmer.stem('national'),stemmer.stem('nation'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuse'),stemmer.stem('amused'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> 랭커스터 알고리즘이 포터 알고리즘에 비해 단어 원형을 알아볼 수 없을 정도로 축소 시키므로, 데이터셋을 축소시켜야 하는 특정 상황에서 더 유용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제1. 다음 문장을 nltk의 PorterStemmer 추출 방식을 사용하여 단어 토큰화를 진행해보시오.\n",
    "```python\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "sentences = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "st = word_tokenize(sentences)\n",
    "\n",
    "st_p= [stemmer.stem(sentence) for sentence in st]\n",
    "print(st_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제2. 다음 단어들을 nltk의 두 가지 어간 추출 방식을 사용하여 어간을 추출하고 그 결과를 비교하시오.\r",
    "```python\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "```.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer1 = LancasterStemmer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "st_p= [stemmer.stem(word) for word in words]\n",
    "st_l = [stemmer1.stem(word) for word in words]\n",
    "\n",
    "print(st_p)\n",
    "print(st_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> 어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, Porter 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Lemmatization(표제어 추출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lemma : 표제어, 기본 사전형 단어\n",
    "- 표제어 추출은 표제어를 찾아가는 과정\n",
    "- 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아 줌\n",
    "- 어간 추출보다 성능이 더 좋음\n",
    "- 품사와 같은 문법 뿐만아니라 문장 내에서 단어 의미도 고려함\n",
    "- 어간 추출보다 시간이 더 걸림\n",
    "- NLTK의 Lemmatizer : WordNetLemmatizer    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`WordNetLemmatizer`** API를 이용한 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer() \n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "lemma_words= [lemma.lemmatize(word) for word in words]\n",
    "\n",
    "print(f'표제어 추출 전:',words)\n",
    "print(f'표제어 추출 후:',lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어의 품사 정보를 알면 더 정확한 결과를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse\n",
      "happy happy\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 한국어 어간 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한국어는 5언 9품사의 구조\n",
    "  - 체언 : 명사, 대명사, 수사\n",
    "  - 수식언 : 관형사, 부사\n",
    "  - 관계언 : 조사\n",
    "  - 독립언 : 감탄사\n",
    "  - 용언 : 동사, 형용사\n",
    "    - 동사와 형용사는 어간(stem)과 어미(ending)의 결합으로 구성되므로 용언이라고 언급할 경우 동사와 형용사를 포함하여 언급한 것임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 활용(conjugation)\n",
    "- 활용이란 용언의 어간(stem)이 어미(ending)을 가지는 일\n",
    "- 한국어, 인도유럽어에서 볼 수 있는 언어적 특징의 통칭적인 개념\n",
    "- 어간(stem) : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분\n",
    "    - 어간의 모양이 바뀔 수 있음(예. 긋다, 긋고, 그어서, 그어라)\n",
    "- 어미(ending) : 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분, 여러 문법적 기능을 수행\n",
    "- 활용의 구분\n",
    "    - 규칙활용\n",
    "    - 불규칙활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**규칙 활용**\n",
    "- 어간이 어미를 취할 때 어간의 모습이 일정한 경우\n",
    "- 어간과 어미를 합칠 때 어간의 형태가 바뀌지 않는 경우\n",
    "- 예. 잡다 : 잡(어간) + 다(어미)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**불규칙 활용**\n",
    "- 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우\n",
    "- 어간의 형식이 달라지는 경우\n",
    "    - 예. 듣/들-, 돕/도우-, 곱/고우-, 잇/이-, 올/올-, 노랗/노라-’\n",
    "- 특수한 어미를 취하는 경우\n",
    "    - 예. 오르+ 아/어→올라, 하+아/어→하여, 이르+아/어→이르러, 푸르+아/어→푸르러 \n",
    "- https://namu.wiki/w/한국어/불규칙%20활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
